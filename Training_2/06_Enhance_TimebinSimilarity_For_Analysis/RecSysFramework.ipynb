{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This projects aims to provide a framework like API in order to make high quality recommendations through various methods and also provide metrics in order to measure the results in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example usage of the framework is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose a data set.\n",
    "2. Choose a Similarity Measure\n",
    "3. Provide Hyperparameters\n",
    "4. Choose performance metrics\n",
    "5. Get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Framework Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pearson Correlation         (Linear Similarity)\n",
    "* Mutual Information          (Non-Linear Similarity)\n",
    "* Timebin-Based Neighbourhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed dataset analysis are provided as extra notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Movielens 100k\n",
    "* Movielens 1M\n",
    "* Netflix Prize (This version not exactly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy\n",
    "* Balanced Accuracy\n",
    "* Informedness\n",
    "* Markedness\n",
    "* F1\n",
    "* MCC\n",
    "* Precision\n",
    "* Recall\n",
    "* Specificity\n",
    "* NPV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features In Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Refactor The Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implement Timebin-Based Neighbourhood With Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add Deep Learning Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shortucts can be used to navigate to the related code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Time Constraint](#TimeConstraint)\n",
    "\n",
    "* [Accuracy](#Accuracy)\n",
    "\n",
    "* [Dataset](#Dataset)\n",
    "\n",
    "\n",
    "* [Dataset Operator](#DatasetOperator)\n",
    "  \n",
    "  * [Dataset User Operator](#DatasetUserOperator)\n",
    "\n",
    "  * [Dataset Movie Operator](#DatasetMovieOperator)\n",
    "\n",
    "\n",
    "* [Similarity Measure](#SimilarityMeasure)\n",
    "    \n",
    "  * [Pearson Similarity](#PearsonSimilarity)\n",
    "  \n",
    "  * [Pearson Similarity With Significance Weighting](#SignificanceWeighting)\n",
    "  \n",
    "  * [Mutual Information Similarity](#MutualInformationSimilarity)\n",
    "  \n",
    "  * [Timebin Similarity](#TimebinSimilarity)\n",
    "  \n",
    "\n",
    "* [Predict](#Predict)\n",
    "\n",
    "  * [Pearson Predict](#PearsonPredict)\n",
    "  \n",
    "  * [Mutual Information Predict](#MutualInformationPredict)\n",
    "  \n",
    "  * [Timebin Predict](#TimebinPredict)\n",
    "\n",
    "\n",
    "* [Analize Timebin Similarity](#AnalizeTimebinSimilarity)\n",
    "\n",
    "* [In Progress](#InProgress)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measure Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing The Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TimeConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeConstraint:\n",
    "    \"\"\"\n",
    "    TimeConstraint is a constraint on the timestamp of the movie ratings.\n",
    "    We classify a TimeConstraint as either max_time_constraint or time_bin_constraint.\n",
    "    max_time_constraint is used to simulate real life in which we do not know the future but all the data up until one point in time.\n",
    "    time_bin_constraint is used to grab a portion of a time interval where starting and ending points are strictly defined and data is well known.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, end_dt, start_dt=None):\n",
    "        \"\"\"\n",
    "        When end_dt is only given, system will have a max time constraint only.\n",
    "        When end_dt and start_dt are given, system will have a time_bin_constraint.\n",
    "        \n",
    "        :param end_dt: The ending time boundary.\n",
    "        :param start_dt: The starting time boundary.\n",
    "            Always set start_dt to None if you change the object from time_bin to max_limit.\n",
    "        \"\"\"\n",
    "        self.end_dt = end_dt\n",
    "        self.start_dt = start_dt\n",
    "\n",
    "    def is_valid_time_bin(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether this TimeConstraint object represents a valid time bin.\n",
    "        \"\"\"\n",
    "        if self.is_time_bin() and (self._end_dt > self._start_dt):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_valid_max_limit(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether this TimeConstraint represents a valid max time limit.\n",
    "        \"\"\"\n",
    "        if (self._end_dt is not None) and (self._start_dt is None):\n",
    "            return True\n",
    "\n",
    "    def is_time_bin(self) -> bool:\n",
    "        if (self._start_dt is not None) and (self._end_dt is not None):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Comparing TimeConstraints\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return self._start_dt == other.start_dt and self._end_dt == other.end_dt\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return self._start_dt != other.start_dt or self._end_dt != other.end_dt\n",
    "\n",
    "    # Properties\n",
    "\n",
    "    @property\n",
    "    def end_dt(self):\n",
    "        return self._end_dt\n",
    "\n",
    "    @end_dt.setter\n",
    "    def end_dt(self, value):\n",
    "        self._end_dt = value\n",
    "\n",
    "    @property\n",
    "    def start_dt(self):\n",
    "        return self._start_dt\n",
    "\n",
    "    @start_dt.setter\n",
    "    def start_dt(self, value):\n",
    "        self._start_dt = value\n",
    "\n",
    "    # Printing TimeConstraints\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"(start = {self._start_dt}, end= {self._end_dt})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"(start = {self._start_dt}, end= {self._end_dt})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy currently only supports ratings in between 0.5 and 5 with 0.5 increments.\n",
    "\n",
    "* Add min_rating - max_rating and increment parameters to accuracy class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    \"\"\"\n",
    "    Accuracy class provides deffirent metrics in order to measure accuracy of our analysis.\n",
    "    \n",
    "    Supported Measures:\n",
    "    rmse, accuracy, balanced accuracy, informedness, markedness, \n",
    "    f1, mcc, precision, recall, specificity, NPV and \n",
    "    other threshold measures where we round ratings less than 3.5 to min rating, upper to max rating and use supported measures on this data.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def rmse(predictions) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Root Mean Square Error of given list or Dataframe of (prediction, actual) data.\n",
    "        \n",
    "        In case rmse value is found 0, it is returned as 0.001 to differentiate between successfull rmse\n",
    "        calculation and erroneous calculations where no prediction data is provided.\n",
    "        \"\"\"\n",
    "        \n",
    "        # In case dataframe of predictions wher each row[0]=prediction, row[1]=actual rating\n",
    "        if type(predictions) is pd.DataFrame:\n",
    "            number_of_predictions = 0\n",
    "            sum_of_square_differences = 0.0\n",
    "            for row in predictions.itertuples(index=False):\n",
    "                prediction = row[0]\n",
    "                # In case valid prediction is made(0 is invalid, minimum 0.5 in movielens dataset)\n",
    "                if prediction != 0:\n",
    "                    # Round the ratings to the closest half or exact number\n",
    "                    # since movielens dataset only containst ratings 0.5, 1, 1.5,..., 4, 4.5, 5\n",
    "                    actual = Accuracy.half_round_rating(row[1])\n",
    "                    prediction = Accuracy.half_round_rating(prediction)\n",
    "                    \n",
    "                    sum_of_square_differences += (actual - prediction) ** 2\n",
    "                    number_of_predictions += 1\n",
    "                \n",
    "                if number_of_predictions == 0:\n",
    "                    return 0 \n",
    "                rmse_value = sum_of_square_differences / number_of_predictions\n",
    "            return rmse_value if rmse_value != 0 else 0.001\n",
    "        # In case list of predictions where each element is (prediction, actual)\n",
    "        elif type(predictions) is list:\n",
    "            number_of_predictions = 0\n",
    "            sum_of_square_differences = 0.0\n",
    "            for prediction, actual in predictions:\n",
    "                if prediction != 0:                  # if the prediction is valid\n",
    "                    actual = Accuracy.half_round_rating(actual)\n",
    "                    prediction = Accuracy.half_round_rating(prediction)\n",
    "                    \n",
    "                    sum_of_square_differences += (actual - prediction) ** 2\n",
    "                    number_of_predictions += 1\n",
    "                \n",
    "            if number_of_predictions == 0:\n",
    "                return 0\n",
    "        \n",
    "            rmse_value = sum_of_square_differences / number_of_predictions \n",
    "            return rmse_value if rmse_value != 0 else 0.001    \n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_accuracy(predictions) -> float:\n",
    "        \"\"\"\n",
    "        Threshold accuracy is the rate of sucessful prediction when we round \n",
    "        ratings between 0.5 and 3.5 to the lowest rating(0.5) ,\n",
    "        ratings between 3.5 and 5 to the highest rating(5)\n",
    "        \n",
    "        Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if type(predictions) is pd.DataFrame:\n",
    "            number_of_predictions = 0\n",
    "            number_of_hit = 0\n",
    "            for row in predictions.itertuples(index=False):\n",
    "                # row[1] : actual rating, row[0] : prediction\n",
    "                prediction = row[0]\n",
    "                if prediction != 0:\n",
    "                    actual = Accuracy.threshold_round_rating(row[1])\n",
    "                    prediction = Accuracy.threshold_round_rating(prediction)\n",
    "                    \n",
    "                    if actual == prediction:\n",
    "                        number_of_hit += 1\n",
    "                    number_of_predictions += 1\n",
    "            return number_of_hit / number_of_predictions if number_of_predictions != 0 else 0\n",
    "        elif type(predictions) is list:            \n",
    "            number_of_predictions = 0\n",
    "            number_of_hit = 0\n",
    "            for prediction, actual in predictions:\n",
    "                if prediction != 0:\n",
    "                    actual = Accuracy.threshold_round_rating(actual)\n",
    "                    prediction = Accuracy.threshold_round_rating(prediction)\n",
    "                    \n",
    "                    if actual == prediction:\n",
    "                        number_of_hit += 1\n",
    "                        \n",
    "                    number_of_predictions += 1\n",
    "            return number_of_hit / number_of_predictions if number_of_predictions != 0 else 0\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_analize(predictions):\n",
    "        \"\"\"\n",
    "        Analize the threshold predictions with all metrics found in the Accuracy class.\n",
    "        \"\"\"\n",
    "        \n",
    "        TP, FN, FP, TN = Accuracy.threshold_confusion_matrix(predictions)\n",
    "        precision = Accuracy.precision(TP, FP)     # also called PPV\n",
    "        recall = Accuracy.recall(TP, FN)           # also called TPR\n",
    "        specificity = Accuracy.specificity(FP, TN) # also called TNR\n",
    "        NPV = Accuracy.negative_predictive_value(FN, TN)\n",
    "        \n",
    "        accuracy = Accuracy.accuracy(TP, FN, FP, TN)\n",
    "        balanced_accuracy = Accuracy.balanced_accuracy(TPR=recall, TNR=specificity)\n",
    "        informedness = Accuracy.informedness(TPR=recall, TNR=specificity)\n",
    "        markedness = Accuracy.markedness(PPV=precision, NPV=NPV)\n",
    "        \n",
    "        f1 = Accuracy.f_measure(precision, recall)\n",
    "        mcc = Accuracy.mcc(TP, FN, FP, TN)\n",
    "        \n",
    "                \n",
    "        output = {\n",
    "                  \"accuracy\"         :round(accuracy, 3),\n",
    "                  \"balanced_accuracy\":round(balanced_accuracy, 3),\n",
    "                  \"informedness\"     :round(informedness, 3),\n",
    "                  \"markedness\"       :round(markedness, 3),\n",
    "                  \"f1\"               :round(f1, 3),\n",
    "                  \"mcc\"              :round(mcc, 3),\n",
    "                  \"precision\"        :round(precision, 3),\n",
    "                  \"recall\"           :round(recall, 3),\n",
    "                  \"specificity\"      :round(specificity, 3),\n",
    "                  \"NPV\"              :round(NPV, 3)\n",
    "                 }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def analize(predictions):\n",
    "        \"\"\"\n",
    "        Analize the threshold predictions with all metrics found in the Accuracy class.\n",
    "        \n",
    "        https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "        \n",
    "        Returns analysis for each class as list\n",
    "        :return: accuracy, balanced_accuracy, informedness, markedness, f1, mcc, precision, recall, specificity, NPV\n",
    "        \"\"\"\n",
    "        confusion_mtr = Accuracy.confusion_matrix(predictions)\n",
    "        \n",
    "        # Use macro averaging (https://stats.stackexchange.com/questions/187768/matthews-correlation-coefficient-with-multi-class)\n",
    "        precision = [0] * 10   # 10 is the number of classes found \n",
    "        recall = [0] * 10      # 0.5 -> Class 0 , 1 -> Class 1, 1.5 -> Class 2 .... \n",
    "        specificity = [0] * 10\n",
    "        NPV = [0] * 10 \n",
    "        \n",
    "        accuracy = [0] * 10 \n",
    "        balanced_accuracy = [0] * 10 \n",
    "        informedness = [0] * 10 \n",
    "        markedness = [0] * 10 \n",
    "        \n",
    "        f1 = [0] * 10 \n",
    "        mcc = [0] * 10\n",
    "        \n",
    "        for i in range(0, 10): # For Each Class\n",
    "            TP, FN, FP, TN = Accuracy.confusion_matrix_one_against_all(confusion_mtr, i)\n",
    "            precision[i] = Accuracy.precision(TP, FP)     # also called PPV\n",
    "            recall[i] = Accuracy.recall(TP, FN)           # also called TPR\n",
    "            specificity[i] = Accuracy.specificity(FP, TN) # also called TNR\n",
    "            NPV[i] = Accuracy.negative_predictive_value(FN, TN)\n",
    "\n",
    "            accuracy[i] = Accuracy.accuracy(TP, FN, FP, TN)\n",
    "            balanced_accuracy[i] = Accuracy.balanced_accuracy(TPR=recall[i], TNR=specificity[i])\n",
    "            informedness[i] = Accuracy.informedness(TPR=recall[i], TNR=specificity[i])\n",
    "            markedness[i] = Accuracy.markedness(PPV=precision[i], NPV=NPV[i])\n",
    "\n",
    "            f1[i] = Accuracy.f_measure(precision[i], recall[i])\n",
    "            mcc[i] = Accuracy.mcc(TP, FN, FP, TN)\n",
    "        \n",
    "        output = {\n",
    "                  \"accuracy\"         :Accuracy.round_list_elements(accuracy, 3),\n",
    "                  \"balanced_accuracy\":Accuracy.round_list_elements(balanced_accuracy, 3),\n",
    "                  \"informedness\"     :Accuracy.round_list_elements(informedness, 3),\n",
    "                  \"markedness\"       :Accuracy.round_list_elements(markedness, 3),\n",
    "                  \"f1\"               :Accuracy.round_list_elements(f1, 3),\n",
    "                  \"mcc\"              :Accuracy.round_list_elements(mcc, 3),\n",
    "                  \"precision\"        :Accuracy.round_list_elements(precision, 3),\n",
    "                  \"recall\"           :Accuracy.round_list_elements(recall, 3),\n",
    "                  \"specificity\"      :Accuracy.round_list_elements(specificity, 3),\n",
    "                  \"NPV\"              :Accuracy.round_list_elements(NPV, 3)\n",
    "                 }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def round_list_elements(l, precision):\n",
    "        \"\"\"\n",
    "        :param l: list of floats\n",
    "        :param precision: precision after dot\n",
    "        \"\"\"\n",
    "        return [ round(x, precision) for x in l ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy_multi_class(confusion_mtr):\n",
    "        length = len(confusion_mtr)\n",
    "        numenator = 0\n",
    "        denuminator = 0 \n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                temp = confusion_mtr[i][j]\n",
    "                denuminator += temp\n",
    "                if i == j:\n",
    "                    numenator += temp\n",
    "        return numenator / denominator\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(TP, FN, FP, TN):\n",
    "        return  (TP + TN) / (TP + FN + FP + TN)\n",
    "    \n",
    "    @staticmethod\n",
    "    def balanced_accuracy(TPR, TNR):\n",
    "        \"\"\"\n",
    "        :param TPR : True Positive Rate or recall or sensitivity\n",
    "        :param TNR : True Negative Rate or specificity or  selectivity\n",
    "        \"\"\"\n",
    "        return (TPR + TNR) / 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def informedness(TPR, TNR):\n",
    "        \"\"\"\n",
    "        :param TPR : True Positive Rate or recall or sensitivity\n",
    "        :param TNR : True Negative Rate or specificity or  selectivity\n",
    "        \"\"\"\n",
    "        return TPR + TNR - 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def markedness(PPV, NPV):\n",
    "        \"\"\"\n",
    "        :param PPV: Positive Predictive Value also known as precision\n",
    "        :param NPV: Negative Predictive Value\n",
    "        \"\"\"\n",
    "        return PPV + NPV - 1 \n",
    "    \n",
    "    @staticmethod\n",
    "    def precision(TP, FP):\n",
    "        \"\"\"\n",
    "        Also called as precision or positive predictive value (PPV)\n",
    "        \n",
    "        Precision = TP / (TP + FP) for binary class\n",
    "        Precision = TP / (All Predicted Positive) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TP + FP\n",
    "        return TP / denuminator if denuminator != 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def negative_predictive_value(FN, TN):\n",
    "        \"\"\"     \n",
    "        NPV = TN / (TN + FN) for binary class\n",
    "        NPV = TN / (All Predicted Negative) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TN + FN\n",
    "        return TN / denuminator if denuminator != 0 else 0    \n",
    "    \n",
    "    @staticmethod\n",
    "    def recall(TP, FN):\n",
    "        \"\"\"\n",
    "        Also called as sensitivity, recall, hitrate, or true positive rate(TPR)\n",
    "        Recall = TP / (TP + FN) for binary class\n",
    "        Recall = TP / (All Actual Positive) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TP + FN\n",
    "        return TP / denuminator if denuminator != 0 else 0    \n",
    "    \n",
    "    @staticmethod\n",
    "    def specificity(FP, TN):\n",
    "        \"\"\"\n",
    "        Also called as specificity, selectivity or true negative rate (TNR)\n",
    "        specificity = TN / (TP + FN) for binary class\n",
    "        specificity = TN / (All Actual Negative) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = FP + TN \n",
    "        return TN / denuminator if denuminator != 0 else 0 \n",
    "    \n",
    "    @staticmethod\n",
    "    def f_measure(precision, recall):\n",
    "        \"\"\"\n",
    "        F-Measure is the harmonic mean of the precision and recall.\n",
    "        \"\"\"\n",
    "        sum_of_both = precision + recall\n",
    "        return (2 * precision * recall) / sum_of_both if sum_of_both != 0 else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mcc(TP, FN, FP, TN):\n",
    "        \"\"\"\n",
    "        MCC(Matthews Correlation Coefficient)\n",
    "        \"\"\"\n",
    "        # Calulate Matthews Correlation Coefficient\n",
    "        numenator   = (TP * TN) - (FP * FN) \n",
    "        denominator = (TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)\n",
    "        denominator = math.sqrt(denominator) if denominator > 0 else 0\n",
    "        return numenator / denominator if denominator != 0 else 0\n",
    "  \n",
    "    @staticmethod\n",
    "    def confusion_matrix_one_against_all(confusion_mtr, class_i):\n",
    "        \"\"\"\n",
    "        Create binary confusion matrix out of multi-class confusion matrix\n",
    "        \n",
    "        Positive Class: class_i\n",
    "        Negative Class: non class_i\n",
    "                \n",
    "        TP: True Positive   FN: False Negative\n",
    "        FP: False Positive  TN: True Negative\n",
    "        \n",
    "        \"TP of Class_1\" is all Class_1 instances that are classified as Class_1.\n",
    "        \"TN of Class_1\" is all non-Class_1 instances that are not classified as Class_1.\n",
    "        \"FP of Class_1\" is all non-Class_1 instances that are classified as Class_1.\n",
    "        \"FN of Class_1\" is all Class_1 instances that are not classified as Class_1.\n",
    "        # https://www.researchgate.net/post/How_do_you_measure_specificity_and_sensitivity_in_a_multiple_class_classification_problem\n",
    "        \n",
    "        --> Input matrix\n",
    "                 | 0 Prediction | 1 Prediction | 2 Prediction | .....\n",
    "        0 Class  |     T0       |     ..       |      ..      |\n",
    "        1 Class  |     ..       |     T1       |      ..      | \n",
    "        2 Class  |     ..       |     ..       |      T2      |\n",
    "        \n",
    "        --> Output matrix\n",
    "        \n",
    "                        | Positive Prediction | Negative Prediction\n",
    "        Positive Class  |       TP            |       FN\n",
    "        Negative Class  |       FP            |       TN\n",
    "        \n",
    "        :param confusion_mtr: 10 class confusion matrix designed for movielens\n",
    "        :param class_i: index of the class we are interested in(0-9)\n",
    "        :return: TP, FN, FP, TN\n",
    "        \"\"\"\n",
    "        length = len(confusion_mtr)\n",
    "\n",
    "        TP = confusion_mtr[class_i][class_i] \n",
    "        \n",
    "        actual_class_i_count = 0 \n",
    "        for i in range(length):  # sum of the row\n",
    "            actual_class_i_count += confusion_mtr[class_i][i]\n",
    "        FN = actual_class_i_count - TP\n",
    "        \n",
    "        predicted_class_i_count = 0\n",
    "        for i in range(length): # sum of the column\n",
    "            predicted_class_i_count += confusion_mtr[i][class_i]\n",
    "        FP = predicted_class_i_count - TP\n",
    "        \n",
    "        # sum of matrix\n",
    "        sum_of_matrix = np.sum(confusion_mtr)\n",
    "        # TN is found by summing up all values except the row and column of the class \n",
    "        TN = sum_of_matrix - predicted_class_i_count - actual_class_i_count - TP \n",
    "        \n",
    "        return TP, FN, FP, TN\n",
    "        \n",
    "    @staticmethod\n",
    "    def confusion_matrix(predictions):\n",
    "        \"\"\"\n",
    "        Create confusion matrix and then return TP, FN, FP, TN\n",
    "\n",
    "        0 Class: 0.5\n",
    "        1 Class: 1\n",
    "        2 Class: 1.5\n",
    "        3 Class: 2\n",
    "        4 Class: 2.5\n",
    "        5 Class: 3\n",
    "        6 Class: 3.5\n",
    "        7 Class: 4\n",
    "        8 Class: 4.5\n",
    "        9 Class: 5\n",
    "\n",
    "        T0: True 0\n",
    "        F0: False 0\n",
    "        T1: True 1\n",
    "        F1: False 1\n",
    "        ...\n",
    "\n",
    "                 | 0 Prediction | 1 Prediction | 2 Prediction | .....\n",
    "        0 Class  |     T0       |     ..       |      ..      |\n",
    "        1 Class  |     ..       |     T1       |      ..      | \n",
    "        2 Class  |     ..       |     ..       |      T2      |\n",
    "        ...\n",
    "        \"\"\"\n",
    "        # Create multiclass confusion matrix\n",
    "\n",
    "        conf_mtr = np.zeros( (10,10) )\n",
    "\n",
    "        for prediction in predictions:\n",
    "            predicted = Accuracy.half_round_rating(prediction[0])\n",
    "            actual    = Accuracy.half_round_rating(prediction[1])\n",
    "\n",
    "            predicted_class_index = int( (predicted * 2) - 1 )\n",
    "            actual_class_index = int( (actual * 2) - 1 )\n",
    "\n",
    "            conf_mtr[actual_class_index][predicted_class_index] += 1\n",
    "\n",
    "        return conf_mtr\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_confusion_matrix(predictions):\n",
    "        \"\"\"\n",
    "        Create confusion matrix and then return TP, FN, FP, TN\n",
    "\n",
    "        Positive Class: 5\n",
    "        Negative Class: 0.5\n",
    "\n",
    "        TP: True Positive\n",
    "        TN: True Negative\n",
    "        FP: False Positive\n",
    "        FN: False Negative\n",
    "\n",
    "                        | Positive Prediction | Negative Prediction\n",
    "        Positive Class  |       TP            |       FN\n",
    "        Negative Class  |       FP            |       TN\n",
    "\n",
    "        \"\"\"\n",
    "        # Create confusion matrix\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for prediction in predictions:\n",
    "            predicted = Accuracy.threshold_round_rating(prediction[0])\n",
    "            actual = Accuracy.threshold_round_rating(prediction[1])\n",
    "            if predicted == 5 and actual == 5:\n",
    "                TP += 1\n",
    "            elif predicted == 5 and actual == 0.5:\n",
    "                FP += 1\n",
    "            elif predicted == 0.5 and actual == 0.5:\n",
    "                TN += 1\n",
    "            elif predicted == 0.5 and actual == 5:\n",
    "                FN += 1\n",
    "        return TP, FN, FP, TN\n",
    "\n",
    "    @staticmethod\n",
    "    def half_round_rating(rating):\n",
    "        \"\"\"\n",
    "        Round ratings to the closest match in the movielens dataset\n",
    "        For ex.\n",
    "          ratings between 2 and 2.25 -> round to 2\n",
    "          ratings between 2.25 and 2.5 -> round to 2.5\n",
    "          ratings between 2.5 and 2.75 -> round to 2.5\n",
    "          ratings between 2.75 and 3 -> round to 3\n",
    "\n",
    "        \"\"\"\n",
    "        floor_value = math.floor(rating)\n",
    "        if(rating > floor_value + 0.75):\n",
    "            return floor_value + 1\n",
    "        elif(rating > floor_value + 0.5 or rating > floor_value + 0.25):\n",
    "            return floor_value + 0.5\n",
    "        else:\n",
    "            return floor_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_round_rating(rating):\n",
    "        \"\"\"\n",
    "        Round ratings to the closest match in threshold fashion\n",
    "          ratings between 0.5 and 3.5 -> round to 0.5\n",
    "          ratings between 3.5 and 5 -> round to 5\n",
    "        \"\"\"\n",
    "        if (0.5 <= rating < 3.5):\n",
    "            return 0.5\n",
    "        elif (3.5 <= rating <= 5):\n",
    "            return 5\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(ABC):\n",
    "    \"\"\"\n",
    "    Dataset class and its subclasses provides utilities in order to import datasets.\n",
    "    \n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def load():\n",
    "        \"\"\" Every subclass must provide static load method\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp'),\n",
    "                 ratings_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv',\n",
    "                 movies_col_names=('item_id', 'title', 'genres'),\n",
    "                 movies_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv',\n",
    "                 is_ratings_cached=True,\n",
    "                 is_movies_cached=True):\n",
    "        Dataset.__init__(self)\n",
    "        self.is_ratings_cached = is_ratings_cached\n",
    "        self.is_movies_cached = is_movies_cached\n",
    "        self.ratings = MovieLensDataset.load_ratings(ratings_path,\n",
    "                                                     ratings_col_names) if self.is_ratings_cached else None\n",
    "        self.movies = MovieLensDataset.load_movies(movies_path,\n",
    "                                                   movies_col_names) if self.is_movies_cached else None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_movies(movies_path,\n",
    "                    movies_col_names=('item_id', 'title', 'genres')):\n",
    "        if not os.path.isfile(movies_path) or not movies_col_names:\n",
    "            return None\n",
    "\n",
    "        # read movies\n",
    "        movies = pd.read_csv(movies_path, sep=',', header=1, names=movies_col_names)\n",
    "\n",
    "        # Extract Movie Year\n",
    "        movies['year'] = movies.title.str.extract(\"\\((\\d{4})\\)\", expand=True)\n",
    "        movies.year = pd.to_datetime(movies.year, format='%Y')\n",
    "        movies.year = movies.year.dt.year  # As there are some NaN years, resulting type will be float (decimals)\n",
    "\n",
    "        # Remove year part from the title\n",
    "        movies.title = movies.title.str[:-7]\n",
    "\n",
    "        return movies\n",
    "\n",
    "    @staticmethod\n",
    "    def load_ratings(ratings_path,\n",
    "                     ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp')):\n",
    "        if not os.path.isfile(ratings_path) or not ratings_col_names:\n",
    "            return None\n",
    "\n",
    "        # read ratings\n",
    "        ratings = pd.read_csv(ratings_path, sep=',', header=1, names=ratings_col_names)\n",
    "\n",
    "        # Convert timestamp into readable format\n",
    "        ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s', origin='unix')\n",
    "        \n",
    "        ratings.sort_values(by='timestamp', inplace=True)\n",
    "        \n",
    "        return ratings\n",
    "\n",
    "    @staticmethod\n",
    "    def create_movie_ratings(ratings, movies):\n",
    "        return pd.merge(ratings, movies, on='item_id')\n",
    "\n",
    "    @staticmethod\n",
    "    def load(ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp'),\n",
    "             ratings_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv',\n",
    "             movies_col_names=('item_id', 'title', 'genres'),\n",
    "             movies_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv'\n",
    "             ):\n",
    "        # Load movies\n",
    "        movies = MovieLensDataset.load_movies(movies_path=movies_path, movies_col_names=movies_col_names)\n",
    "        # Load ratings\n",
    "        ratings = MovieLensDataset.load_ratings(ratings_path=ratings_path, ratings_col_names=ratings_col_names)\n",
    "\n",
    "        # Merge the ratings and movies\n",
    "        movie_ratings = pd.merge(ratings, movies, on='item_id')\n",
    "\n",
    "        return movie_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DatasetOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetOperator:\n",
    "    @staticmethod\n",
    "    def apply_time_constraint(movie_ratings : pd.DataFrame, time_constraint : TimeConstraint = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        movie_ratings can be ratings or movie_ratings.\n",
    "        \"\"\"\n",
    "        if time_constraint is None:\n",
    "            return movie_ratings\n",
    "\n",
    "        if time_constraint.is_valid_max_limit():\n",
    "            return movie_ratings.loc[movie_ratings.timestamp < time_constraint.end_dt]\n",
    "        \n",
    "        if time_constraint.is_valid_time_bin():\n",
    "            return movie_ratings.loc[(movie_ratings.timestamp >= time_constraint.start_dt) & (movie_ratings.timestamp < time_constraint.end_dt)]\n",
    "        \n",
    "        return movie_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DatasetUserOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetUserOperator:\n",
    "    \"\"\"\n",
    "    Provides utility methods to get user data out of datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_users(ratings : pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Get list of unique 'user_id's\n",
    "\n",
    "        :return: the ids of the users found in movie_ratings\n",
    "        \"\"\"\n",
    "        return pd.unique(ratings['user_id'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_active_users(ratings : pd.DataFrame, n : int=10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get Users in sorted order where the first one is the one who has given most ratings.\n",
    "\n",
    "        :param n: Number of users to retrieve.\n",
    "        :return: user DataFrame with index of 'user_id' and columns of ['mean_rating', 'ratings_count'] .\n",
    "        \"\"\"\n",
    "        active_users = pd.DataFrame(ratings.groupby('user_id')['rating'].mean())\n",
    "        active_users['ratings_count'] = pd.DataFrame(ratings.groupby('user_id')['rating'].count())\n",
    "        active_users.sort_values(by=['ratings_count'], ascending=False, inplace=True)\n",
    "        active_users.columns = ['mean_rating', 'ratings_count']\n",
    "        return active_users.head(n)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_random_users(ratings : pd.DataFrame, n : int=1):\n",
    "        \"\"\"\n",
    "        Get list of random n number of 'user_id's\n",
    "\n",
    "        :param n: Number of random users\n",
    "        :return: List of random 'user_id's\n",
    "        \"\"\"\n",
    "        return random.choices(population=DatasetUserOperator.get_users(ratings), k=n)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_ratings(ratings : pd.DataFrame, user_id : int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all the ratings given by of the chosen users\n",
    "\n",
    "        :param user_id: id of the chosen user\n",
    "        :return: Ratings given by the 'user_id'\n",
    "        \"\"\"\n",
    "        return ratings.loc[ratings['user_id'] == user_id]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_avg(ratings : pd.DataFrame, user_id : int):\n",
    "        user_ratings = DatasetUserOperator.get_user_ratings(ratings, user_id)\n",
    "        return user_ratings.rating.mean() if not user_ratings.empty else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_timestamp(ratings : pd.DataFrame, user_id: int, movie_id: int):\n",
    "        \"\"\"\n",
    "        Get the timestamp of the given rating\n",
    "\n",
    "        :param user_id: the users whose rating timestamp we are searching\n",
    "        :param movie_id: id of the movie that the user gave the rating\n",
    "        :return: if found the datetime object otherwise None\n",
    "        \"\"\"\n",
    "        timestamp = ratings.loc[(ratings['user_id'] == user_id) & (ratings['item_id'] == movie_id)]\n",
    "        return timestamp.values[0, 3] if not timestamp.empty else None\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_first_timestamp(ratings : pd.DataFrame):\n",
    "        return ratings['timestamp'].min()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_avg_timestamp(ratings : pd.DataFrame, user_id: int):\n",
    "        user_ratings = DatasetUserOperator.get_user_ratings(ratings, user_id)\n",
    "        return user_ratings.timestamp.mean() if not user_ratings.empty else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_ratings_at(ratings : pd.DataFrame, user_id: int, at: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get user ratings up until the given datetime\n",
    "        :param user_id: id of the chosen user\n",
    "        :param at: only those ratings that are before this date will be taken into account\n",
    "        :return: Ratings given by the 'user_id' before given datetime\n",
    "        \"\"\"\n",
    "        return ratings.loc[(ratings['user_id'] == user_id) & (ratings.timestamp < at)]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_user_avg_at(ratings : pd.DataFrame, user_id: int, at: datetime):\n",
    "        user_ratings = DatasetUserOperator.get_user_ratings_at(ratings, user_id, at)\n",
    "        return user_ratings.rating.mean() if not user_ratings.empty else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DatasetMovieOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMovieOperator:\n",
    "    \"\"\"\n",
    "    Provides utility methods to get user data out of datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_movie(movies : pd.DataFrame, movie_id : int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get Movie Record\n",
    "\n",
    "        :return: DataFrame which contains the given 'movie_id's details. If not found empty DataFrame .\n",
    "        \"\"\"\n",
    "        return movies.loc[movies['item_id'] == movie_id]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_movies(movies : pd.DataFrame) -> list:\n",
    "        \"\"\"\n",
    "        Get list of unique movies.\n",
    "\n",
    "        :return: List of movie ids\n",
    "        \"\"\"\n",
    "        return movies['item_id'].values.tolist()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_random_movies(movies, n=10):\n",
    "        \"\"\"\n",
    "        Get list of random n number of 'item_id's or in other words the movies\n",
    "\n",
    "        :param n: Number of random movies\n",
    "        :return: List of random 'movie_id's\n",
    "        \"\"\"\n",
    "        return random.choices(population=DatasetMovieOperator.get_movies(movies), k=n)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_movies_watched(movie_ratings : pd.DataFrame, \n",
    "                           user_id: int, \n",
    "                           time_constraint: TimeConstraint = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all the movies watched by the chosen user.\n",
    "\n",
    "        :param user_id: the user that we want to get the movies he-she has watched.\n",
    "        :param time_constraint: type of the time constraint.\n",
    "        :return: DataFrame of all movies watched with 'item_id', 'rating' columns\n",
    "        \"\"\"\n",
    "        filtered_movie_ratings = DatasetOperator.apply_time_constraint(movie_ratings, time_constraint)\n",
    "        return filtered_movie_ratings.loc[filtered_movie_ratings['user_id'] == user_id]\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_movie_rating(ratings : pd.DataFrame, movie_id: int, user_id: int) -> int:\n",
    "        \"\"\"\n",
    "        Get the movie rating taken by the chosen user\n",
    "\n",
    "        :param movie_id: the movie chosen movie's id\n",
    "        :param user_id: id of the chosen user\n",
    "        :return: Rating given by user. If not found, returns 0\n",
    "        \"\"\"\n",
    "        movie_rating = ratings.loc[(ratings['user_id'] == user_id) & (ratings['item_id'] == movie_id)]\n",
    "        return movie_rating.values[0, 2] if not movie_rating.empty else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_random_movie_watched(movie_ratings : pd.DataFrame, user_id: int) -> int:\n",
    "        \"\"\"\n",
    "        Get random movie id watched.\n",
    "\n",
    "        :param user_id: User of interest\n",
    "        :return:  movie_id or item_id of the random movie watched by the user.\n",
    "                  In case non-valid user_id supplied then returns 0\n",
    "        \"\"\"\n",
    "        movies_watched = DatasetMovieOperator.get_movies_watched(movie_ratings, user_id)[['item_id', 'rating']]\n",
    "        return random.choice(movies_watched['item_id'].values.tolist()) if not movies_watched.empty else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_random_movies_watched(movie_ratings : pd.DataFrame, user_id: int, n=2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get random n movies watched by the user. Only use when n > 2\n",
    "\n",
    "        Use get_random_movie_watched if n=1 since that one 2 fold faster.\n",
    "\n",
    "        :param user_id: the user of interest\n",
    "        :param n: number of random movies to get\n",
    "        :return: DataFrame of movies, if none found then empty DataFrame\n",
    "        \"\"\"\n",
    "        movies_watched = DatasetMovieOperator.get_movies_watched(movie_ratings, user_id)[['item_id', 'rating']]\n",
    "        return random.choices(population=movies_watched['item_id'].values.tolist(),\n",
    "                              k=n) if not movies_watched.empty else movies_watched\n",
    "\n",
    "    @staticmethod\n",
    "    def get_random_movie_per_user(movie_ratings : pd.DataFrame, user_id_list : list) -> list:\n",
    "        \"\"\"\n",
    "        Get random movie for each user given in the 'user_id_list'\n",
    "\n",
    "        :param user_id_list: List of valid user_ids\n",
    "        :return: List of (user_id, movie_id) tuples\n",
    "                where each movie_id is randomly chosen from watched movies of the user_id .\n",
    "                In case any one of the user_id's supplies invalid, then the movie_id will be 0 for that user.\n",
    "        \"\"\"\n",
    "        user_movie_list = list()\n",
    "        for user_id in user_id_list:\n",
    "            user_movie_list.append((user_id, DatasetMovieOperator.get_random_movie_watched(movie_ratings, user_id)))\n",
    "        return user_movie_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimilarityMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PearsonSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PearsonSimilarity:\n",
    "    \"\"\"\n",
    "    Pearson Correlation is the classic way of giving recommendations. \n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_knn_movie(ratings:pd.DataFrame, \n",
    "                      user_id:int, movie_id:int, \n",
    "                      knn, k=25,\n",
    "                      corr_threshold=0.1):\n",
    "        if knn is None or knn.empty:\n",
    "            return None\n",
    "\n",
    "        mv_data = list()\n",
    "        target_user_hist = DatasetMovieOperator.get_movies_watched(ratings, user_id)\n",
    "\n",
    "        for neighbour_id, row in knn.iterrows():\n",
    "            rating = DatasetMovieOperator.get_movie_rating(ratings, movie_id, neighbour_id)\n",
    "\n",
    "            user_hist = DatasetMovieOperator.get_movies_watched(ratings, neighbour_id)\n",
    "            n_common = len(target_user_hist.merge(user_hist, on='item_id'))\n",
    "            if(rating == 0 or row['correlation'] < corr_threshold):\n",
    "                continue\n",
    "            mv_data.append( (neighbour_id, row['correlation'], n_common) )\n",
    "\n",
    "        mv_df = pd.DataFrame(mv_data)\n",
    "\n",
    "        if mv_df.empty:\n",
    "            return None\n",
    "\n",
    "        mv_df.columns = ['user_id', 'corr', 'n_common']\n",
    "        mv_df.set_index('user_id', inplace=True)       \n",
    "\n",
    "        return mv_df.head(k) if k is not None else mv_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_knn(movie_ratings:pd.DataFrame, \n",
    "                user_id:int, k:int=10, \n",
    "                min_common_elements: int = 5,\n",
    "                user_corr_matrix = None):\n",
    "        \"\"\"\n",
    "        :param user_id: the user of interest\n",
    "        :param k: number of neighbours to retrieve, None to get all\n",
    "        :param time_constraint: time constraint when choosing neighbours\n",
    "        :param user_corr_matrix: Provide correlation matrix if you want to optimize the process\n",
    "        :return: Returns the k neighbours and correlations in between them. If no neighbours found, returns None\n",
    "                 DataFrame which has 'Correlation' column and 'user_id' index.\n",
    "        \"\"\"\n",
    "        \n",
    "        if(user_corr_matrix is None):\n",
    "            user_corr_matrix = PearsonSimilarity.create_user_corrs(movie_ratings, min_common_elements)\n",
    "            # Exit if matrix is None after creation\n",
    "            if user_corr_matrix is None:\n",
    "                return None\n",
    "\n",
    "        # Get the chosen 'user_id's correlations\n",
    "        user_correlations = user_corr_matrix.get(user_id)\n",
    "        if user_correlations is None:\n",
    "            return None\n",
    "\n",
    "        # Drop any null, if found\n",
    "        user_correlations.dropna(inplace=True)\n",
    "\n",
    "        # Create A DataFrame from not-null correlations of the 'user_id'\n",
    "        users_alike = pd.DataFrame(user_correlations)\n",
    "        \n",
    "        # Rename the only column to 'correlation'\n",
    "        users_alike.columns = ['correlation']\n",
    "\n",
    "        # Sort the user correlations in descending order\n",
    "        # so that first one is the most similar, last one least similar\n",
    "        users_alike.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "\n",
    "        # Eliminate Correlation to itself by deleting first row,\n",
    "        # since biggest corr is with itself it is in first row\n",
    "        return users_alike.iloc[1:k+1] if k is not None else users_alike.iloc[1:]\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_user_corrs(movie_ratings, min_common_elements):\n",
    "        user_movie_matrix = movie_ratings.pivot_table(index='title', columns='user_id', values='rating')\n",
    "        return user_movie_matrix.corr(method=\"pearson\", min_periods=min_common_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SignificanceWeighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignificanceWeighting:\n",
    "    \"\"\"\n",
    "    All the methods takes a DataFrame of neighbours\n",
    "    Input DataFrame index='user_id' columns=['corr', 'n_common']\n",
    "    \n",
    "    Use PearsonSimilarity.get_knn_movie method as input to provided methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_corated_item_knn(movie_knn):\n",
    "        if movie_knn is None:\n",
    "            return None\n",
    "        \n",
    "        data = list()\n",
    "        for neighbour_id, row in movie_knn.iterrows():\n",
    "            u = row['n_common']\n",
    "            data.append( (neighbour_id, u * row['corr']) )\n",
    "        data_df = pd.DataFrame(data)\n",
    "        data_df.columns=['NeighbourId', 'correlation']\n",
    "        data_df.set_index('NeighbourId', inplace=True)\n",
    "        data_df.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "        return data_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_static_weighting_knn(movie_knn, alpha=50):\n",
    "        if movie_knn is None:\n",
    "            return None\n",
    "\n",
    "        data = list()\n",
    "        for neighbour_id, row in movie_knn.iterrows():\n",
    "            u = 1\n",
    "            if row['n_common'] < alpha:\n",
    "                u = row['n_common'] / alpha\n",
    "            data.append( (neighbour_id, u * row['corr']) )\n",
    "        data_df = pd.DataFrame(data)\n",
    "        data_df.columns=['NeighbourId', 'correlation']\n",
    "        data_df.set_index('NeighbourId', inplace=True)\n",
    "        data_df.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "        return data_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_dynamic_weighting_knn(movie_knn, beta=3/4):\n",
    "        if movie_knn is None:\n",
    "            return None\n",
    "\n",
    "        mean_n_common = movie_knn['n_common'].mean()\n",
    "\n",
    "        alpha = 2 * beta * mean_n_common \n",
    "\n",
    "        data = list()\n",
    "        for neighbour_id, row in movie_knn.iterrows():\n",
    "            u = 1\n",
    "            if row['n_common'] < alpha:\n",
    "                u = row['n_common'] / alpha\n",
    "            data.append( (neighbour_id, u * row['corr']) )\n",
    "        data_df = pd.DataFrame(data)\n",
    "        data_df.columns=['NeighbourId', 'correlation']\n",
    "        data_df.set_index('NeighbourId', inplace=True)\n",
    "        data_df.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "        return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MutualInformationSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualInformationSimilarity:\n",
    "    \"\"\"\n",
    "    Mutual information is used to give recommendations.\n",
    "    It is based on entropy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lowest_rating, highest_rating, rating_increment, ratings):\n",
    "        self.lowest_rating = lowest_rating\n",
    "        self.highest_rating = highest_rating\n",
    "        self.rating_increment = rating_increment\n",
    "        self.ratings = ratings\n",
    "    \n",
    "    def get_k_neighbors(self, user_id, movie_id, k):\n",
    "        mi_data = list()\n",
    "        for i in DatasetUserOperator.get_users(self.ratings):\n",
    "            rating = DatasetMovieOperator.get_movie_rating(self.ratings, movie_id, i)\n",
    "            if rating != 0:\n",
    "                corr = self.mutual_information(user_id, i)\n",
    "                if(corr != 0):\n",
    "                    #print(f\"rating={rating} corr={corr}\")\n",
    "                    mi_data.append( (i, corr) )\n",
    "        \n",
    "        mi_df = pd.DataFrame(mi_data)\n",
    "        \n",
    "        if mi_df.empty:\n",
    "            return mi_df\n",
    "        \n",
    "        mi_df.columns=['NeighbourId', 'corr']\n",
    "        mi_df.set_index('NeighbourId', inplace=True)\n",
    "        mi_df.sort_values(by='corr', ascending=False, inplace=True)\n",
    "        return mi_df.drop(user_id, errors='ignore').head(k)\n",
    "    \n",
    "    def get_n_rating(self, common_ratings, isFirstUser:bool, rating):\n",
    "        if isFirstUser == True:\n",
    "            return common_ratings.loc[common_ratings['rating_x'] == rating].count()[0]\n",
    "        else:\n",
    "            return common_ratings.loc[common_ratings['rating_y'] == rating].count()[0]\n",
    "    \n",
    "    def get_n_rating_pair(self, common_ratings, user1_rating, user2_rating):\n",
    "        return common_ratings.loc[(common_ratings['rating_x'] == user1_rating) & \n",
    "                                  (common_ratings['rating_y'] == user2_rating) ].count()[0]\n",
    "    \n",
    "    def mutual_information(self, user1_id, user2_id):\n",
    "        common_ratings = pd.merge(self.ratings.loc[(self.ratings['user_id'] == user1_id)],\n",
    "                                  self.ratings.loc[(self.ratings['user_id'] == user2_id)],\n",
    "                                  on=\"item_id\")\n",
    "        n_common = common_ratings.count()[0]\n",
    "        if(n_common == 0):\n",
    "            return 0.0\n",
    "        # Calculate entropies\n",
    "        user1_entropy, n1 = self.user_entropy(common_ratings, True, n_common)\n",
    "        user2_entropy, n2 = self.user_entropy(common_ratings, False, n_common)\n",
    "        user1_and_2_entropy, n1_2 = self.cross_entropy(common_ratings, n_common)\n",
    "        \n",
    "        # Handle bias of the entropy values and calculate mutual information\n",
    "        I  = user1_entropy + ((n1 - 1) / (2*n_common))         # User1 entropy \n",
    "        I += user2_entropy + ((n2 - 1) / (2*n_common))         # Plus User2 entropy\n",
    "        I -= user1_and_2_entropy + ((n1_2 - 1) / (2*n_common)) # Plus Cross entropy\n",
    "        return I\n",
    "    \n",
    "    def user_entropy(self, common_ratings, isFirstUser:bool, n_common):\n",
    "        entropy = 0.0\n",
    "        rating = self.lowest_rating\n",
    "        n = 0\n",
    "        while(rating <= self.highest_rating):\n",
    "            n_rating = self.get_n_rating(common_ratings, isFirstUser, rating)\n",
    "            Pi = n_rating / n_common\n",
    "            if(Pi > 0):\n",
    "                entropy -= Pi * math.log(Pi)\n",
    "                n += 1\n",
    "            rating += self.rating_increment\n",
    "            \n",
    "        return entropy, n\n",
    "    \n",
    "    def cross_entropy(self, common_ratings, n_common):\n",
    "        entropy = 0.0\n",
    "        user1_rating = self.lowest_rating\n",
    "        n = 0\n",
    "        while(user1_rating <= self.highest_rating):\n",
    "            user2_rating = self.lowest_rating\n",
    "            while(user2_rating <= self.highest_rating):\n",
    "                n_rating = self.get_n_rating_pair(common_ratings, user1_rating, user2_rating)\n",
    "                Pi = n_rating / n_common\n",
    "                if(Pi > 0):\n",
    "                    entropy -= Pi * math.log(Pi)\n",
    "                    n += 1\n",
    "                user2_rating += self.rating_increment\n",
    "            user1_rating += self.rating_increment\n",
    "                    \n",
    "        return entropy, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TimebinSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimebinSimilarity:\n",
    "    \"\"\"\n",
    "    TimebinSimilarity is a way to provide personalized timebin based recommendations.\n",
    "    Here we define an alternative to classical pearson by providing temporal timebin aspect.\n",
    "    Here is how we do in summary:\n",
    "      1. Choose a person to make prediction on (as always).\n",
    "      2. Take the last s movie ratings of this person as a timebin from a random timepoint.(user must have at least s movie watched before!)\n",
    "      3. Find neighbour timebins to the selected timebin.(using correlations between them, higher, better)\n",
    "      4. Predict rating for the person using the timebin neighbours the same way as we do with k nearest neighbour.\n",
    "    \"\"\"\n",
    "       \n",
    "    @staticmethod\n",
    "    def timebin_similarity(timebin_a:pd.DataFrame, timebin_a_user_avg_rating:float,\n",
    "                           timebin_b:pd.DataFrame, timebin_b_user_avg_rating:float)->(float,int):\n",
    "        \"\"\"\n",
    "        Find Pearson correlation between the timebin and its given neighbour timebin\n",
    "        \n",
    "        :param merged: Merged version of timebin and neighbour timebin in this order.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Filter unrelated movie ratings and only keep common movie ratings\n",
    "        merged = timebin_b.merge(timebin_a, on='item_id')\n",
    "\n",
    "        # Calculate Pearson Correlation in between the self.timebin and given neighbour\n",
    "        numenator = ((merged['rating_x'] - timebin_a_user_avg_rating) * (merged['rating_y'] - timebin_b_user_avg_rating)).sum()\n",
    "        denominator = math.sqrt(((merged['rating_x'] - timebin_a_user_avg_rating) ** 2).sum())\n",
    "        denominator *= math.sqrt(((merged['rating_y'] - timebin_b_user_avg_rating) ** 2).sum())\n",
    "        pearson = numenator / denominator if denominator != 0 else 0\n",
    "        \n",
    "        return pearson, len(merged)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_timebin(ratings, user_id, timebin_i, timebin_size):\n",
    "        \"\"\"\n",
    "        Generate the timebin by using given info.\n",
    "        timebin_i is the index the first movie\n",
    "        timebin_size is the number of movies to take starting at timebin_i th index.\n",
    "        \"\"\"\n",
    "        all_movies_watched = DatasetMovieOperator.get_movies_watched(ratings, user_id)[['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "        return all_movies_watched.iloc[timebin_i:timebin_i+timebin_size]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_possible_neighbours(ratings, user_id:int, \n",
    "                                    timebin_i:int, timebin_size:int,\n",
    "                                    movie_id:int, \n",
    "                                    min_common_neighbour_ratings:int) -> list:\n",
    "        \"\"\"\n",
    "        Get possible neighbour Ids as list.\n",
    "\n",
    "        Possible neighbours have watched'min_common_neighbour_ratings' number movies in common with the target timebin.\n",
    "        Possible neighbours also have to watch the target movie('movie_id') that we want to make prediction on.\n",
    "        \"\"\"\n",
    "\n",
    "        # Recreate the user timebin using its identifiers\n",
    "        timebin = TimebinSimilarity.get_timebin(ratings, user_id, timebin_i, timebin_size)\n",
    "        timebin.drop(movie_id, inplace=True, errors='ignore')\n",
    "\n",
    "        # :::: Change filter on ratings.loc by adding time constraint in case we filter future neighbour timebins::::\n",
    "\n",
    "        # Count number of common ratings with other users\n",
    "        userdict = defaultdict(int)\n",
    "        for movie_id in timebin.index.values.tolist():\n",
    "            movie_raters = ratings.loc[(ratings['item_id'] == movie_id)][['user_id']].values.tolist()\n",
    "            for rater in movie_raters:\n",
    "                userdict[rater[0]] += 1\n",
    "\n",
    "        # save as neighbour, if common rating count greater than min_common_neighbour_ratings and given rating to the movie index at i\n",
    "        neighbour_id_list = []\n",
    "        for possible_neighbour_id, n_rating in userdict.items():\n",
    "            if n_rating > min_common_neighbour_ratings:\n",
    "                if DatasetMovieOperator.get_movie_rating(ratings, movie_id, possible_neighbour_id) != 0 :\n",
    "                    if possible_neighbour_id != user_id:\n",
    "                        neighbour_id_list.append(possible_neighbour_id)\n",
    "        return neighbour_id_list\n",
    "\n",
    "    @staticmethod\n",
    "    def get_neighbour_timebins(ratings, neighbour_id:int, movie_id:int,\n",
    "                               neighbour_min_s, neighbour_max_s, neighbour_step_size)->list:\n",
    "        \"\"\"\n",
    "        Given a neighbour_id, take the all the timebins of this neighbour which includes the target movie.\n",
    "        \n",
    "        :param neighbour_min_s: Minimum number of movies to include inside neighbour timebins\n",
    "        :param neighbour_max_s: Maximum number of movies to include inside neighbour timebins\n",
    "        :param neighbour_step_size: Number of movies to extend per step the timebin size\n",
    "        \"\"\"\n",
    "        # Start by taking all movies watched by the neighbour\n",
    "        all_movies_watched = DatasetMovieOperator.get_movies_watched(ratings, neighbour_id)[['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "        n_movies = len(all_movies_watched)\n",
    "        \n",
    "        neighbour_timebins = list()\n",
    "\n",
    "        # For each timebin_size\n",
    "        for timebin_size in range(neighbour_min_s, neighbour_max_s, neighbour_step_size):\n",
    "            # Traverse the all movies by taking 'timebin_size' piece of ratings per loop\n",
    "            for i in range(0, n_movies, timebin_size):\n",
    "                timebin = all_movies_watched.iloc[i:i+timebin_size]\n",
    "                # Take only timebins which includes this movie\n",
    "                if not (movie_id in timebin.index):\n",
    "                    continue\n",
    "                # Insert the timebin, its index i, and its size timebin_size into the list as a tuple\n",
    "                neighbour_timebins.append(  (timebin, i, timebin_size)  )\n",
    "        return neighbour_timebins\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_knn(ratings, user_id, movie_id, k, timebin_i, timebin_size,\n",
    "                         neighbour_min_s=5, neighbour_max_s=50, neighbour_step_size=1,\n",
    "                         corr_threshold=0.1, min_common_elements=3, min_common_neighbour_ratings=3):\n",
    "        \"\"\"\n",
    "        ratings must be sorted in 'timestamp' order.\n",
    "        \n",
    "        :param timebin_i: timebin identifier, identifies start of timebin\n",
    "        :param timebin_size: timebin identifier, identifies size of the timebin starting at timebin_i index\n",
    "        :param neighbour_min_s: Minimum number of movies to include inside neighbour timebins \n",
    "        :param neighbour_max_s: Maximum number of movies to include inside neighbour timebins\n",
    "        :param neighbour_step_size: Number of movies to extend per step the timebin size\n",
    "        :param min_common_elements: Minimum number of common elements in between timebins in order them to become neighbour.\n",
    "                                              This  value is used to find similar timebins.\n",
    "        :param min_common_neighbour_ratings: Minimum number of ratings in common between two users neighbour when taking timebins of them.\n",
    "                                   This value is used to create possible timebins.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Recreate the user timebin using its identifiers\n",
    "        timebin = TimebinSimilarity.get_timebin(ratings, user_id, timebin_i, timebin_size)\n",
    "        timebin.drop(movie_id, inplace=True, errors='ignore')\n",
    "        \n",
    "        if( timebin is None ) or timebin.empty:\n",
    "            return None\n",
    "        \n",
    "        timebin_user_avg = DatasetUserOperator.get_user_avg_at(ratings, user_id, timebin.iloc[-1]['timestamp'])\n",
    "        \n",
    "        # Neighbours contains users who has rated min_common_neighbour_ratings movies in common \n",
    "        # and also rated last movie of the timebin by the way, remember we are trying to predict last movie\n",
    "        neighbours = TimebinSimilarity.get_possible_neighbours(ratings, user_id, \n",
    "                                                               timebin_i, timebin_size, \n",
    "                                                               movie_id, min_common_neighbour_ratings)\n",
    "\n",
    "        accounted_neighbours = defaultdict(int)\n",
    "        flag = False\n",
    "        \n",
    "        data = list()\n",
    "        neighbour_count = 0\n",
    "        for neighbour_id in neighbours:\n",
    "            # Get all possible timebins of the neighbour that includes the movie with a timebin size with given constraints.\n",
    "            neighbour_timebins = TimebinSimilarity.get_neighbour_timebins(ratings, neighbour_id, movie_id,\n",
    "                                                                          neighbour_min_s, neighbour_max_s, \n",
    "                                                                          neighbour_step_size)\n",
    "            \n",
    "            # For each neighbour timebin, calculate pearson between the timebin and neighbour timebin \n",
    "            for neighbour_timebin, timebin_i, timebin_size in neighbour_timebins:\n",
    "                \n",
    "                # Calculate user avg for each timebin by assuming system at the time neighbour rated the last movie\n",
    "                neighbour_time_constraint = TimeConstraint(end_dt=neighbour_timebin.iloc[-1]['timestamp'])\n",
    "                neighbour_avg_rating = DatasetUserOperator.get_user_avg_at(ratings, neighbour_id, neighbour_time_constraint.end_dt)\n",
    "                #print(neighbour_timebin)\n",
    "                # Calculate Pearson Correlation between neighbour timebin and self.timebin\n",
    "                corr, common_elements = TimebinSimilarity.timebin_similarity(timebin, timebin_user_avg,\n",
    "                                                                             neighbour_timebin, neighbour_avg_rating)\n",
    "                \n",
    "                #print(f\"Corr -> {corr}, CommonElements -> {common_elements}\")\n",
    "                \n",
    "                # If less than min_timebin_neighbour_ratings movies in common rated in the neighbour_timebin, dont process this timebin\n",
    "                if common_elements < min_common_elements:\n",
    "                    continue\n",
    "                \n",
    "                # Dont include the neighbour timebin if corr is invalid or less than corr_threshold\n",
    "                if math.isnan(corr) or corr < corr_threshold:                     \n",
    "                    continue\n",
    "                \n",
    "                accounted_neighbours[neighbour_id] += 1\n",
    "                if len(accounted_neighbours) > k :\n",
    "                    break\n",
    "                data.append( (neighbour_id, common_elements, corr, timebin_i, timebin_size) )\n",
    "                \n",
    "            if flag:\n",
    "                break\n",
    "            \n",
    "        \n",
    "        similar_timebins = pd.DataFrame(data, columns=['neighbour_id', 'n_common','pearson_corr', 'timebin_i', 'timebin_size'])\n",
    "        \n",
    "        # Drop duplicate (neighbour_id, n_common, pearson_corr, timebin_i, timebin_size) tuples so that now no repeating timebin exists\n",
    "        similar_timebins.drop_duplicates(['neighbour_id','n_common', 'pearson_corr', 'timebin_i'], inplace=True)\n",
    "        \n",
    "        return similar_timebins \n",
    "    \n",
    "    @staticmethod\n",
    "    def store_neighbour_data(ratings, similar_timebins, movie_id):\n",
    "        \"\"\"\n",
    "            Collect structured data aimed to generate predictions to the target movie later when needed.\n",
    "        \"\"\"\n",
    "        data = list()\n",
    "        \n",
    "        for row in similar_timebins.itertuples(index=False):\n",
    "            # Get neighbour timebin data\n",
    "            neighbour_id = row[0]\n",
    "            n_common = row[1]\n",
    "            corr = row[2]\n",
    "            timebin_i = row[3]\n",
    "            timebin_size = row[4]\n",
    "            \n",
    "            # Create the neighbour timebin from its data\n",
    "            neighbour_bin = TimebinSimilarity.get_timebin(ratings, neighbour_id, timebin_i, timebin_size)\n",
    "            rating = None\n",
    "            try: \n",
    "                rating = neighbour_bin.loc[movie_id]     # test whether the neighbour has rating for the item\n",
    "            except KeyError:\n",
    "                continue                                 # if neighbour dont have rating for the item, then pass this round of loop\n",
    "            \n",
    "            # insert rating for the neighbour into data \n",
    "            #  and also insert other data identifying the neighbour(for in case we use them in analysis)\n",
    "            if rating is not None:\n",
    "                data.append( (rating[0], corr, neighbour_id, n_common, timebin_i, timebin_size) )\n",
    "        \n",
    "        return data \n",
    "    \n",
    "    @staticmethod\n",
    "    def store_common_neighbour_ratings(ratings, timebin, similar_timebins):\n",
    "        \"\"\"\n",
    "        Collect neighbour data for each commonly rated movie.\n",
    "        \"\"\"\n",
    "        # Store data in order to return as result\n",
    "        data = defaultdict(list)\n",
    "        for row in similar_timebins.itertuples(index=False):\n",
    "            # Get neighbour timebin data\n",
    "            neighbour_id = row[0]\n",
    "            n_common = row[1]\n",
    "            corr = row[2]\n",
    "            timebin_i = row[3]\n",
    "            timebin_size = row[4]\n",
    "            \n",
    "            # Create the neighbour timebin from its data\n",
    "            neighbour_bin = TimebinSimilarity.get_timebin(ratings, neighbour_id, timebin_i, timebin_size)\n",
    "            \n",
    "            # Get rid of movie ratings of the neighbour that are not found in the timebin\n",
    "            merged_bin = pd.merge(timebin, neighbour_bin, left_index=True, right_index=True)\n",
    "            \n",
    "            # For each common movie rating, store neighbour rating and its correlation to the timebin\n",
    "            for bin_row in merged_bin.itertuples(index=True):\n",
    "                curr_movie = bin_row[0]\n",
    "                neighbour_rating = bin_row[3]\n",
    "                #print(bin_row[0], bin_row[1], bin_row[2], bin_row[3], bin_row[4])\n",
    "                data[curr_movie].append( (neighbour_rating, corr, neighbour_id, n_common, timebin_i, timebin_size) )\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_timebin_size(tc: TimeConstraint):\n",
    "        return abs((tc.start_dt - tc.end_dt).days)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PearsonPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PearsonPredict:\n",
    "    \"\"\"\n",
    "    Only neighbourhood based methods are supported.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_movie(movie_ratings, user_id, movie_id, \n",
    "                      time_constraint: TimeConstraint = None, k = 10, min_common_elements = 5):\n",
    "        \"\"\"\n",
    "        Predict a movie of a user.\n",
    "        \n",
    "        For optimization, use predict function instead of this.\n",
    "        \"\"\"\n",
    "        movie_ratings = DatasetOperator.apply_time_constraint(movie_ratings, time_constraint)\n",
    "        knn = PearsonSimilarity.get_knn(movie_ratings, user_id, k, min_common_elements)\n",
    "        return PearsonPredict.predict(movie_ratings, user_id, movie_id=movie_id, k_neighbours=knn)\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict(ratings, user_id, movie_id, k_neighbours: pd.DataFrame, k=None) -> float:\n",
    "        \"\"\"\n",
    "        Predict the movie for given user using Pearson Correlation\n",
    "\n",
    "        :param user_id: user of interest\n",
    "        :param movie_id: the movie's rating is the one we we want to predict\n",
    "        :param k_neighbours: k nearest neighbours in DataFrame where index user_id, column correlation in between.\n",
    "        :return: Prediction rating\n",
    "        \"\"\"\n",
    "        # If a movie with movie_id not exists, predict 0\n",
    "        if DatasetMovieOperator.get_movie(ratings, movie_id).empty:\n",
    "            return 0\n",
    "\n",
    "        if k_neighbours is None or k_neighbours.empty:\n",
    "            return 0\n",
    "        \n",
    "        user_avg_rating = DatasetUserOperator.get_user_avg(ratings, user_id)\n",
    "        weighted_sum = 0.0\n",
    "        sum_of_weights = 0.0\n",
    "        count = 0\n",
    "        for neighbour_id, data in k_neighbours.iterrows():\n",
    "            # Get each neighbour's correlation 'user_id' and her rating to 'movie_id'\n",
    "            neighbour_corr = data['correlation']\n",
    "            neighbour_rating = DatasetMovieOperator.get_movie_rating(ratings, movie_id, neighbour_id)\n",
    "            # If the neighbour doesnt give rating to the movie_id, pass this around of the loop\n",
    "            if neighbour_rating == 0:\n",
    "                continue\n",
    "            neighbour_avg_rating = DatasetUserOperator.get_user_avg(ratings, neighbour_id)\n",
    "            neighbour_mean_centered_rating = neighbour_rating - neighbour_avg_rating\n",
    "            # Calculate Weighted sum and sum of weights\n",
    "            weighted_sum += neighbour_mean_centered_rating * neighbour_corr\n",
    "            sum_of_weights += neighbour_corr\n",
    "            count += 1\n",
    "            \n",
    "            if (k is not None) and (count > k):\n",
    "                break\n",
    "            \n",
    "        if sum_of_weights != 0:\n",
    "            prediction = user_avg_rating + (weighted_sum / sum_of_weights)\n",
    "        else:\n",
    "            prediction = 0  # In this case, none of the neighbours have given rating to 'the movie'\n",
    "\n",
    "        return prediction\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_movies_watched(movie_ratings, user_id, n=10, k=10, time_constraint=None, min_common_elements = 5) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        :param user_id: user of interest\n",
    "        :param n: Number of movies to predict\n",
    "        :param k: k neighbours to take into account\n",
    "        :param time_constraint: When calculating k neighbours,\n",
    "                                only those that comply to time_constraints will be taken into account.\n",
    "        :return: DataFrame of Predictions where columns = ['prediction', 'rating'] index = 'movie_id'\n",
    "        \"\"\"\n",
    "        \n",
    "        movie_ratings = DatasetOperator.apply_time_constraint(movie_ratings)\n",
    "        \n",
    "        # Get all movies watched by a user\n",
    "        movies_watched = DatasetMovieOperator.get_movies_watched(movie_ratings, user_id)[['item_id', 'rating']]\n",
    "\n",
    "        if movies_watched.empty:\n",
    "            return None\n",
    "        \n",
    "        # Cache user correlations because of the repeated predictions\n",
    "        user_corr_matrix = PearsonSimilarity.create_user_corrs(movie_ratings, min_common_elements)\n",
    "        \n",
    "        predictions = list()\n",
    "        number_of_predictions = 0\n",
    "        for row in movies_watched.itertuples(index=False):\n",
    "            knn = PearsonSimilarity.get_knn(movie_ratings, user_id, k, min_common_elements, user_corr_matrix)\n",
    "            prediction = PearsonPredict.predict(movie_ratings, user_id, movie_id=row[0], k_neighbours=knn)\n",
    "            if number_of_predictions == n:\n",
    "                break\n",
    "            predictions.append([prediction, row[1], row[0]])\n",
    "            number_of_predictions += 1\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions, columns=['prediction', 'rating', 'movie_id'])\n",
    "        predictions_df.movie_id = predictions_df.movie_id.astype(int)\n",
    "        return predictions_df.set_index('movie_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MutualInformationPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualInformationPredict:\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(ratings, user_id, movie_id, k=25, min_neighbour=3,lowest_rating=0.5, highest_rating=5, rating_increment=0.5):\n",
    "        MI = MutualInformationSimilarity(lowest_rating, highest_rating, rating_increment, ratings)\n",
    "        \n",
    "        knn = MI.get_k_neighbors(user_id, movie_id, k)\n",
    "        \n",
    "        if knn is None or knn.empty:\n",
    "            return 0\n",
    "        \n",
    "        weighted_sum = 0\n",
    "        weight_sum = 0\n",
    "        count = 0\n",
    "        for neighbour_id, row in knn.iterrows():\n",
    "            rating = DatasetMovieOperator.get_movie_rating(ratings, movie_id, neighbour_id)\n",
    "            if rating == 0:\n",
    "                continue\n",
    "            weighted_sum += rating * row['corr']\n",
    "            weight_sum += row['corr']\n",
    "            count += 1\n",
    "            \n",
    "        if weight_sum == 0 or count < min_neighbour:\n",
    "            return 0\n",
    "        \n",
    "        prediction = weighted_sum / weight_sum\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TimebinPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimebinPredict:\n",
    "    @staticmethod\n",
    "    def predict_movie(ratings, user_id, movie_id, timebin_size,\n",
    "                      neighbour_min_s=5, neighbour_max_s=50, neighbour_step_size=1,\n",
    "                      k = 10,\n",
    "                      corr_threshold=0.1, \n",
    "                      min_common_neighbour_ratings=3, min_common_elements=3, \n",
    "                      min_neighbour_count=5):\n",
    "        \"\"\"\n",
    "        Predict Movie with given timebin. Last element of the provided timebin is the movie we want to predict.\n",
    "        \"\"\"\n",
    "        timebin_i = len(DatasetMovieOperator.get_movies_watched(ratings, user_id)) - 1 - timebin_size\n",
    "        \n",
    "        if(timebin_i < 0):\n",
    "            print(\"The specified user timebin is not valid.\")\n",
    "            return None\n",
    "        \n",
    "        knn = TimebinSimilarity.get_knn(ratings, \n",
    "                                        user_id,\n",
    "                                        movie_id,\n",
    "                                        k,\n",
    "                                        timebin_i, \n",
    "                                        timebin_size, \n",
    "                                        neighbour_min_s, \n",
    "                                        neighbour_max_s, \n",
    "                                        neighbour_step_size, \n",
    "                                        corr_threshold, \n",
    "                                        min_common_neighbour_ratings, \n",
    "                                        min_common_elements)\n",
    "    \n",
    "        return TimebinPredict.predict(ratings, movie_id, knn, min_neighbour_count)\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_knn_data(knn_data,  min_neighbour_count=5):\n",
    "        \"\"\"\n",
    "        Make prediction by using the neighbour timebin data on the movie of interest.\n",
    "        K nearest neighbours is used only.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sort the data by corr\n",
    "        knn_data.sort(key=lambda data_element: data_element[1])\n",
    "        \n",
    "        # each row of data ->(rating, corr, neighbour_id, n_common, timebin_i, timebin_size, timebin_size_in_days)\n",
    "        weighted_sum = 0\n",
    "        weight_sum = 0\n",
    "        count = 0\n",
    "        for (rating, corr, _, _, _, _) in knn_data:\n",
    "            weighted_sum += rating * corr\n",
    "            weight_sum += corr            \n",
    "            count += 1\n",
    "            \n",
    "        if count < min_neighbour_count:\n",
    "            return 0\n",
    "        \n",
    "        prediction = weighted_sum / weight_sum\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(ratings, movie_id, knn, min_neighbour_count=5, k=None):\n",
    "        if knn is None or knn.empty:\n",
    "            return 0\n",
    "        \n",
    "        neighbours_data = knn[['neighbour_id', 'pearson_corr']].groupby('neighbour_id').mean()\n",
    "        neighbours_data.sort_values(by='pearson_corr', ascending=False, inplace=True)\n",
    "        \n",
    "        weighted_sum = 0\n",
    "        weight_sum = 0\n",
    "        count = 0\n",
    "        for index, row in knn[['neighbour_id', 'pearson_corr']].groupby('neighbour_id').mean().iterrows():\n",
    "            neighbour_id = index\n",
    "            corr = row[0]\n",
    "            rating = DatasetMovieOperator.get_movie_rating(ratings, movie_id, neighbour_id)\n",
    "            weighted_sum += rating * corr\n",
    "            weight_sum += corr\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if k is not None and count > k:\n",
    "                break\n",
    "\n",
    "        if count < min_neighbour_count:\n",
    "            return 0\n",
    "        \n",
    "        prediction = weighted_sum / weight_sum\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AnalizeTimebinSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalizeTimebinSimilarity:\n",
    "    \n",
    "    @staticmethod\n",
    "    def analize_timebin_prediction(movie_ratings, ratings, n, s, k=10, min_neighbour=3, corr_threshold=0.1, \n",
    "                                   neighbour_min_s=5, neighbour_max_s=50, neighbour_step_size=5, \n",
    "                                   min_common_elements=3,\n",
    "                                   min_common_neighbour_ratings=3):\n",
    "        \"\"\"\n",
    "        :param s: Timebin size\n",
    "        :param n: Number of prediction to make where each prediction is for a randomly chosen users' a random movie\n",
    "        :param k: K nearest neighbour of the timebin based neighbourhood will be taken when making prediction.\n",
    "        :param min_neighbour: Minimum number of neighbour timebins when making prediction.\n",
    "        :param corr_threshold: Overrides the default behaviour if you set any value other than None\n",
    "        :param neighbour_min_s: Minimum number of movies to include inside neighbour timebins\n",
    "                                Overrides default behaviour of the class if given any value other than None \n",
    "        :param neighbour_max_s: Maximum number of movies to include inside neighbour timebins\n",
    "                                Overrides default behaviour of the class if given any value other than None\n",
    "        :param neighbour_step_size: Increase of number of movies in the timebin per step\n",
    "                                Overrides default behaviour of the class if given any value other than None\n",
    "        :param min_common_elements: Minimum number of common elements in between timebins in order them to become neighbour.\n",
    "                                              This  value is used to find similar timebins.\n",
    "        :param min_common_ratings: Minimum number of ratings in common between two users neighbour when taking timebins of them.\n",
    "                                   This value is used to create possible timebins.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Correlation threshold should be greater than or equal to 0 for best quality. \n",
    "        if corr_threshold < 0:\n",
    "            corr_threshold = 0\n",
    "        \n",
    "        output = list()\n",
    "        \n",
    "        # collected data count\n",
    "        count = 0\n",
    "        \n",
    "        user_predictions = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "        while count < n:\n",
    "            \n",
    "            # Choose a random user\n",
    "            user_id = DatasetUserOperator.get_random_users(ratings, n=1)[0]\n",
    "            \n",
    "            ### Choose Random timebin\n",
    "            \n",
    "            # Get all movies that has been watched by the user\n",
    "            movies_watched = DatasetMovieOperator.get_movies_watched(ratings, user_id)\n",
    "            movies_watched = movies_watched[['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "            \n",
    "            # In order to simulate that we are making prediction in random point in time\n",
    "            # take a random starting point and take s piece movies starting from that point as timebin.\n",
    "            max_movie_index = ( len(movies_watched) - 1 ) - s\n",
    "            if max_movie_index <= 0:\n",
    "                continue\n",
    "            timebin_i = random.randint(0, max_movie_index)\n",
    "            timebin = TimebinSimilarity.get_timebin(ratings, user_id, timebin_i, s)\n",
    "            \n",
    "            # Make sure we have a valid timebin before moving forward\n",
    "            if timebin is None or s < min_common_elements:\n",
    "                continue\n",
    "            \n",
    "            ### Predict last movie of the timebin with Pearson Correlation\n",
    "            movie_id = timebin.index[-1]\n",
    "            \n",
    "            # Get max limit time constraint where max limit is the time we rate the last movie inside the timebin\n",
    "            time_constraint = TimeConstraint(end_dt=(timebin.iloc[-1]['timestamp'] - timedelta(milliseconds=1))) \n",
    "            movie_ratings_with_constraint = DatasetOperator.apply_time_constraint(movie_ratings, time_constraint)\n",
    "            \n",
    "            # Store K Nearest Neighbours\n",
    "            pearson_knn = PearsonSimilarity.get_knn(movie_ratings_with_constraint, user_id, k=None)\n",
    "            pearson_knn_with_movie = PearsonSimilarity.get_knn_movie(ratings, user_id, movie_id, pearson_knn)    \n",
    "            \n",
    "            # Actual\n",
    "            actual = DatasetMovieOperator.get_movie_rating(ratings, movie_id, user_id)\n",
    "            user_predictions[user_id][movie_id]['actual'] = actual\n",
    "            \n",
    "            # Pearson Correlation Coefficient(PCC)\n",
    "            pearson_prediction = PearsonPredict.predict(ratings, user_id, movie_id, pearson_knn, k=k)\n",
    "            user_predictions[user_id][movie_id]['pearson_predictions'] = pearson_prediction\n",
    "            \n",
    "            # Staticly Weighted PCC\n",
    "            static_weighted_prediction = PearsonPredict.predict(ratings, user_id, movie_id, \n",
    "                                                                SignificanceWeighting.get_static_weighting_knn(pearson_knn_with_movie))\n",
    "            user_predictions[user_id][movie_id]['PCC_static_weighted_predictions']  = static_weighted_prediction\n",
    "            \n",
    "            # Dynamicly Weighted PCC\n",
    "            dynamic_weighted_prediction = PearsonPredict.predict(ratings, user_id, movie_id, \n",
    "                                                                 SignificanceWeighting.get_dynamic_weighting_knn(pearson_knn_with_movie))\n",
    "            user_predictions[user_id][movie_id]['PCC_dynamic_weighted_predictions'] = dynamic_weighted_prediction\n",
    "            \n",
    "            # Co-rated Item Count(CIC) Based PCC\n",
    "            CIC_prediction = PearsonPredict.predict(ratings, user_id, movie_id, SignificanceWeighting.get_corated_item_knn(pearson_knn_with_movie))\n",
    "            user_predictions[user_id][movie_id]['PCC_CIC_predictions']              = CIC_prediction\n",
    "            \n",
    "            # Mutual Information\n",
    "            MI_prediction = MutualInformationPredict.predict(ratings, user_id, movie_id)\n",
    "            user_predictions[user_id][movie_id]['mutual_information_predictions']   = MI_prediction\n",
    "            \n",
    "            \n",
    "            # Timebin Based Neighbourhood\n",
    "            similar_timebins = TimebinSimilarity.get_knn(ratings, user_id, \n",
    "                                                         movie_id,\n",
    "                                                         k,\n",
    "                                                         timebin_i, s,\n",
    "                                                         neighbour_min_s=neighbour_min_s,\n",
    "                                                         neighbour_max_s=neighbour_max_s,\n",
    "                                                         neighbour_step_size=neighbour_step_size,\n",
    "                                                         corr_threshold=corr_threshold, \n",
    "                                                         min_common_elements=min_common_elements, \n",
    "                                                         min_common_neighbour_ratings=min_common_neighbour_ratings)\n",
    "            timebin_prediction = TimebinPredict.predict(ratings, movie_id, similar_timebins, min_neighbour_count=min_neighbour)\n",
    "            user_predictions[user_id][movie_id]['timebin_predictions'] = timebin_prediction\n",
    "            \n",
    "            # Save the data\n",
    "            output.append( (user_id, timebin_i, s, similar_timebins) )\n",
    "            \n",
    "            # we saved a data succesfully, increment collected data cout\n",
    "            if timebin_prediction != 0:\n",
    "                count += 1\n",
    "        \n",
    "        # RMSE for debug output\n",
    "        #normal_rmse = Accuracy.rmse(pearson_predictions)\n",
    "        #timebins_rmse = Accuracy.rmse(timebin_predictions)\n",
    "        #print(f\"Normal RMSE: {normal_rmse:.2} Timebin RMSE:{timebins_rmse:.2}\")\n",
    "        \n",
    "        result = {\n",
    "            \"output\":output,\n",
    "            \"user_predictions\":user_predictions\n",
    "        }\n",
    "        \n",
    "        return result         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InProgress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens = MovieLensDataset()\n",
    "movie_ratings = movielens.create_movie_ratings(movielens.ratings ,movielens.movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = AnalizeTimebinSimilarity.analize_timebin_prediction(movie_ratings, movielens.ratings,n=10, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [(129,\n",
       "   72,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0            68         3      0.841888         90            45),\n",
       "  (564,\n",
       "   131,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (37,\n",
       "   1,\n",
       "   10,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0             99         4      0.500000          0            20\n",
       "   1             99         5      0.649486          0            25\n",
       "   4             99         6      0.709299          0            40\n",
       "   6            353         3      0.994135          5             5\n",
       "   7            353         6      0.963058          0            10\n",
       "   8            353         6      0.965035          0            15\n",
       "   9            353         7      0.890034          0            20\n",
       "   10           353         7      0.869261          0            25),\n",
       "  (349,\n",
       "   23,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (31,\n",
       "   39,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (511,\n",
       "   31,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (207,\n",
       "   3,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (566,\n",
       "   30,\n",
       "   10,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0            411         4      0.727409         35            35\n",
       "   1            455         5      0.417390         25            25\n",
       "   2            455         5      0.799478         30            30\n",
       "   3              8         4      0.187851         30            10\n",
       "   4              8         5      0.251370         30            15\n",
       "   5              8         7      0.426108         20            20\n",
       "   6              8         7      0.419353         25            25\n",
       "   7              8         5      0.252568         30            30\n",
       "   8              8         4      0.613360          0            35\n",
       "   9              8         7      0.426108          0            40\n",
       "   10             8         8      0.431764          0            45),\n",
       "  (124,\n",
       "   20,\n",
       "   10,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0            274         3      0.934551         25            25\n",
       "   1            274         3      0.934551         30            30\n",
       "   2            274         4      0.171060         35            35\n",
       "   3            274         3      0.963902          0            45\n",
       "   4            132         3      0.986600          0            30\n",
       "   5            132         3      0.986712          0            35\n",
       "   6            132         3      0.987244          0            40\n",
       "   7            132         3      0.987446          0            45\n",
       "   8            522         3      0.762536         20            20\n",
       "   9            522         3      0.752791         25            25\n",
       "   10           522         3      0.762536          0            40),\n",
       "  (307,\n",
       "   936,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (504,\n",
       "   4,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (191,\n",
       "   11,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (371,\n",
       "   12,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (146,\n",
       "   21,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (72,\n",
       "   1,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (121,\n",
       "   3,\n",
       "   10,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0             40         6      0.290655          0            20\n",
       "   1             40         6      0.324577          0            25\n",
       "   2             40         6      0.237839          0            30\n",
       "   3             40         6      0.250684          0            35\n",
       "   4             40         6      0.204176          0            40\n",
       "   5             40         6      0.204994          0            45\n",
       "   6            604         4      0.674043          0            10\n",
       "   7            604         5      0.673580          0            15\n",
       "   8            604         5      0.499528          0            20\n",
       "   9            604         5      0.511462          0            25\n",
       "   10           604         5      0.599640          0            30),\n",
       "  (298,\n",
       "   105,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (166,\n",
       "   90,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (524,\n",
       "   79,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           137         4      0.324985         60            30\n",
       "   1           137         3      0.258957         70            35\n",
       "   2           137         4      0.324985         45            45),\n",
       "  (445,\n",
       "   11,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0            63         5      0.406711        125            25\n",
       "   1            63         5      0.406711        120            30\n",
       "   2            63         5      0.400045        120            40\n",
       "   3            16         3      0.150346         20            20\n",
       "   4            16         3      0.150254          0            35\n",
       "   5            16         3      0.150346          0            40\n",
       "   6            16         3      0.150417          0            45),\n",
       "  (162,\n",
       "   7,\n",
       "   10,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0            602         4      0.241819         25            25\n",
       "   1            602         3      0.780773         30            30\n",
       "   2            602         4      0.294401         35            35\n",
       "   3            373         3      0.465314         30            10\n",
       "   4            373         3      0.420162         30            15\n",
       "   5            373         4      0.503184         20            20\n",
       "   6            373         4      0.344902         25            25\n",
       "   7            373         4      0.284978         30            30\n",
       "   8            373         3      0.376363         35            35\n",
       "   9            373         8      0.147374          0            40\n",
       "   10           373         8      0.124202          0            45),\n",
       "  (154,\n",
       "   6,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (162,\n",
       "   19,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0            84         3      0.421669         30            30\n",
       "   1            84         3      0.286610         45            45),\n",
       "  (404,\n",
       "   22,\n",
       "   10,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0            411         3      0.546869         30            15\n",
       "   1            411         5      0.224234         25            25\n",
       "   2            411         5      0.221184         30            30\n",
       "   3            411         4      0.342791         35            35\n",
       "   4            411         4      0.323355          0            45\n",
       "   5            229         4      0.125747         20            20\n",
       "   6            229         3      1.000000         25            25\n",
       "   7            229         4      0.875097         30            30\n",
       "   8            117         3      0.369541         50            10\n",
       "   9            117         3      0.369541         45            15\n",
       "   10           117         3      0.411726         50            25),\n",
       "  (245,\n",
       "   1,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (254,\n",
       "   72,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           573         4      0.250878         30            15\n",
       "   1           573         5      0.443562         25            25\n",
       "   2           573         5      0.399075         30            30\n",
       "   3           573         4      0.351620         35            35\n",
       "   4           573         4      0.250878          0            45\n",
       "   5           326         3      0.848532         20            20\n",
       "   6           326         3      0.848532          0            40\n",
       "   7           326         4      0.845407          0            45\n",
       "   8           249         4      0.888633         90            45\n",
       "   9           328         3      0.873451        180            45),\n",
       "  (208,\n",
       "   14,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           217         3      0.327210         60            15\n",
       "   1           217         3      0.326737         60            20\n",
       "   2           217         4      0.566947         50            25\n",
       "   3           217         3      0.323902         60            30\n",
       "   4           217         4      0.564389         40            40\n",
       "   5           217         4      0.552310         45            45\n",
       "   6           469         3      0.248159         70            35),\n",
       "  (300,\n",
       "   14,\n",
       "   10,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0             64         3      0.127515          0            40\n",
       "   1             64         3      0.127515          0            45\n",
       "   2            178         3      0.319870          0            35\n",
       "   3            178         4      0.196717          0            40\n",
       "   4            178         4      0.129981          0            45\n",
       "   5            580         3      0.812672         70            35\n",
       "   6            480         3      0.745425         20            10\n",
       "   7            480         3      0.745425         15            15\n",
       "   8            480         3      0.666741         20            20\n",
       "   9            480         3      0.745425          0            30\n",
       "   10           480         3      0.711635          0            35),\n",
       "  (188,\n",
       "   9,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (430,\n",
       "   33,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           504         3      0.894919         50            25\n",
       "   1           504         3      0.890613         35            35\n",
       "   2           504         3      0.895647         40            40\n",
       "   3           504         3      0.897500         45            45),\n",
       "  (207,\n",
       "   9,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (93,\n",
       "   76,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (408,\n",
       "   17,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (213,\n",
       "   36,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (147,\n",
       "   1,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (224,\n",
       "   11,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           201         3      0.491168         50            25\n",
       "   1           201         3      0.298637         60            30\n",
       "   2           201         3      0.501134         35            35\n",
       "   3           201         3      0.502526         40            40\n",
       "   4           201         4      0.374915         45            45\n",
       "   5           275         5      0.536299        105            35\n",
       "   6           275         7      0.319166         90            45),\n",
       "  (322,\n",
       "   35,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           391         3      0.609954          0            45\n",
       "   1           520         3      0.116336         45            15\n",
       "   2           520         4      0.124537         40            20\n",
       "   3           520         4      0.144877         40            40\n",
       "   4           520         3      0.136041         45            45),\n",
       "  (338,\n",
       "   28,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (295,\n",
       "   3,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (105,\n",
       "   60,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (296,\n",
       "   8,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (145,\n",
       "   5,\n",
       "   10,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0            468         3      0.729076         10            10\n",
       "   1            468         7      0.591733          0            20\n",
       "   2            468         7      0.551262          0            25\n",
       "   3            468         7      0.543437          0            30\n",
       "   7            353         3      0.472949         10            10\n",
       "   8             40         4      0.272513         10            10\n",
       "   9            134         3      0.216371          5             5\n",
       "   10           134         4      0.184826          0            10),\n",
       "  (462,\n",
       "   411,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (107,\n",
       "   0,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           604         4      0.979065          0            45),\n",
       "  (58,\n",
       "   64,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           372         3      0.227643        160            40),\n",
       "  (177,\n",
       "   152,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (477,\n",
       "   368,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (105,\n",
       "   322,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (184,\n",
       "   114,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (225,\n",
       "   4,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (154,\n",
       "   0,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (331,\n",
       "   150,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (356,\n",
       "   103,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (469,\n",
       "   88,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           217         3      0.648035         90            30\n",
       "   1           217         4      0.625859         80            40\n",
       "   2           217         4      0.472672         90            45),\n",
       "  (159,\n",
       "   21,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           292         3      0.590755        225            45),\n",
       "  (324,\n",
       "   5,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (260,\n",
       "   67,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (428,\n",
       "   2,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           274         3      0.500582        200            40),\n",
       "  (486,\n",
       "   45,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           592         4      0.372497         40            40\n",
       "   1           592         4      0.319291         45            45\n",
       "   2           353         3      0.392555         40            40\n",
       "   3           353         3      0.346790         45            45),\n",
       "  (18,\n",
       "   212,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (240,\n",
       "   113,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (433,\n",
       "   9,\n",
       "   10,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0             63         3      0.261789         25            25\n",
       "   1             18         3      0.265741         40            20\n",
       "   2             18         3      0.265741         30            30\n",
       "   3             18         3      0.181402         35            35\n",
       "   4             18         3      0.479217         40            40\n",
       "   5            560         3      0.993927         35            35\n",
       "   6            233         3      0.440885         35            35\n",
       "   7            233         4      0.360026         40            40\n",
       "   8            528         3      0.610274          0            35\n",
       "   9            528         3      0.626528          0            40\n",
       "   10           528         3      0.644320          0            45),\n",
       "  (594,\n",
       "   199,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (90,\n",
       "   7,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (606,\n",
       "   452,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (28,\n",
       "   539,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (176,\n",
       "   22,\n",
       "   10,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0            117         3      0.143288         25            25\n",
       "   1            524         3      0.277297         30            30\n",
       "   2            541         4      0.537119          0            30\n",
       "   3            541         4      0.520000          0            35\n",
       "   4            541         4      0.528457          0            40\n",
       "   5            541         4      0.523494          0            45\n",
       "   6             56         5      0.398448          0            45\n",
       "   7             46         4      0.306906          0            40\n",
       "   9            584         4      0.809046          0            40\n",
       "   10           584         4      0.812090          0            45),\n",
       "  (88,\n",
       "   30,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           434         4      0.950440         45            45\n",
       "   1           599         3      0.285351         80            40\n",
       "   2           249         3      0.915332        120            40),\n",
       "  (40,\n",
       "   84,\n",
       "   10,\n",
       "   Empty DataFrame\n",
       "   Columns: [neighbour_id, n_common, pearson_corr, timebin_i, timebin_size]\n",
       "   Index: []),\n",
       "  (36,\n",
       "   36,\n",
       "   10,\n",
       "      neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0           368         3      0.985095        435            15\n",
       "   1           368         3      0.985095        425            25\n",
       "   2           368         3      0.985095        420            30\n",
       "   3           368         3      0.985905        420            35\n",
       "   4           368         3      0.985095        405            45),\n",
       "  (434,\n",
       "   163,\n",
       "   10,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0             68         3      0.289635        105            35\n",
       "   1            219         3      0.962271        240            30\n",
       "   2            219         3      0.962075        240            40\n",
       "   3            219         3      0.962271        225            45\n",
       "   4            376         3      0.370524         90            15\n",
       "   5            376         4      0.203452         90            30\n",
       "   6            376         4      0.273347         70            35\n",
       "   7            376         5      0.133206         80            40\n",
       "   8            376         5      0.128139         90            45\n",
       "   9            480         4      0.951334        650            25\n",
       "   10           480         3      0.994577        630            35)],\n",
       " 'user_predictions': defaultdict(<function __main__.AnalizeTimebinSimilarity.analize_timebin_prediction.<locals>.<lambda>()>,\n",
       "             {129: defaultdict(dict,\n",
       "                          {1517: {'actual': 3.5,\n",
       "                            'pearson_predictions': 3.576668527454281,\n",
       "                            'PCC_static_weighted_predictions': 3.9824863932895447,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.004270988047972,\n",
       "                            'PCC_CIC_predictions': 4.017565423909658,\n",
       "                            'mutual_information_predictions': 3.480158470353568,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              564: defaultdict(dict,\n",
       "                          {69278: {'actual': 4.5,\n",
       "                            'pearson_predictions': -1.8545566518154755,\n",
       "                            'PCC_static_weighted_predictions': 3.354440023364451,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.3596522431152853,\n",
       "                            'PCC_CIC_predictions': 3.3617544227139065,\n",
       "                            'mutual_information_predictions': 2.8675975759267676,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              37: defaultdict(dict,\n",
       "                          {349: {'actual': 4.0,\n",
       "                            'pearson_predictions': 4.2116003542261975,\n",
       "                            'PCC_static_weighted_predictions': 4.247261232779157,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.247261232779157,\n",
       "                            'PCC_CIC_predictions': 4.247261232779157,\n",
       "                            'mutual_information_predictions': 3.5138267563905154,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              349: defaultdict(dict,\n",
       "                          {230: {'actual': 3.0,\n",
       "                            'pearson_predictions': 4.962739438467594,\n",
       "                            'PCC_static_weighted_predictions': 4.962739438467594,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.962739438467594,\n",
       "                            'PCC_CIC_predictions': 4.962739438467594,\n",
       "                            'mutual_information_predictions': 3.5335834182153367,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              31: defaultdict(dict,\n",
       "                          {605: {'actual': 4.0,\n",
       "                            'pearson_predictions': 0,\n",
       "                            'PCC_static_weighted_predictions': 0,\n",
       "                            'PCC_dynamic_weighted_predictions': 0,\n",
       "                            'PCC_CIC_predictions': 0,\n",
       "                            'mutual_information_predictions': 2.86357945449146,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              511: defaultdict(dict,\n",
       "                          {112623: {'actual': 4.0,\n",
       "                            'pearson_predictions': 4.315576139012092,\n",
       "                            'PCC_static_weighted_predictions': 4.422160953458384,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.422160953458384,\n",
       "                            'PCC_CIC_predictions': 4.422160953458384,\n",
       "                            'mutual_information_predictions': 3.027767241228639,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              207: defaultdict(dict,\n",
       "                          {2384: {'actual': 0.5,\n",
       "                            'pearson_predictions': 0.8015026069155118,\n",
       "                            'PCC_static_weighted_predictions': 3.2408243080625754,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.2408243080625754,\n",
       "                            'PCC_CIC_predictions': 3.2408243080625754,\n",
       "                            'mutual_information_predictions': 2.9095210567429217,\n",
       "                            'timebin_predictions': 0},\n",
       "                           2858: {'actual': 5.0,\n",
       "                            'pearson_predictions': 4.2991442555829344,\n",
       "                            'PCC_static_weighted_predictions': 4.321215203968517,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.321215203968516,\n",
       "                            'PCC_CIC_predictions': 4.321215203968516,\n",
       "                            'mutual_information_predictions': 4.27229211947233,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              566: defaultdict(dict,\n",
       "                          {432: {'actual': 2.0,\n",
       "                            'pearson_predictions': 2.9463381078944693,\n",
       "                            'PCC_static_weighted_predictions': 2.930308977204053,\n",
       "                            'PCC_dynamic_weighted_predictions': 2.923726120365733,\n",
       "                            'PCC_CIC_predictions': 2.9266450381759688,\n",
       "                            'mutual_information_predictions': 2.0752216560539787,\n",
       "                            'timebin_predictions': 2.560652366308529}}),\n",
       "              124: defaultdict(dict,\n",
       "                          {3949: {'actual': 5.0,\n",
       "                            'pearson_predictions': 4.744884832185782,\n",
       "                            'PCC_static_weighted_predictions': 4.677486362216794,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.6781446812035,\n",
       "                            'PCC_CIC_predictions': 4.677486362216794,\n",
       "                            'mutual_information_predictions': 3.4955345135988862,\n",
       "                            'timebin_predictions': 3.8463224261660875}}),\n",
       "              307: defaultdict(dict,\n",
       "                          {434: {'actual': 1.0,\n",
       "                            'pearson_predictions': 2.2207093612826503,\n",
       "                            'PCC_static_weighted_predictions': 2.1295153975130283,\n",
       "                            'PCC_dynamic_weighted_predictions': 2.123202172716658,\n",
       "                            'PCC_CIC_predictions': 2.1751778911640978,\n",
       "                            'mutual_information_predictions': 3.198440063949322,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              504: defaultdict(dict,\n",
       "                          {1302: {'actual': 3.0,\n",
       "                            'pearson_predictions': 4.515745485428675,\n",
       "                            'PCC_static_weighted_predictions': 4.39409071114727,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.3962655422696875,\n",
       "                            'PCC_CIC_predictions': 4.397167032321967,\n",
       "                            'mutual_information_predictions': 3.0118513091609875,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              191: defaultdict(dict,\n",
       "                          {608: {'actual': 5.0,\n",
       "                            'pearson_predictions': 0,\n",
       "                            'PCC_static_weighted_predictions': 0,\n",
       "                            'PCC_dynamic_weighted_predictions': 0,\n",
       "                            'PCC_CIC_predictions': 0,\n",
       "                            'mutual_information_predictions': 3.860806055215808,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              371: defaultdict(dict,\n",
       "                          {3000: {'actual': 5.0,\n",
       "                            'pearson_predictions': 5.250698222020951,\n",
       "                            'PCC_static_weighted_predictions': 5.328742939825201,\n",
       "                            'PCC_dynamic_weighted_predictions': 5.328308785809115,\n",
       "                            'PCC_CIC_predictions': 5.3287429398252,\n",
       "                            'mutual_information_predictions': 3.64489372197561,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              146: defaultdict(dict,\n",
       "                          {410: {'actual': 3.5,\n",
       "                            'pearson_predictions': 2.814430468997951,\n",
       "                            'PCC_static_weighted_predictions': 2.8279116820890016,\n",
       "                            'PCC_dynamic_weighted_predictions': 2.839046473249769,\n",
       "                            'PCC_CIC_predictions': 2.8279116820890016,\n",
       "                            'mutual_information_predictions': 3.0719708555343384,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              72: defaultdict(dict,\n",
       "                          {2634: {'actual': 4.0,\n",
       "                            'pearson_predictions': 0,\n",
       "                            'PCC_static_weighted_predictions': 0,\n",
       "                            'PCC_dynamic_weighted_predictions': 0,\n",
       "                            'PCC_CIC_predictions': 0,\n",
       "                            'mutual_information_predictions': 0,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              121: defaultdict(dict,\n",
       "                          {434: {'actual': 3.0,\n",
       "                            'pearson_predictions': 3.03153500696788,\n",
       "                            'PCC_static_weighted_predictions': 2.901169131206337,\n",
       "                            'PCC_dynamic_weighted_predictions': 2.900735206762951,\n",
       "                            'PCC_CIC_predictions': 2.901169131206337,\n",
       "                            'mutual_information_predictions': 2.9533430192516903,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              298: defaultdict(dict,\n",
       "                          {81156: {'actual': 3.0,\n",
       "                            'pearson_predictions': 2.591313702711144,\n",
       "                            'PCC_static_weighted_predictions': 2.591313702711144,\n",
       "                            'PCC_dynamic_weighted_predictions': 2.7407799485902054,\n",
       "                            'PCC_CIC_predictions': 2.7424649080191896,\n",
       "                            'mutual_information_predictions': 3.904671476595681,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              166: defaultdict(dict,\n",
       "                          {3114: {'actual': 4.5,\n",
       "                            'pearson_predictions': 4.526316774127184,\n",
       "                            'PCC_static_weighted_predictions': 4.595070461566126,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.631918086643549,\n",
       "                            'PCC_CIC_predictions': 4.660589407652013,\n",
       "                            'mutual_information_predictions': 3.7893269171912536,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              524: defaultdict(dict,\n",
       "                          {1200: {'actual': 5.0,\n",
       "                            'pearson_predictions': 3.0836582196498648,\n",
       "                            'PCC_static_weighted_predictions': 3.325779700357517,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.4296929215357967,\n",
       "                            'PCC_CIC_predictions': 3.4296929215357963,\n",
       "                            'mutual_information_predictions': 3.596409450123784,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              445: defaultdict(dict,\n",
       "                          {5618: {'actual': 4.5,\n",
       "                            'pearson_predictions': 4.731930923907151,\n",
       "                            'PCC_static_weighted_predictions': 4.756071012696042,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.757351692251053,\n",
       "                            'PCC_CIC_predictions': 4.758793950570134,\n",
       "                            'mutual_information_predictions': 4.018358553268107,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              162: defaultdict(dict,\n",
       "                          {527: {'actual': 5.0,\n",
       "                            'pearson_predictions': 5.911991066300726,\n",
       "                            'PCC_static_weighted_predictions': 5.845770916297139,\n",
       "                            'PCC_dynamic_weighted_predictions': 5.844430017017857,\n",
       "                            'PCC_CIC_predictions': 5.845770916297139,\n",
       "                            'mutual_information_predictions': 3.4792501626246057,\n",
       "                            'timebin_predictions': 0},\n",
       "                           515: {'actual': 5.0,\n",
       "                            'pearson_predictions': 3.3378562370523013,\n",
       "                            'PCC_static_weighted_predictions': 3.309716789841416,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.3097167898414157,\n",
       "                            'PCC_CIC_predictions': 3.3097167898414157,\n",
       "                            'mutual_information_predictions': 3.3873199326488375,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              154: defaultdict(dict,\n",
       "                          {76093: {'actual': 5.0,\n",
       "                            'pearson_predictions': 4.857251057580567,\n",
       "                            'PCC_static_weighted_predictions': 4.968347377585953,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.968347377585952,\n",
       "                            'PCC_CIC_predictions': 4.968347377585952,\n",
       "                            'mutual_information_predictions': 3.761616507868842,\n",
       "                            'timebin_predictions': 0},\n",
       "                           115617: {'actual': 5.0,\n",
       "                            'pearson_predictions': 3.4315446585593143,\n",
       "                            'PCC_static_weighted_predictions': 4.3028143687485265,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.3028143687485265,\n",
       "                            'PCC_CIC_predictions': 4.3028143687485265,\n",
       "                            'mutual_information_predictions': 3.7115762060241213,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              404: defaultdict(dict,\n",
       "                          {236: {'actual': 3.0,\n",
       "                            'pearson_predictions': 3.400893724984222,\n",
       "                            'PCC_static_weighted_predictions': 3.346127418934354,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.3489495717938076,\n",
       "                            'PCC_CIC_predictions': 3.346127418934354,\n",
       "                            'mutual_information_predictions': 3.621464465824702,\n",
       "                            'timebin_predictions': 3.9650271021699033}}),\n",
       "              245: defaultdict(dict,\n",
       "                          {3196: {'actual': 3.0,\n",
       "                            'pearson_predictions': 0,\n",
       "                            'PCC_static_weighted_predictions': 0,\n",
       "                            'PCC_dynamic_weighted_predictions': 0,\n",
       "                            'PCC_CIC_predictions': 0,\n",
       "                            'mutual_information_predictions': 3.918123945462575,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              254: defaultdict(dict,\n",
       "                          {49530: {'actual': 4.0,\n",
       "                            'pearson_predictions': 4.633667984530274,\n",
       "                            'PCC_static_weighted_predictions': 4.580408682216861,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.602015862808543,\n",
       "                            'PCC_CIC_predictions': 4.602015862808543,\n",
       "                            'mutual_information_predictions': 3.834458972700798,\n",
       "                            'timebin_predictions': 3.544310364145087}}),\n",
       "              208: defaultdict(dict,\n",
       "                          {2058: {'actual': 4.0,\n",
       "                            'pearson_predictions': 2.519230769230769,\n",
       "                            'PCC_static_weighted_predictions': 2.519230769230769,\n",
       "                            'PCC_dynamic_weighted_predictions': 2.519230769230769,\n",
       "                            'PCC_CIC_predictions': 2.519230769230769,\n",
       "                            'mutual_information_predictions': 3.598938021998659,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              300: defaultdict(dict,\n",
       "                          {2858: {'actual': 5.0,\n",
       "                            'pearson_predictions': 4.771150336600887,\n",
       "                            'PCC_static_weighted_predictions': 4.8726345958435715,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.871625624249902,\n",
       "                            'PCC_CIC_predictions': 4.872634595843571,\n",
       "                            'mutual_information_predictions': 3.645536341461019,\n",
       "                            'timebin_predictions': 4.445493356358191}}),\n",
       "              188: defaultdict(dict,\n",
       "                          {830: {'actual': 4.0,\n",
       "                            'pearson_predictions': 10.434032737015631,\n",
       "                            'PCC_static_weighted_predictions': 4.673845090539863,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.673845090539863,\n",
       "                            'PCC_CIC_predictions': 4.673845090539863,\n",
       "                            'mutual_information_predictions': 2.5003884496481943,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              430: defaultdict(dict,\n",
       "                          {4447: {'actual': 3.0,\n",
       "                            'pearson_predictions': 4.006236412091731,\n",
       "                            'PCC_static_weighted_predictions': 4.001437030810519,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.9843369776940434,\n",
       "                            'PCC_CIC_predictions': 3.9843369776940434,\n",
       "                            'mutual_information_predictions': 3.015895578387789,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              93: defaultdict(dict,\n",
       "                          {2021: {'actual': 4.0,\n",
       "                            'pearson_predictions': 2.5645218627799506,\n",
       "                            'PCC_static_weighted_predictions': 0,\n",
       "                            'PCC_dynamic_weighted_predictions': 0,\n",
       "                            'PCC_CIC_predictions': 0,\n",
       "                            'mutual_information_predictions': 3.274574165484222,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              408: defaultdict(dict,\n",
       "                          {101864: {'actual': 5.0,\n",
       "                            'pearson_predictions': 3.6031339403434712,\n",
       "                            'PCC_static_weighted_predictions': 3.717089600030488,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.5899027041051057,\n",
       "                            'PCC_CIC_predictions': 3.578252817632941,\n",
       "                            'mutual_information_predictions': 3.170749997454645,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              213: defaultdict(dict,\n",
       "                          {1: {'actual': 3.5,\n",
       "                            'pearson_predictions': 4.582775907500802,\n",
       "                            'PCC_static_weighted_predictions': 4.323361561322583,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.32187764746464,\n",
       "                            'PCC_CIC_predictions': 4.323361561322583,\n",
       "                            'mutual_information_predictions': 3.7353868962274657,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              147: defaultdict(dict,\n",
       "                          {168: {'actual': 0.5,\n",
       "                            'pearson_predictions': 0.7923943346880473,\n",
       "                            'PCC_static_weighted_predictions': 3.141269841269841,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.141269841269841,\n",
       "                            'PCC_CIC_predictions': 3.141269841269841,\n",
       "                            'mutual_information_predictions': 2.831749590635292,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              224: defaultdict(dict,\n",
       "                          {2100: {'actual': 5.0,\n",
       "                            'pearson_predictions': 3.393569655469855,\n",
       "                            'PCC_static_weighted_predictions': 3.7182464376583155,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.7182464376583155,\n",
       "                            'PCC_CIC_predictions': 3.7182464376583155,\n",
       "                            'mutual_information_predictions': 2.8383331284275757,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              322: defaultdict(dict,\n",
       "                          {2997: {'actual': 0.5,\n",
       "                            'pearson_predictions': 3.9498147460438244,\n",
       "                            'PCC_static_weighted_predictions': 4.129185412786186,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.146010993797807,\n",
       "                            'PCC_CIC_predictions': 4.17475815645618,\n",
       "                            'mutual_information_predictions': 3.5481411167695893,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              338: defaultdict(dict,\n",
       "                          {190207: {'actual': 1.5,\n",
       "                            'pearson_predictions': 0,\n",
       "                            'PCC_static_weighted_predictions': 0,\n",
       "                            'PCC_dynamic_weighted_predictions': 0,\n",
       "                            'PCC_CIC_predictions': 0,\n",
       "                            'mutual_information_predictions': 0,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              295: defaultdict(dict,\n",
       "                          {4351: {'actual': 2.5,\n",
       "                            'pearson_predictions': 3.2466464105572297,\n",
       "                            'PCC_static_weighted_predictions': 3.3411496008845707,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.3411496008845707,\n",
       "                            'PCC_CIC_predictions': 3.3411496008845707,\n",
       "                            'mutual_information_predictions': 3.1514900783669257,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              105: defaultdict(dict,\n",
       "                          {7254: {'actual': 4.0,\n",
       "                            'pearson_predictions': 4.400527681271143,\n",
       "                            'PCC_static_weighted_predictions': 4.375529667994324,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.415990797604992,\n",
       "                            'PCC_CIC_predictions': 4.439609464397387,\n",
       "                            'mutual_information_predictions': 3.820704727645425,\n",
       "                            'timebin_predictions': 0},\n",
       "                           3617: {'actual': 3.5,\n",
       "                            'pearson_predictions': 3.480897048114388,\n",
       "                            'PCC_static_weighted_predictions': 3.594616968046316,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.6226263524391125,\n",
       "                            'PCC_CIC_predictions': 3.694268850030488,\n",
       "                            'mutual_information_predictions': 2.9631945864526728,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              296: defaultdict(dict,\n",
       "                          {5618: {'actual': 4.5,\n",
       "                            'pearson_predictions': 4.7873495853604044,\n",
       "                            'PCC_static_weighted_predictions': 4.81501132420327,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.79103048588614,\n",
       "                            'PCC_CIC_predictions': 4.81501132420327,\n",
       "                            'mutual_information_predictions': 4.13096454897843,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              145: defaultdict(dict,\n",
       "                          {231: {'actual': 3.0,\n",
       "                            'pearson_predictions': 2.710186991696046,\n",
       "                            'PCC_static_weighted_predictions': 2.6010802734627294,\n",
       "                            'PCC_dynamic_weighted_predictions': 2.6010802734627294,\n",
       "                            'PCC_CIC_predictions': 2.60108027346273,\n",
       "                            'mutual_information_predictions': 2.9005372443236133,\n",
       "                            'timebin_predictions': 2.563882820975482}}),\n",
       "              462: defaultdict(dict,\n",
       "                          {2997: {'actual': 3.5,\n",
       "                            'pearson_predictions': 3.985049862769298,\n",
       "                            'PCC_static_weighted_predictions': 3.817066621954082,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.953034226001034,\n",
       "                            'PCC_CIC_predictions': 3.9979120179179226,\n",
       "                            'mutual_information_predictions': 3.6607662104425387,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              107: defaultdict(dict,\n",
       "                          {296: {'actual': 3.0,\n",
       "                            'pearson_predictions': 0,\n",
       "                            'PCC_static_weighted_predictions': 0,\n",
       "                            'PCC_dynamic_weighted_predictions': 0,\n",
       "                            'PCC_CIC_predictions': 0,\n",
       "                            'mutual_information_predictions': 4.202466098949673,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              58: defaultdict(dict,\n",
       "                          {555: {'actual': 3.0,\n",
       "                            'pearson_predictions': 3.7224800712790835,\n",
       "                            'PCC_static_weighted_predictions': 3.654634381956976,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.5997512252168145,\n",
       "                            'PCC_CIC_predictions': 3.5717671841905707,\n",
       "                            'mutual_information_predictions': 3.989174759506206,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              177: defaultdict(dict,\n",
       "                          {66934: {'actual': 4.0,\n",
       "                            'pearson_predictions': 3.642199949727506,\n",
       "                            'PCC_static_weighted_predictions': 3.5441198201869892,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.434024589894476,\n",
       "                            'PCC_CIC_predictions': 3.4950279455445425,\n",
       "                            'mutual_information_predictions': 4.006525655833124,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              477: defaultdict(dict,\n",
       "                          {6264: {'actual': 3.0,\n",
       "                            'pearson_predictions': 2.5081175038786316,\n",
       "                            'PCC_static_weighted_predictions': 2.407035233226553,\n",
       "                            'PCC_dynamic_weighted_predictions': 2.4105405264838353,\n",
       "                            'PCC_CIC_predictions': 2.4054302169672033,\n",
       "                            'mutual_information_predictions': 2.4347569326889484,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              184: defaultdict(dict,\n",
       "                          {122898: {'actual': 2.0,\n",
       "                            'pearson_predictions': 2.9867486014259748,\n",
       "                            'PCC_static_weighted_predictions': 3.1538502081240285,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.1575659103467784,\n",
       "                            'PCC_CIC_predictions': 3.1575659103467784,\n",
       "                            'mutual_information_predictions': 2.1132468628195196,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              225: defaultdict(dict,\n",
       "                          {69: {'actual': 5.0,\n",
       "                            'pearson_predictions': 4.96698258100562,\n",
       "                            'PCC_static_weighted_predictions': 0,\n",
       "                            'PCC_dynamic_weighted_predictions': 0,\n",
       "                            'PCC_CIC_predictions': 0,\n",
       "                            'mutual_information_predictions': 3.809160293398656,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              331: defaultdict(dict,\n",
       "                          {1136: {'actual': 4.5,\n",
       "                            'pearson_predictions': 4.361510404359655,\n",
       "                            'PCC_static_weighted_predictions': 4.3564734349148395,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.383857388773437,\n",
       "                            'PCC_CIC_predictions': 4.444263803490239,\n",
       "                            'mutual_information_predictions': 3.7129717995526277,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              356: defaultdict(dict,\n",
       "                          {1641: {'actual': 4.5,\n",
       "                            'pearson_predictions': 4.413615603134142,\n",
       "                            'PCC_static_weighted_predictions': 4.298035191103839,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.377197802618127,\n",
       "                            'PCC_CIC_predictions': 4.386700119827638,\n",
       "                            'mutual_information_predictions': 3.782176121158929,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              469: defaultdict(dict,\n",
       "                          {10: {'actual': 2.0,\n",
       "                            'pearson_predictions': 3.1979436322606896,\n",
       "                            'PCC_static_weighted_predictions': 3.18724908454568,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.0657364252752126,\n",
       "                            'PCC_CIC_predictions': 2.7162874827303085,\n",
       "                            'mutual_information_predictions': 3.54493774510845,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              159: defaultdict(dict,\n",
       "                          {377: {'actual': 3.0,\n",
       "                            'pearson_predictions': 3.7503702910337537,\n",
       "                            'PCC_static_weighted_predictions': 3.594375862275454,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.6172206898987502,\n",
       "                            'PCC_CIC_predictions': 3.594375862275454,\n",
       "                            'mutual_information_predictions': 3.0654445204094745,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              324: defaultdict(dict,\n",
       "                          {3949: {'actual': 3.0,\n",
       "                            'pearson_predictions': 3.4723921902672763,\n",
       "                            'PCC_static_weighted_predictions': 3.589222575656391,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.589222575656391,\n",
       "                            'PCC_CIC_predictions': 3.589222575656391,\n",
       "                            'mutual_information_predictions': 4.060947698306035,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              260: defaultdict(dict,\n",
       "                          {6713: {'actual': 2.5,\n",
       "                            'pearson_predictions': 4.2045460559309795,\n",
       "                            'PCC_static_weighted_predictions': 4.182526562313876,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.185570451687027,\n",
       "                            'PCC_CIC_predictions': 4.185570451687027,\n",
       "                            'mutual_information_predictions': 4.272429050343538,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              428: defaultdict(dict,\n",
       "                          {145: {'actual': 2.5,\n",
       "                            'pearson_predictions': 2.7043600278373585,\n",
       "                            'PCC_static_weighted_predictions': 2.6897133984672594,\n",
       "                            'PCC_dynamic_weighted_predictions': 2.4739931325168527,\n",
       "                            'PCC_CIC_predictions': 2.4656871950949277,\n",
       "                            'mutual_information_predictions': 3.257457014273119,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              486: defaultdict(dict,\n",
       "                          {198: {'actual': 2.0,\n",
       "                            'pearson_predictions': 4.298652578210026,\n",
       "                            'PCC_static_weighted_predictions': 4.379259193450859,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.379259193450859,\n",
       "                            'PCC_CIC_predictions': 4.379259193450859,\n",
       "                            'mutual_information_predictions': 3.1923848300158473,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              18: defaultdict(dict,\n",
       "                          {4327: {'actual': 4.0,\n",
       "                            'pearson_predictions': 4.463949205215154,\n",
       "                            'PCC_static_weighted_predictions': 4.265586644646406,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.412632031266529,\n",
       "                            'PCC_CIC_predictions': 4.458412839408524,\n",
       "                            'mutual_information_predictions': 3.864399655033321,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              240: defaultdict(dict,\n",
       "                          {761: {'actual': 3.0,\n",
       "                            'pearson_predictions': 2.696169565267434,\n",
       "                            'PCC_static_weighted_predictions': 0,\n",
       "                            'PCC_dynamic_weighted_predictions': 0,\n",
       "                            'PCC_CIC_predictions': 0,\n",
       "                            'mutual_information_predictions': 2.412643319594554,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              433: defaultdict(dict,\n",
       "                          {91529: {'actual': 4.5,\n",
       "                            'pearson_predictions': 3.685003572524246,\n",
       "                            'PCC_static_weighted_predictions': 3.73992870810823,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.73992870810823,\n",
       "                            'PCC_CIC_predictions': 3.73992870810823,\n",
       "                            'mutual_information_predictions': 3.4946753827193455,\n",
       "                            'timebin_predictions': 3.952781354966789}}),\n",
       "              594: defaultdict(dict,\n",
       "                          {186: {'actual': 3.5,\n",
       "                            'pearson_predictions': 3.8039377603166935,\n",
       "                            'PCC_static_weighted_predictions': 3.809143041952346,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.8140047686917047,\n",
       "                            'PCC_CIC_predictions': 3.8080169873195007,\n",
       "                            'mutual_information_predictions': 2.587023238374862,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              90: defaultdict(dict,\n",
       "                          {805: {'actual': 3.0,\n",
       "                            'pearson_predictions': 6.4511001311282286,\n",
       "                            'PCC_static_weighted_predictions': 4.7090235185084515,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.7090235185084515,\n",
       "                            'PCC_CIC_predictions': 4.7090235185084515,\n",
       "                            'mutual_information_predictions': 3.5328809132765735,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              606: defaultdict(dict,\n",
       "                          {3626: {'actual': 3.0,\n",
       "                            'pearson_predictions': 3.149445762736044,\n",
       "                            'PCC_static_weighted_predictions': 3.149445762736044,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.149445762736044,\n",
       "                            'PCC_CIC_predictions': 3.149445762736044,\n",
       "                            'mutual_information_predictions': 0,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              28: defaultdict(dict,\n",
       "                          {70286: {'actual': 4.5,\n",
       "                            'pearson_predictions': 3.396859742532058,\n",
       "                            'PCC_static_weighted_predictions': 3.396859742532058,\n",
       "                            'PCC_dynamic_weighted_predictions': 3.5792575874664934,\n",
       "                            'PCC_CIC_predictions': 3.639786174271783,\n",
       "                            'mutual_information_predictions': 3.499401337954653,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              176: defaultdict(dict,\n",
       "                          {454: {'actual': 4.0,\n",
       "                            'pearson_predictions': 4.242871681912612,\n",
       "                            'PCC_static_weighted_predictions': 4.19264004366542,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.193231756138723,\n",
       "                            'PCC_CIC_predictions': 4.19264004366542,\n",
       "                            'mutual_information_predictions': 3.1162800595530866,\n",
       "                            'timebin_predictions': 4.1211313998700465}}),\n",
       "              88: defaultdict(dict,\n",
       "                          {4963: {'actual': 4.5,\n",
       "                            'pearson_predictions': 4.5042200204904335,\n",
       "                            'PCC_static_weighted_predictions': 4.599920652376302,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.5974274059787845,\n",
       "                            'PCC_CIC_predictions': 4.599920652376302,\n",
       "                            'mutual_information_predictions': 3.555990915434589,\n",
       "                            'timebin_predictions': 4.0}}),\n",
       "              40: defaultdict(dict,\n",
       "                          {383: {'actual': 3.0,\n",
       "                            'pearson_predictions': 0,\n",
       "                            'PCC_static_weighted_predictions': 0,\n",
       "                            'PCC_dynamic_weighted_predictions': 0,\n",
       "                            'PCC_CIC_predictions': 0,\n",
       "                            'mutual_information_predictions': 3.0704614097138343,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              36: defaultdict(dict,\n",
       "                          {548: {'actual': 3.0,\n",
       "                            'pearson_predictions': 1.7296875438770942,\n",
       "                            'PCC_static_weighted_predictions': 1.7911158493248047,\n",
       "                            'PCC_dynamic_weighted_predictions': 1.7911158493248047,\n",
       "                            'PCC_CIC_predictions': 1.7911158493248047,\n",
       "                            'mutual_information_predictions': 2.6533052560557784,\n",
       "                            'timebin_predictions': 0}}),\n",
       "              434: defaultdict(dict,\n",
       "                          {1517: {'actual': 4.0,\n",
       "                            'pearson_predictions': 4.3771042169436285,\n",
       "                            'PCC_static_weighted_predictions': 4.237059188283805,\n",
       "                            'PCC_dynamic_weighted_predictions': 4.233752427707234,\n",
       "                            'PCC_CIC_predictions': 4.266283322455674,\n",
       "                            'mutual_information_predictions': 3.6095188766455886,\n",
       "                            'timebin_predictions': 3.619892311043738}})})}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimebinPredict.predict_movie(movielens.ratings, 448, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimebinSimilarity.get_knn(movielens.ratings, 448, 10, 1863-43, 43, 5,50,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

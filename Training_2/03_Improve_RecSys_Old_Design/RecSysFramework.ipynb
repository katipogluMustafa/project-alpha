{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This projects aims to provide a framework like API in order to make high quality recommendations through various methods and also provide metrics in order to measure the results in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example usage of the framework is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose a data set.\n",
    "2. Choose a Similarity Measure\n",
    "3. Provide Hyperparameters\n",
    "4. Choose performance metrics\n",
    "5. Get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Framework Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pearson Correlation         (Linear Similarity)\n",
    "* Mutual Information          (Non-Linear Similarity)\n",
    "* Timebin-Based Neighbourhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed dataset analysis are provided as extra notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Movielens 100k\n",
    "* Movielens 1M\n",
    "* Netflix Prize (This version not exactly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy\n",
    "* Balanced Accuracy\n",
    "* Informedness\n",
    "* Markedness\n",
    "* F1\n",
    "* MCC\n",
    "* Precision\n",
    "* Recall\n",
    "* Specificity\n",
    "* NPV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features In Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Refactor The Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implement Timebin-Based Neighbourhood With Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add Deep Learning Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shortucts can be used to navigate to the related code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Time Constraint](#TimeConstraint)\n",
    "\n",
    "* [Accuracy](#Accuracy)\n",
    "\n",
    "* [Dataset](#Dataset)\n",
    "\n",
    "\n",
    "* [Dataset Operator](#DatasetOperator)\n",
    "  \n",
    "  * [Dataset User Operator](#DatasetUserOperator)\n",
    "\n",
    "  * [Dataset Movie Operator](#DatasetMovieOperator)\n",
    "\n",
    "\n",
    "* [Similarity Measure](#SimilarityMeasure)\n",
    "    \n",
    "  * [Pearson Similarity](#PearsonSimilarity)\n",
    "  \n",
    "  * [Timebin Similarity](#TimebinSimilarity)\n",
    "  \n",
    "\n",
    "* [Predict](#Predict)\n",
    "\n",
    "  * [Pearson Predict](#PearsonPredict)\n",
    "  \n",
    "  * [Timebin Predict](#TimebinPredict)\n",
    "\n",
    "\n",
    "* [Analize Timebin Similarity](#AnalizeTimebinSimilarity)\n",
    "\n",
    "* [In Progress](#InProgress)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measure Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing The Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TimeConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeConstraint:\n",
    "    \"\"\"\n",
    "    TimeConstraint is a constraint on the timestamp of the movie ratings.\n",
    "    We classify a TimeConstraint as either max_time_constraint or time_bin_constraint.\n",
    "    max_time_constraint is used to simulate real life in which we do not know the future but all the data up until one point in time.\n",
    "    time_bin_constraint is used to grab a portion of a time interval where starting and ending points are strictly defined and data is well known.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, end_dt, start_dt=None):\n",
    "        \"\"\"\n",
    "        When end_dt is only given, system will have a max time constraint only.\n",
    "        When end_dt and start_dt are given, system will have a time_bin_constraint.\n",
    "        \n",
    "        :param end_dt: The ending time boundary.\n",
    "        :param start_dt: The starting time boundary.\n",
    "            Always set start_dt to None if you change the object from time_bin to max_limit.\n",
    "        \"\"\"\n",
    "        self.end_dt = end_dt\n",
    "        self.start_dt = start_dt\n",
    "\n",
    "    def is_valid_time_bin(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether this TimeConstraint object represents a valid time bin.\n",
    "        \"\"\"\n",
    "        if self.is_time_bin() and (self._end_dt > self._start_dt):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_valid_max_limit(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether this TimeConstraint represents a valid max time limit.\n",
    "        \"\"\"\n",
    "        if (self._end_dt is not None) and (self._start_dt is None):\n",
    "            return True\n",
    "\n",
    "    def is_time_bin(self) -> bool:\n",
    "        if (self._start_dt is not None) and (self._end_dt is not None):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Comparing TimeConstraints\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return self._start_dt == other.start_dt and self._end_dt == other.end_dt\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return self._start_dt != other.start_dt or self._end_dt != other.end_dt\n",
    "\n",
    "    # Properties\n",
    "\n",
    "    @property\n",
    "    def end_dt(self):\n",
    "        return self._end_dt\n",
    "\n",
    "    @end_dt.setter\n",
    "    def end_dt(self, value):\n",
    "        self._end_dt = value\n",
    "\n",
    "    @property\n",
    "    def start_dt(self):\n",
    "        return self._start_dt\n",
    "\n",
    "    @start_dt.setter\n",
    "    def start_dt(self, value):\n",
    "        self._start_dt = value\n",
    "\n",
    "    # Printing TimeConstraints\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"(start = {self._start_dt}, end= {self._end_dt})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"(start = {self._start_dt}, end= {self._end_dt})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy currently only supports ratings in between 0.5 and 5 with 0.5 increments.\n",
    "\n",
    "* Add min_rating - max_rating and increment parameters to accuracy class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    \"\"\"\n",
    "    Accuracy class provides deffirent metrics in order to measure accuracy of our analysis.\n",
    "    \n",
    "    Supported Measures:\n",
    "    rmse, accuracy, balanced accuracy, informedness, markedness, \n",
    "    f1, mcc, precision, recall, specificity, NPV and \n",
    "    other threshold measures where we round ratings less than 3.5 to min rating, upper to max rating and use supported measures on this data.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def rmse(predictions) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Root Mean Square Error of given list or Dataframe of (prediction, actual) data.\n",
    "        \n",
    "        In case rmse value is found 0, it is returned as 0.001 to differentiate between successfull rmse\n",
    "        calculation and erroneous calculations where no prediction data is provided.\n",
    "        \"\"\"\n",
    "        \n",
    "        # In case dataframe of predictions wher each row[0]=prediction, row[1]=actual rating\n",
    "        if type(predictions) is pd.DataFrame:\n",
    "            number_of_predictions = 0\n",
    "            sum_of_square_differences = 0.0\n",
    "            for row in predictions.itertuples(index=False):\n",
    "                prediction = row[0]\n",
    "                # In case valid prediction is made(0 is invalid, minimum 0.5 in movielens dataset)\n",
    "                if prediction != 0:\n",
    "                    # Round the ratings to the closest half or exact number\n",
    "                    # since movielens dataset only containst ratings 0.5, 1, 1.5,..., 4, 4.5, 5\n",
    "                    actual = Accuracy.half_round_rating(row[1])\n",
    "                    prediction = Accuracy.half_round_rating(prediction)\n",
    "                    \n",
    "                    sum_of_square_differences += (actual - prediction) ** 2\n",
    "                    number_of_predictions += 1\n",
    "                \n",
    "                if number_of_predictions == 0:\n",
    "                    return 0 \n",
    "                rmse_value = sum_of_square_differences / number_of_predictions\n",
    "            return rmse_value if rmse_value != 0 else 0.001\n",
    "        # In case list of predictions where each element is (prediction, actual)\n",
    "        elif type(predictions) is list:\n",
    "            number_of_predictions = 0\n",
    "            sum_of_square_differences = 0.0\n",
    "            for prediction, actual in predictions:\n",
    "                if prediction != 0:                  # if the prediction is valid\n",
    "                    actual = Accuracy.half_round_rating(actual)\n",
    "                    prediction = Accuracy.half_round_rating(prediction)\n",
    "                    \n",
    "                    sum_of_square_differences += (actual - prediction) ** 2\n",
    "                    number_of_predictions += 1\n",
    "                \n",
    "            if number_of_predictions == 0:\n",
    "                return 0\n",
    "        \n",
    "            rmse_value = sum_of_square_differences / number_of_predictions \n",
    "            return rmse_value if rmse_value != 0 else 0.001    \n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_accuracy(predictions) -> float:\n",
    "        \"\"\"\n",
    "        Threshold accuracy is the rate of sucessful prediction when we round \n",
    "        ratings between 0.5 and 3.5 to the lowest rating(0.5) ,\n",
    "        ratings between 3.5 and 5 to the highest rating(5)\n",
    "        \n",
    "        Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if type(predictions) is pd.DataFrame:\n",
    "            number_of_predictions = 0\n",
    "            number_of_hit = 0\n",
    "            for row in predictions.itertuples(index=False):\n",
    "                # row[1] : actual rating, row[0] : prediction\n",
    "                prediction = row[0]\n",
    "                if prediction != 0:\n",
    "                    actual = Accuracy.threshold_round_rating(row[1])\n",
    "                    prediction = Accuracy.threshold_round_rating(prediction)\n",
    "                    \n",
    "                    if actual == prediction:\n",
    "                        number_of_hit += 1\n",
    "                    number_of_predictions += 1\n",
    "            return number_of_hit / number_of_predictions if number_of_predictions != 0 else 0\n",
    "        elif type(predictions) is list:            \n",
    "            number_of_predictions = 0\n",
    "            number_of_hit = 0\n",
    "            for prediction, actual in predictions:\n",
    "                if prediction != 0:\n",
    "                    actual = Accuracy.threshold_round_rating(actual)\n",
    "                    prediction = Accuracy.threshold_round_rating(prediction)\n",
    "                    \n",
    "                    if actual == prediction:\n",
    "                        number_of_hit += 1\n",
    "                        \n",
    "                    number_of_predictions += 1\n",
    "            return number_of_hit / number_of_predictions if number_of_predictions != 0 else 0\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_analize(predictions):\n",
    "        \"\"\"\n",
    "        Analize the threshold predictions with all metrics found in the Accuracy class.\n",
    "        \"\"\"\n",
    "        \n",
    "        TP, FN, FP, TN = Accuracy.threshold_confusion_matrix(predictions)\n",
    "        precision = Accuracy.precision(TP, FP)     # also called PPV\n",
    "        recall = Accuracy.recall(TP, FN)           # also called TPR\n",
    "        specificity = Accuracy.specificity(FP, TN) # also called TNR\n",
    "        NPV = Accuracy.negative_predictive_value(FN, TN)\n",
    "        \n",
    "        accuracy = Accuracy.accuracy(TP, FN, FP, TN)\n",
    "        balanced_accuracy = Accuracy.balanced_accuracy(TPR=recall, TNR=specificity)\n",
    "        informedness = Accuracy.informedness(TPR=recall, TNR=specificity)\n",
    "        markedness = Accuracy.markedness(PPV=precision, NPV=NPV)\n",
    "        \n",
    "        f1 = Accuracy.f_measure(precision, recall)\n",
    "        mcc = Accuracy.mcc(TP, FN, FP, TN)\n",
    "        \n",
    "                \n",
    "        output = {\n",
    "                  \"accuracy\"         :round(accuracy, 3),\n",
    "                  \"balanced_accuracy\":round(balanced_accuracy, 3),\n",
    "                  \"informedness\"     :round(informedness, 3),\n",
    "                  \"markedness\"       :round(markedness, 3),\n",
    "                  \"f1\"               :round(f1, 3),\n",
    "                  \"mcc\"              :round(mcc, 3),\n",
    "                  \"precision\"        :round(precision, 3),\n",
    "                  \"recall\"           :round(recall, 3),\n",
    "                  \"specificity\"      :round(specificity, 3),\n",
    "                  \"NPV\"              :round(NPV, 3)\n",
    "                 }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def analize(predictions):\n",
    "        \"\"\"\n",
    "        Analize the threshold predictions with all metrics found in the Accuracy class.\n",
    "        \n",
    "        https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "        \n",
    "        Returns analysis for each class as list\n",
    "        :return: accuracy, balanced_accuracy, informedness, markedness, f1, mcc, precision, recall, specificity, NPV\n",
    "        \"\"\"\n",
    "        confusion_mtr = Accuracy.confusion_matrix(predictions)\n",
    "        \n",
    "        # Use macro averaging (https://stats.stackexchange.com/questions/187768/matthews-correlation-coefficient-with-multi-class)\n",
    "        precision = [0] * 10   # 10 is the number of classes found \n",
    "        recall = [0] * 10      # 0.5 -> Class 0 , 1 -> Class 1, 1.5 -> Class 2 .... \n",
    "        specificity = [0] * 10\n",
    "        NPV = [0] * 10 \n",
    "        \n",
    "        accuracy = [0] * 10 \n",
    "        balanced_accuracy = [0] * 10 \n",
    "        informedness = [0] * 10 \n",
    "        markedness = [0] * 10 \n",
    "        \n",
    "        f1 = [0] * 10 \n",
    "        mcc = [0] * 10\n",
    "        \n",
    "        for i in range(0, 10): # For Each Class\n",
    "            TP, FN, FP, TN = Accuracy.confusion_matrix_one_against_all(confusion_mtr, i)\n",
    "            precision[i] = Accuracy.precision(TP, FP)     # also called PPV\n",
    "            recall[i] = Accuracy.recall(TP, FN)           # also called TPR\n",
    "            specificity[i] = Accuracy.specificity(FP, TN) # also called TNR\n",
    "            NPV[i] = Accuracy.negative_predictive_value(FN, TN)\n",
    "\n",
    "            accuracy[i] = Accuracy.accuracy(TP, FN, FP, TN)\n",
    "            balanced_accuracy[i] = Accuracy.balanced_accuracy(TPR=recall[i], TNR=specificity[i])\n",
    "            informedness[i] = Accuracy.informedness(TPR=recall[i], TNR=specificity[i])\n",
    "            markedness[i] = Accuracy.markedness(PPV=precision[i], NPV=NPV[i])\n",
    "\n",
    "            f1[i] = Accuracy.f_measure(precision[i], recall[i])\n",
    "            mcc[i] = Accuracy.mcc(TP, FN, FP, TN)\n",
    "        \n",
    "        output = {\n",
    "                  \"accuracy\"         :Accuracy.round_list_elements(accuracy, 3),\n",
    "                  \"balanced_accuracy\":Accuracy.round_list_elements(balanced_accuracy, 3),\n",
    "                  \"informedness\"     :Accuracy.round_list_elements(informedness, 3),\n",
    "                  \"markedness\"       :Accuracy.round_list_elements(markedness, 3),\n",
    "                  \"f1\"               :Accuracy.round_list_elements(f1, 3),\n",
    "                  \"mcc\"              :Accuracy.round_list_elements(mcc, 3),\n",
    "                  \"precision\"        :Accuracy.round_list_elements(precision, 3),\n",
    "                  \"recall\"           :Accuracy.round_list_elements(recall, 3),\n",
    "                  \"specificity\"      :Accuracy.round_list_elements(specificity, 3),\n",
    "                  \"NPV\"              :Accuracy.round_list_elements(NPV, 3)\n",
    "                 }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def round_list_elements(l, precision):\n",
    "        \"\"\"\n",
    "        :param l: list of floats\n",
    "        :param precision: precision after dot\n",
    "        \"\"\"\n",
    "        return [ round(x, precision) for x in l ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy_multi_class(confusion_mtr):\n",
    "        length = len(confusion_mtr)\n",
    "        numenator = 0\n",
    "        denuminator = 0 \n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                temp = confusion_mtr[i][j]\n",
    "                denuminator += temp\n",
    "                if i == j:\n",
    "                    numenator += temp\n",
    "        return numenator / denominator\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(TP, FN, FP, TN):\n",
    "        return  (TP + TN) / (TP + FN + FP + TN)\n",
    "    \n",
    "    @staticmethod\n",
    "    def balanced_accuracy(TPR, TNR):\n",
    "        \"\"\"\n",
    "        :param TPR : True Positive Rate or recall or sensitivity\n",
    "        :param TNR : True Negative Rate or specificity or  selectivity\n",
    "        \"\"\"\n",
    "        return (TPR + TNR) / 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def informedness(TPR, TNR):\n",
    "        \"\"\"\n",
    "        :param TPR : True Positive Rate or recall or sensitivity\n",
    "        :param TNR : True Negative Rate or specificity or  selectivity\n",
    "        \"\"\"\n",
    "        return TPR + TNR - 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def markedness(PPV, NPV):\n",
    "        \"\"\"\n",
    "        :param PPV: Positive Predictive Value also known as precision\n",
    "        :param NPV: Negative Predictive Value\n",
    "        \"\"\"\n",
    "        return PPV + NPV - 1 \n",
    "    \n",
    "    @staticmethod\n",
    "    def precision(TP, FP):\n",
    "        \"\"\"\n",
    "        Also called as precision or positive predictive value (PPV)\n",
    "        \n",
    "        Precision = TP / (TP + FP) for binary class\n",
    "        Precision = TP / (All Predicted Positive) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TP + FP\n",
    "        return TP / denuminator if denuminator != 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def negative_predictive_value(FN, TN):\n",
    "        \"\"\"     \n",
    "        NPV = TN / (TN + FN) for binary class\n",
    "        NPV = TN / (All Predicted Negative) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TN + FN\n",
    "        return TN / denuminator if denuminator != 0 else 0    \n",
    "    \n",
    "    @staticmethod\n",
    "    def recall(TP, FN):\n",
    "        \"\"\"\n",
    "        Also called as sensitivity, recall, hitrate, or true positive rate(TPR)\n",
    "        Recall = TP / (TP + FN) for binary class\n",
    "        Recall = TP / (All Actual Positive) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TP + FN\n",
    "        return TP / denuminator if denuminator != 0 else 0    \n",
    "    \n",
    "    @staticmethod\n",
    "    def specificity(FP, TN):\n",
    "        \"\"\"\n",
    "        Also called as specificity, selectivity or true negative rate (TNR)\n",
    "        specificity = TN / (TP + FN) for binary class\n",
    "        specificity = TN / (All Actual Negative) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = FP + TN \n",
    "        return TN / denuminator if denuminator != 0 else 0 \n",
    "    \n",
    "    @staticmethod\n",
    "    def f_measure(precision, recall):\n",
    "        \"\"\"\n",
    "        F-Measure is the harmonic mean of the precision and recall.\n",
    "        \"\"\"\n",
    "        sum_of_both = precision + recall\n",
    "        return (2 * precision * recall) / sum_of_both if sum_of_both != 0 else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mcc(TP, FN, FP, TN):\n",
    "        \"\"\"\n",
    "        MCC(Matthews Correlation Coefficient)\n",
    "        \"\"\"\n",
    "        # Calulate Matthews Correlation Coefficient\n",
    "        numenator   = (TP * TN) - (FP * FN) \n",
    "        denominator = (TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)\n",
    "        denominator = math.sqrt(denominator) if denominator > 0 else 0\n",
    "        return numenator / denominator if denominator != 0 else 0\n",
    "  \n",
    "    @staticmethod\n",
    "    def confusion_matrix_one_against_all(confusion_mtr, class_i):\n",
    "        \"\"\"\n",
    "        Create binary confusion matrix out of multi-class confusion matrix\n",
    "        \n",
    "        Positive Class: class_i\n",
    "        Negative Class: non class_i\n",
    "                \n",
    "        TP: True Positive   FN: False Negative\n",
    "        FP: False Positive  TN: True Negative\n",
    "        \n",
    "        \"TP of Class_1\" is all Class_1 instances that are classified as Class_1.\n",
    "        \"TN of Class_1\" is all non-Class_1 instances that are not classified as Class_1.\n",
    "        \"FP of Class_1\" is all non-Class_1 instances that are classified as Class_1.\n",
    "        \"FN of Class_1\" is all Class_1 instances that are not classified as Class_1.\n",
    "        # https://www.researchgate.net/post/How_do_you_measure_specificity_and_sensitivity_in_a_multiple_class_classification_problem\n",
    "        \n",
    "        --> Input matrix\n",
    "                 | 0 Prediction | 1 Prediction | 2 Prediction | .....\n",
    "        0 Class  |     T0       |     ..       |      ..      |\n",
    "        1 Class  |     ..       |     T1       |      ..      | \n",
    "        2 Class  |     ..       |     ..       |      T2      |\n",
    "        \n",
    "        --> Output matrix\n",
    "        \n",
    "                        | Positive Prediction | Negative Prediction\n",
    "        Positive Class  |       TP            |       FN\n",
    "        Negative Class  |       FP            |       TN\n",
    "        \n",
    "        :param confusion_mtr: 10 class confusion matrix designed for movielens\n",
    "        :param class_i: index of the class we are interested in(0-9)\n",
    "        :return: TP, FN, FP, TN\n",
    "        \"\"\"\n",
    "        length = len(confusion_mtr)\n",
    "\n",
    "        TP = confusion_mtr[class_i][class_i] \n",
    "        \n",
    "        actual_class_i_count = 0 \n",
    "        for i in range(length):  # sum of the row\n",
    "            actual_class_i_count += confusion_mtr[class_i][i]\n",
    "        FN = actual_class_i_count - TP\n",
    "        \n",
    "        predicted_class_i_count = 0\n",
    "        for i in range(length): # sum of the column\n",
    "            predicted_class_i_count += confusion_mtr[i][class_i]\n",
    "        FP = predicted_class_i_count - TP\n",
    "        \n",
    "        # sum of matrix\n",
    "        sum_of_matrix = np.sum(confusion_mtr)\n",
    "        # TN is found by summing up all values except the row and column of the class \n",
    "        TN = sum_of_matrix - predicted_class_i_count - actual_class_i_count - TP \n",
    "        \n",
    "        return TP, FN, FP, TN\n",
    "        \n",
    "    @staticmethod\n",
    "    def confusion_matrix(predictions):\n",
    "        \"\"\"\n",
    "        Create confusion matrix and then return TP, FN, FP, TN\n",
    "\n",
    "        0 Class: 0.5\n",
    "        1 Class: 1\n",
    "        2 Class: 1.5\n",
    "        3 Class: 2\n",
    "        4 Class: 2.5\n",
    "        5 Class: 3\n",
    "        6 Class: 3.5\n",
    "        7 Class: 4\n",
    "        8 Class: 4.5\n",
    "        9 Class: 5\n",
    "\n",
    "        T0: True 0\n",
    "        F0: False 0\n",
    "        T1: True 1\n",
    "        F1: False 1\n",
    "        ...\n",
    "\n",
    "                 | 0 Prediction | 1 Prediction | 2 Prediction | .....\n",
    "        0 Class  |     T0       |     ..       |      ..      |\n",
    "        1 Class  |     ..       |     T1       |      ..      | \n",
    "        2 Class  |     ..       |     ..       |      T2      |\n",
    "        ...\n",
    "        \"\"\"\n",
    "        # Create multiclass confusion matrix\n",
    "\n",
    "        conf_mtr = np.zeros( (10,10) )\n",
    "\n",
    "        for prediction in predictions:\n",
    "            predicted = Accuracy.half_round_rating(prediction[0])\n",
    "            actual    = Accuracy.half_round_rating(prediction[1])\n",
    "\n",
    "            predicted_class_index = int( (predicted * 2) - 1 )\n",
    "            actual_class_index = int( (actual * 2) - 1 )\n",
    "\n",
    "            conf_mtr[actual_class_index][predicted_class_index] += 1\n",
    "\n",
    "        return conf_mtr\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_confusion_matrix(predictions):\n",
    "        \"\"\"\n",
    "        Create confusion matrix and then return TP, FN, FP, TN\n",
    "\n",
    "        Positive Class: 5\n",
    "        Negative Class: 0.5\n",
    "\n",
    "        TP: True Positive\n",
    "        TN: True Negative\n",
    "        FP: False Positive\n",
    "        FN: False Negative\n",
    "\n",
    "                        | Positive Prediction | Negative Prediction\n",
    "        Positive Class  |       TP            |       FN\n",
    "        Negative Class  |       FP            |       TN\n",
    "\n",
    "        \"\"\"\n",
    "        # Create confusion matrix\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for prediction in predictions:\n",
    "            predicted = Accuracy.threshold_round_rating(prediction[0])\n",
    "            actual = Accuracy.threshold_round_rating(prediction[1])\n",
    "            if predicted == 5 and actual == 5:\n",
    "                TP += 1\n",
    "            elif predicted == 5 and actual == 0.5:\n",
    "                FP += 1\n",
    "            elif predicted == 0.5 and actual == 0.5:\n",
    "                TN += 1\n",
    "            elif predicted == 0.5 and actual == 5:\n",
    "                FN += 1\n",
    "        return TP, FN, FP, TN\n",
    "\n",
    "    @staticmethod\n",
    "    def half_round_rating(rating):\n",
    "        \"\"\"\n",
    "        Round ratings to the closest match in the movielens dataset\n",
    "        For ex.\n",
    "          ratings between 2 and 2.25 -> round to 2\n",
    "          ratings between 2.25 and 2.5 -> round to 2.5\n",
    "          ratings between 2.5 and 2.75 -> round to 2.5\n",
    "          ratings between 2.75 and 3 -> round to 3\n",
    "\n",
    "        \"\"\"\n",
    "        floor_value = math.floor(rating)\n",
    "        if(rating > floor_value + 0.75):\n",
    "            return floor_value + 1\n",
    "        elif(rating > floor_value + 0.5 or rating > floor_value + 0.25):\n",
    "            return floor_value + 0.5\n",
    "        else:\n",
    "            return floor_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_round_rating(rating):\n",
    "        \"\"\"\n",
    "        Round ratings to the closest match in threshold fashion\n",
    "          ratings between 0.5 and 3.5 -> round to 0.5\n",
    "          ratings between 3.5 and 5 -> round to 5\n",
    "        \"\"\"\n",
    "        if (0.5 <= rating < 3.5):\n",
    "            return 0.5\n",
    "        elif (3.5 <= rating <= 5):\n",
    "            return 5\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(ABC):\n",
    "    \"\"\"\n",
    "    Dataset class and its subclasses provides utilities in order to import datasets.\n",
    "    \n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def load():\n",
    "        \"\"\" Every subclass must provide static load method\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp'),\n",
    "                 ratings_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv',\n",
    "                 movies_col_names=('item_id', 'title', 'genres'),\n",
    "                 movies_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv',\n",
    "                 is_ratings_cached=True,\n",
    "                 is_movies_cached=True):\n",
    "        Dataset.__init__(self)\n",
    "        self.is_ratings_cached = is_ratings_cached\n",
    "        self.is_movies_cached = is_movies_cached\n",
    "        self.ratings = MovieLensDataset.load_ratings(ratings_path,\n",
    "                                                     ratings_col_names) if self.is_ratings_cached else None\n",
    "        self.movies = MovieLensDataset.load_movies(movies_path,\n",
    "                                                   movies_col_names) if self.is_movies_cached else None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_movies(movies_path,\n",
    "                    movies_col_names=('item_id', 'title', 'genres')):\n",
    "        if not os.path.isfile(movies_path) or not movies_col_names:\n",
    "            return None\n",
    "\n",
    "        # read movies\n",
    "        movies = pd.read_csv(movies_path, sep=',', header=1, names=movies_col_names)\n",
    "\n",
    "        # Extract Movie Year\n",
    "        movies['year'] = movies.title.str.extract(\"\\((\\d{4})\\)\", expand=True)\n",
    "        movies.year = pd.to_datetime(movies.year, format='%Y')\n",
    "        movies.year = movies.year.dt.year  # As there are some NaN years, resulting type will be float (decimals)\n",
    "\n",
    "        # Remove year part from the title\n",
    "        movies.title = movies.title.str[:-7]\n",
    "\n",
    "        return movies\n",
    "\n",
    "    @staticmethod\n",
    "    def load_ratings(ratings_path,\n",
    "                     ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp')):\n",
    "        if not os.path.isfile(ratings_path) or not ratings_col_names:\n",
    "            return None\n",
    "\n",
    "        # read ratings\n",
    "        ratings = pd.read_csv(ratings_path, sep=',', header=1, names=ratings_col_names)\n",
    "\n",
    "        # Convert timestamp into readable format\n",
    "        ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s', origin='unix')\n",
    "        \n",
    "        ratings.sort_values(by='timestamp', inplace=True)\n",
    "        \n",
    "        return ratings\n",
    "\n",
    "    @staticmethod\n",
    "    def create_movie_ratings(ratings, movies):\n",
    "        return pd.merge(ratings, movies, on='item_id')\n",
    "\n",
    "    @staticmethod\n",
    "    def load(ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp'),\n",
    "             ratings_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv',\n",
    "             movies_col_names=('item_id', 'title', 'genres'),\n",
    "             movies_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv'\n",
    "             ):\n",
    "        # Load movies\n",
    "        movies = MovieLensDataset.load_movies(movies_path=movies_path, movies_col_names=movies_col_names)\n",
    "        # Load ratings\n",
    "        ratings = MovieLensDataset.load_ratings(ratings_path=ratings_path, ratings_col_names=ratings_col_names)\n",
    "\n",
    "        # Merge the ratings and movies\n",
    "        movie_ratings = pd.merge(ratings, movies, on='item_id')\n",
    "\n",
    "        return movie_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DatasetOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetOperator:\n",
    "    @staticmethod\n",
    "    def apply_time_constraint(movie_ratings : pd.DataFrame, time_constraint : TimeConstraint = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        movie_ratings can be ratings or movie_ratings.\n",
    "        \"\"\"\n",
    "        if time_constraint is None:\n",
    "            return movie_ratings\n",
    "\n",
    "        if time_constraint.is_valid_max_limit():\n",
    "            return movie_ratings.loc[movie_ratings.timestamp < time_constraint.end_dt]\n",
    "        \n",
    "        if time_constraint.is_valid_time_bin():\n",
    "            return movie_ratings.loc[(movie_ratings.timestamp >= time_constraint.start_dt) & (movie_ratings.timestamp < time_constraint.end_dt)]\n",
    "        \n",
    "        return movie_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DatasetUserOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetUserOperator:\n",
    "    \"\"\"\n",
    "    Provides utility methods to get user data out of datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_users(ratings : pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Get list of unique 'user_id's\n",
    "\n",
    "        :return: the ids of the users found in movie_ratings\n",
    "        \"\"\"\n",
    "        return pd.unique(ratings['user_id'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_active_users(ratings : pd.DataFrame, n : int=10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get Users in sorted order where the first one is the one who has given most ratings.\n",
    "\n",
    "        :param n: Number of users to retrieve.\n",
    "        :return: user DataFrame with index of 'user_id' and columns of ['mean_rating', 'ratings_count'] .\n",
    "        \"\"\"\n",
    "        active_users = pd.DataFrame(ratings.groupby('user_id')['rating'].mean())\n",
    "        active_users['ratings_count'] = pd.DataFrame(ratings.groupby('user_id')['rating'].count())\n",
    "        active_users.sort_values(by=['ratings_count'], ascending=False, inplace=True)\n",
    "        active_users.columns = ['mean_rating', 'ratings_count']\n",
    "        return active_users.head(n)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_random_users(ratings : pd.DataFrame, n : int=1):\n",
    "        \"\"\"\n",
    "        Get list of random n number of 'user_id's\n",
    "\n",
    "        :param n: Number of random users\n",
    "        :return: List of random 'user_id's\n",
    "        \"\"\"\n",
    "        return random.choices(population=DatasetUserOperator.get_users(ratings), k=n)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_ratings(ratings : pd.DataFrame, user_id : int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all the ratings given by of the chosen users\n",
    "\n",
    "        :param user_id: id of the chosen user\n",
    "        :return: Ratings given by the 'user_id'\n",
    "        \"\"\"\n",
    "        return ratings.loc[ratings['user_id'] == user_id]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_avg(ratings : pd.DataFrame, user_id : int):\n",
    "        user_ratings = DatasetUserOperator.get_user_ratings(ratings, user_id)\n",
    "        return user_ratings.rating.mean() if not user_ratings.empty else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_timestamp(ratings : pd.DataFrame, user_id: int, movie_id: int):\n",
    "        \"\"\"\n",
    "        Get the timestamp of the given rating\n",
    "\n",
    "        :param user_id: the users whose rating timestamp we are searching\n",
    "        :param movie_id: id of the movie that the user gave the rating\n",
    "        :return: if found the datetime object otherwise None\n",
    "        \"\"\"\n",
    "        timestamp = ratings.loc[(ratings['user_id'] == user_id) & (ratings['item_id'] == movie_id)]\n",
    "        return timestamp.values[0, 3] if not timestamp.empty else None\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_first_timestamp(ratings : pd.DataFrame):\n",
    "        return ratings['timestamp'].min()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_avg_timestamp(ratings : pd.DataFrame, user_id: int):\n",
    "        user_ratings = DatasetUserOperator.get_user_ratings(ratings, user_id)\n",
    "        return user_ratings.timestamp.mean() if not user_ratings.empty else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_ratings_at(ratings : pd.DataFrame, user_id: int, at: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get user ratings up until the given datetime\n",
    "        :param user_id: id of the chosen user\n",
    "        :param at: only those ratings that are before this date will be taken into account\n",
    "        :return: Ratings given by the 'user_id' before given datetime\n",
    "        \"\"\"\n",
    "        return ratings.loc[(ratings['user_id'] == user_id) & (ratings.timestamp < at)]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_user_avg_at(ratings : pd.DataFrame, user_id: int, at: datetime):\n",
    "        user_ratings = DatasetUserOperator.get_user_ratings_at(ratings, user_id, at)\n",
    "        return user_ratings.rating.mean() if not user_ratings.empty else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DatasetMovieOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMovieOperator:\n",
    "    \"\"\"\n",
    "    Provides utility methods to get user data out of datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_movie(movies : pd.DataFrame, movie_id : int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get Movie Record\n",
    "\n",
    "        :return: DataFrame which contains the given 'movie_id's details. If not found empty DataFrame .\n",
    "        \"\"\"\n",
    "        return movies.loc[movies['item_id'] == movie_id]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_movies(movies : pd.DataFrame) -> list:\n",
    "        \"\"\"\n",
    "        Get list of unique movies.\n",
    "\n",
    "        :return: List of movie ids\n",
    "        \"\"\"\n",
    "        return movies['item_id'].values.tolist()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_random_movies(movies, n=10):\n",
    "        \"\"\"\n",
    "        Get list of random n number of 'item_id's or in other words the movies\n",
    "\n",
    "        :param n: Number of random movies\n",
    "        :return: List of random 'movie_id's\n",
    "        \"\"\"\n",
    "        return random.choices(population=DatasetMovieOperator.get_movies(movies), k=n)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_movies_watched(movie_ratings : pd.DataFrame, \n",
    "                           user_id: int, \n",
    "                           time_constraint: TimeConstraint = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all the movies watched by the chosen user.\n",
    "\n",
    "        :param user_id: the user that we want to get the movies he-she has watched.\n",
    "        :param time_constraint: type of the time constraint.\n",
    "        :return: DataFrame of all movies watched with 'item_id', 'rating' columns\n",
    "        \"\"\"\n",
    "        filtered_movie_ratings = DatasetOperator.apply_time_constraint(movie_ratings, time_constraint)\n",
    "        return filtered_movie_ratings.loc[filtered_movie_ratings['user_id'] == user_id]\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_movie_rating(ratings : pd.DataFrame, movie_id: int, user_id: int) -> int:\n",
    "        \"\"\"\n",
    "        Get the movie rating taken by the chosen user\n",
    "\n",
    "        :param movie_id: the movie chosen movie's id\n",
    "        :param user_id: id of the chosen user\n",
    "        :return: Rating given by user. If not found, returns 0\n",
    "        \"\"\"\n",
    "        movie_rating = ratings.loc[(ratings['user_id'] == user_id) & (ratings['item_id'] == movie_id)]\n",
    "        return movie_rating.values[0, 2] if not movie_rating.empty else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_random_movie_watched(movie_ratings : pd.DataFrame, user_id: int) -> int:\n",
    "        \"\"\"\n",
    "        Get random movie id watched.\n",
    "\n",
    "        :param user_id: User of interest\n",
    "        :return:  movie_id or item_id of the random movie watched by the user.\n",
    "                  In case non-valid user_id supplied then returns 0\n",
    "        \"\"\"\n",
    "        movies_watched = DatasetMovieOperator.get_movies_watched(movie_ratings, user_id)[['item_id', 'rating']]\n",
    "        return random.choice(movies_watched['item_id'].values.tolist()) if not movies_watched.empty else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_random_movies_watched(movie_ratings : pd.DataFrame, user_id: int, n=2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get random n movies watched by the user. Only use when n > 2\n",
    "\n",
    "        Use get_random_movie_watched if n=1 since that one 2 fold faster.\n",
    "\n",
    "        :param user_id: the user of interest\n",
    "        :param n: number of random movies to get\n",
    "        :return: DataFrame of movies, if none found then empty DataFrame\n",
    "        \"\"\"\n",
    "        movies_watched = DatasetMovieOperator.get_movies_watched(movie_ratings, user_id)[['item_id', 'rating']]\n",
    "        return random.choices(population=movies_watched['item_id'].values.tolist(),\n",
    "                              k=n) if not movies_watched.empty else movies_watched\n",
    "\n",
    "    @staticmethod\n",
    "    def get_random_movie_per_user(movie_ratings : pd.DataFrame, user_id_list : list) -> list:\n",
    "        \"\"\"\n",
    "        Get random movie for each user given in the 'user_id_list'\n",
    "\n",
    "        :param user_id_list: List of valid user_ids\n",
    "        :return: List of (user_id, movie_id) tuples\n",
    "                where each movie_id is randomly chosen from watched movies of the user_id .\n",
    "                In case any one of the user_id's supplies invalid, then the movie_id will be 0 for that user.\n",
    "        \"\"\"\n",
    "        user_movie_list = list()\n",
    "        for user_id in user_id_list:\n",
    "            user_movie_list.append((user_id, DatasetMovieOperator.get_random_movie_watched(movie_ratings, user_id)))\n",
    "        return user_movie_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimilarityMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PearsonSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PearsonSimilarity:\n",
    "    \"\"\"\n",
    "    Pearson Correlation is the classic way of giving recommendations. \n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_knn(movie_ratings:pd.DataFrame, \n",
    "                user_id:int, k:int=10, \n",
    "                min_common_elements: int = 5,\n",
    "                user_corr_matrix = None):\n",
    "        \"\"\"\n",
    "        :param user_id: the user of interest\n",
    "        :param k: number of neighbours to retrieve, None to get all\n",
    "        :param time_constraint: time constraint when choosing neighbours\n",
    "        :param user_corr_matrix: Provide correlation matrix if you want to optimize the process\n",
    "        :return: Returns the k neighbours and correlations in between them. If no neighbours found, returns None\n",
    "                 DataFrame which has 'Correlation' column and 'user_id' index.\n",
    "        \"\"\"\n",
    "        \n",
    "        if(user_corr_matrix is None):\n",
    "            user_corr_matrix = PearsonSimilarity.create_user_corrs(movie_ratings, min_common_elements)\n",
    "            # Exit if matrix is None after creation\n",
    "            if user_corr_matrix is None:\n",
    "                return None\n",
    "\n",
    "        # Get the chosen 'user_id's correlations\n",
    "        user_correlations = user_corr_matrix.get(user_id)\n",
    "        if user_correlations is None:\n",
    "            return None\n",
    "\n",
    "        # Drop any null, if found\n",
    "        user_correlations.dropna(inplace=True)\n",
    "\n",
    "        # Create A DataFrame from not-null correlations of the 'user_id'\n",
    "        users_alike = pd.DataFrame(user_correlations)\n",
    "        \n",
    "        # Rename the only column to 'correlation'\n",
    "        users_alike.columns = ['correlation']\n",
    "\n",
    "        # Sort the user correlations in descending order\n",
    "        # so that first one is the most similar, last one least similar\n",
    "        users_alike.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "\n",
    "        # Eliminate Correlation to itself by deleting first row,\n",
    "        # since biggest corr is with itself it is in first row\n",
    "        return users_alike.iloc[1:k+1] if k is not None else users_alike.iloc[1:]\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_user_corrs(movie_ratings, min_common_elements):\n",
    "        user_movie_matrix = movie_ratings.pivot_table(index='title', columns='user_id', values='rating')\n",
    "        return user_movie_matrix.corr(method=\"pearson\", min_periods=min_common_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TimebinSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimebinSimilarity:\n",
    "    \"\"\"\n",
    "    TimebinSimilarity is a way to provide personalized timebin based recommendations.\n",
    "    Here we define an alternative to classical pearson by providing temporal timebin aspect.\n",
    "    Here is how we do in summary:\n",
    "      1. Choose a person to make prediction on (as always).\n",
    "      2. Take the last s movie ratings of this person as a timebin from a random timepoint.(user must have at least s movie watched before!)\n",
    "      3. Find neighbour timebins to the selected timebin.(using correlations between them, higher, better)\n",
    "      4. Predict rating for the person using the timebin neighbours the same way as we do with k nearest neighbour.\n",
    "    \"\"\"\n",
    "       \n",
    "    @staticmethod\n",
    "    def timebin_similarity(timebin_a:pd.DataFrame, timebin_a_user_avg_rating:float,\n",
    "                           timebin_b:pd.DataFrame, timebin_b_user_avg_rating:float)->(float,int):\n",
    "        \"\"\"\n",
    "        Find the correlation between the timebin and its given neighbour timebin\n",
    "        \n",
    "        :param merged: Merged version of timebin and neighbour timebin in this order.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Filter unrelated movie ratings and only keep common movie ratings\n",
    "        merged = timebin_b.merge(timebin_a, on='item_id')\n",
    "\n",
    "        # Calculate Pearson Correlation in between the self.timebin and given neighbour\n",
    "        numenator = ((merged['rating_x'] - timebin_a_user_avg_rating) * (merged['rating_y'] - timebin_b_user_avg_rating)).sum()\n",
    "        denominator = math.sqrt(((merged['rating_x'] - timebin_a_user_avg_rating) ** 2).sum())\n",
    "        denominator *= math.sqrt(((merged['rating_y'] - timebin_b_user_avg_rating) ** 2).sum())\n",
    "        pearson = numenator / denominator if denominator != 0 else 0\n",
    "        \n",
    "        return pearson, len(merged)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_timebin(ratings, user_id, timebin_i, timebin_size):\n",
    "        \"\"\"\n",
    "        Generate the timebin by using given info.\n",
    "        timebin_i is the index the first movie\n",
    "        timebin_size is the number of movies to take starting at timebin_i th index.\n",
    "        \"\"\"\n",
    "        all_movies_watched = DatasetMovieOperator.get_movies_watched(ratings, user_id)[['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "        return all_movies_watched.iloc[timebin_i:timebin_i+timebin_size]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_neighbour_timebins(ratings, neighbour_id:int, movie_id:int,\n",
    "                               neighbour_min_s, neighbour_max_s, neighbour_step_size)->list:\n",
    "        \"\"\"\n",
    "        Given a neighbour_id, take the all the timebins of this neighbour which includes the target movie.\n",
    "        \n",
    "        :param neighbour_min_s: Minimum number of movies to include inside neighbour timebins\n",
    "        :param neighbour_max_s: Maximum number of movies to include inside neighbour timebins\n",
    "        :param neighbour_step_size: Number of movies to extend per step the timebin size\n",
    "        \"\"\"\n",
    "        # Start by taking all movies watched by the neighbour\n",
    "        all_movies_watched = DatasetMovieOperator.get_movies_watched(ratings, neighbour_id)[['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "        n_movies = len(all_movies_watched)\n",
    "        neighbour_timebins = list()\n",
    "\n",
    "        # For each timebin_size\n",
    "        for timebin_size in range(neighbour_min_s, neighbour_max_s, neighbour_step_size):\n",
    "            # Traverse the all movies by taking 'timebin_size' piece of ratings per loop\n",
    "            for i in range(0, n_movies, timebin_size):\n",
    "                timebin = all_movies_watched.iloc[i:i+timebin_size]\n",
    "                # Take only timebins which includes this movie\n",
    "                if not (movie_id in timebin.index):\n",
    "                    continue\n",
    "                # Insert the timebin, its index i, and its size timebin_size into the list as a tuple\n",
    "                neighbour_timebins.append(  (timebin, i, timebin_size)  )\n",
    "        return neighbour_timebins\n",
    "\n",
    "    @staticmethod\n",
    "    def get_possible_neighbour_list(ratings, user_id:int, timebin_i, timebin_size, movie_id, min_common_neighbour_ratings):\n",
    "        \"\"\"\n",
    "        Get possible neighbour Ids as list.\n",
    "        \n",
    "        Possible neighbours have watched'min_common_neighbour_ratings' number movies in common with the target timebin.\n",
    "        Possible neighbours also have to watch the target movie('movie_id') that we want to make prediction on.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Recreate the user timebin using its identifiers\n",
    "        timebin = TimebinSimilarity.get_timebin(ratings, user_id, timebin_i, timebin_size).drop(movie_id, inplace=True, errors='ignore')\n",
    "       \n",
    "        # :::: Change filter on ratings.loc by adding time constraint in case we filter future neighbour timebins::::\n",
    "        \n",
    "        # Count number of common ratings with other users\n",
    "        userlist = [0 for i in range(611)]\n",
    "        for movie_id in timebin.index.values.tolist():\n",
    "            users_who_watched = ratings.loc[(ratings['item_id'] == movie_id)][['user_id']].values.tolist()\n",
    "            for user_who_watched in users_who_watched:\n",
    "                userlist[user_who_watched[0]] += 1\n",
    "\n",
    "        # save as neighbour, if common rating count greater than min_common_neighbour_ratings and given rating to the movie index at i\n",
    "        neighbour_id_list = []\n",
    "        for j in range(0, 611):\n",
    "            if( (userlist[j] > min_common_neighbour_ratings) and \n",
    "               (DatasetMovieOperator.get_movie_rating(ratings, movie_id, j) != 0) and \n",
    "               (j != user_id) ):\n",
    "                neighbour_id_list.append(j)\n",
    "\n",
    "        return neighbour_id_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_knn(ratings, user_id, k, timebin_i, timebin_size,\n",
    "                         neighbour_min_s, neighbour_max_s, neighbour_step_size,\n",
    "                         corr_threshold, movie_id, min_common_elements, min_common_neighbour_ratings):\n",
    "        \"\"\"\n",
    "        ratings must be sorted in 'timestamp' order.\n",
    "        \n",
    "        :param timebin_i: timebin identifier, identifies start of timebin\n",
    "        :param timebin_size: timebin identifier, identifies size of the timebin starting at timebin_i index\n",
    "        :param neighbour_min_s: Minimum number of movies to include inside neighbour timebins \n",
    "        :param neighbour_max_s: Maximum number of movies to include inside neighbour timebins\n",
    "        :param neighbour_step_size: Number of movies to extend per step the timebin size\n",
    "        :param min_common_elements: Minimum number of common elements in between timebins in order them to become neighbour.\n",
    "                                              This  value is used to find similar timebins.\n",
    "        :param min_common_neighbour_ratings: Minimum number of ratings in common between two users neighbour when taking timebins of them.\n",
    "                                   This value is used to create possible timebins.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Recreate the user timebin using its identifiers\n",
    "        timebin = TimebinSimilarity.get_timebin(ratings, user_id, timebin_i, timebin_size).drop(movie_id, inplace=True, errors='ignore')\n",
    "        \n",
    "        timebin_user_avg = DatasetUserOperator.get_user_avg_at(ratings, user_id, timebin.iloc[-1]['timestamp'])\n",
    "        \n",
    "        # Neighbours contains users who has rated min_common_neighbour_ratings movies in common \n",
    "        # and also rated last movie of the timebin by the way, remember we are trying to predict last movie\n",
    "        neighbours = TimebinSimilarity.get_possible_neighbour_list(ratings, user_id, \n",
    "                                                                   timebin_i, timebin_size, \n",
    "                                                                   movie_id, min_common_neighbour_ratings)\n",
    "\n",
    "        data = list()\n",
    "        neighbour_count = 0\n",
    "        for neighbour_id in neighbours:\n",
    "            # Get all possible timebins of the neighbour that includes the movie with a timebin size with given constraints.\n",
    "            neighbour_timebins = TimebinSimilarity.get_neighbour_timebins(ratings, neighbour_id, movie_id,\n",
    "                                                                          neighbour_min_s, neighbour_max_s, \n",
    "                                                                          neighbour_step_size)\n",
    "            \n",
    "            # For each neighbour timebin, calculate pearson between the timebin and neighbour timebin \n",
    "            for neighbour_timebin, timebin_i, timebin_size in neighbour_timebins:\n",
    "                \n",
    "                # Calculate user avg for each timebin by assuming system at the time neighbour rated the last movie\n",
    "                neighbour_time_constraint = TimeConstraint(end_dt=neighbour_timebin.iloc[-1]['timestamp'])\n",
    "                neighbour_avg_rating = DatasetUserOperator.get_user_avg_at(ratings, neighbour_id, neighbour_time_constraint.end_dt)\n",
    "                #print(neighbour_timebin)\n",
    "                # Calculate Pearson Correlation between neighbour timebin and self.timebin\n",
    "                corr, common_elements = TimebinSimilarity.timebin_similarity(timebin, timebin_user_avg,\n",
    "                                                                             neighbour_timebin, neighbour_avg_rating)\n",
    "                \n",
    "                #print(f\"Corr -> {corr}, CommonElements -> {common_elements}\")\n",
    "                \n",
    "                # If less than min_timebin_neighbour_ratings movies in common rated in the neighbour_timebin, dont process this timebin\n",
    "                if common_elements < min_common_elements:\n",
    "                    continue\n",
    "                \n",
    "                # Dont include the neighbour timebin if corr is invalid or less than corr_threshold\n",
    "                if math.isnan(corr) or corr < corr_threshold:                     \n",
    "                    continue\n",
    "                \n",
    "                if neighbour_count > k:\n",
    "                    break\n",
    "                \n",
    "                data.append( (neighbour_id, common_elements, corr, timebin_i, timebin_size) )\n",
    "                neighbour_count += 1\n",
    "            \n",
    "            if neighbour_count > k:\n",
    "                    break\n",
    "            \n",
    "        \n",
    "        similar_timebins = pd.DataFrame(data, columns=['neighbour_id', 'n_common','pearson_corr', 'timebin_i', 'timebin_size'])\n",
    "        \n",
    "        # Drop duplicate (neighbour_id, n_common, pearson_corr, timebin_i, timebin_size) tuples so that now no repeating timebin exists\n",
    "        similar_timebins.drop_duplicates(['neighbour_id','n_common', 'pearson_corr', 'timebin_i'], inplace=True)\n",
    "        \n",
    "        return similar_timebins \n",
    "    \n",
    "    @staticmethod\n",
    "    def store_neighbour_data(ratings, similar_timebins, movie_id):\n",
    "        \"\"\"\n",
    "            Collect structured data aimed to generate predictions to the target movie later when needed.\n",
    "        \"\"\"\n",
    "        data = list()\n",
    "        \n",
    "        for row in similar_timebins.itertuples(index=False):\n",
    "            # Get neighbour timebin data\n",
    "            neighbour_id = row[0]\n",
    "            n_common = row[1]\n",
    "            corr = row[2]\n",
    "            timebin_i = row[3]\n",
    "            timebin_size = row[4]\n",
    "            \n",
    "            # Create the neighbour timebin from its data\n",
    "            neighbour_bin = TimebinSimilarity.get_timebin(ratings, neighbour_id, timebin_i, timebin_size)\n",
    "            rating = None\n",
    "            try: \n",
    "                rating = neighbour_bin.loc[movie_id]     # test whether the neighbour has rating for the item\n",
    "            except KeyError:\n",
    "                continue                                 # if neighbour dont have rating for the item, then pass this round of loop\n",
    "            \n",
    "            # insert rating for the neighbour into data \n",
    "            #  and also insert other data identifying the neighbour(for in case we use them in analysis)\n",
    "            if rating is not None:\n",
    "                data.append( (rating[0], corr, neighbour_id, n_common, timebin_i, timebin_size) )\n",
    "        \n",
    "        return data \n",
    "    \n",
    "    @staticmethod\n",
    "    def store_common_neighbour_ratings(ratings, timebin, similar_timebins):\n",
    "        \"\"\"\n",
    "        Collect neighbour data for each commonly rated movie.\n",
    "        \"\"\"\n",
    "        # Store data in order to return as result\n",
    "        data = defaultdict(list)\n",
    "        for row in similar_timebins.itertuples(index=False):\n",
    "            # Get neighbour timebin data\n",
    "            neighbour_id = row[0]\n",
    "            n_common = row[1]\n",
    "            corr = row[2]\n",
    "            timebin_i = row[3]\n",
    "            timebin_size = row[4]\n",
    "            \n",
    "            # Create the neighbour timebin from its data\n",
    "            neighbour_bin = TimebinSimilarity.get_timebin(ratings, neighbour_id, timebin_i, timebin_size)\n",
    "            \n",
    "            # Get rid of movie ratings of the neighbour that are not found in the timebin\n",
    "            merged_bin = pd.merge(timebin, neighbour_bin, left_index=True, right_index=True)\n",
    "            \n",
    "            # For each common movie rating, store neighbour rating and its correlation to the timebin\n",
    "            for bin_row in merged_bin.itertuples(index=True):\n",
    "                curr_movie = bin_row[0]\n",
    "                neighbour_rating = bin_row[3]\n",
    "                #print(bin_row[0], bin_row[1], bin_row[2], bin_row[3], bin_row[4])\n",
    "                data[curr_movie].append( (neighbour_rating, corr, neighbour_id, n_common, timebin_i, timebin_size) )\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_timebin_size(tc: TimeConstraint):\n",
    "        return abs((tc.start_dt - tc.end_dt).days)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PearsonPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PearsonPredict:\n",
    "    \"\"\"\n",
    "    Only neighbourhood based methods are supported.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_movie(movie_ratings, user_id, movie_id, \n",
    "                      time_constraint: TimeConstraint = None, k = 10, min_common_elements = 5):\n",
    "        \"\"\"\n",
    "        Predict a movie of a user.\n",
    "        \n",
    "        For optimization, use predict function instead of this.\n",
    "        \"\"\"\n",
    "        movie_ratings = DatasetOperator.apply_time_constraint(movie_ratings)\n",
    "        knn = PearsonSimilarity.get_knn(movie_ratings, user_id, k, min_common_elements)\n",
    "        return PearsonPredict.predict(movie_ratings, user_id, movie_id=movie_id, k_neighbours=knn)\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict(ratings, user_id, movie_id, k_neighbours: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Predict the movie for given user using Pearson Correlation\n",
    "\n",
    "        :param user_id: user of interest\n",
    "        :param movie_id: the movie's rating is the one we we want to predict\n",
    "        :param k_neighbours: k nearest neighbours in DataFrame where index user_id, column correlation in between.\n",
    "        :return: Prediction rating\n",
    "        \"\"\"\n",
    "        # If a movie with movie_id not exists, predict 0\n",
    "        if DatasetMovieOperator.get_movie(ratings, movie_id).empty:\n",
    "            return 0\n",
    "\n",
    "        if k_neighbours is None or k_neighbours.empty:\n",
    "            return 0\n",
    "        \n",
    "        user_avg_rating = DatasetUserOperator.get_user_avg(ratings, user_id)\n",
    "        weighted_sum = 0.0\n",
    "        sum_of_weights = 0.0\n",
    "        for neighbour_id, data in k_neighbours.iterrows():\n",
    "            # Get each neighbour's correlation 'user_id' and her rating to 'movie_id'\n",
    "            neighbour_corr = data['correlation']\n",
    "            neighbour_rating = DatasetMovieOperator.get_movie_rating(ratings, movie_id, neighbour_id)\n",
    "            # If the neighbour doesnt give rating to the movie_id, pass this around of the loop\n",
    "            if neighbour_rating == 0:\n",
    "                continue\n",
    "            neighbour_avg_rating = DatasetUserOperator.get_user_avg(ratings, neighbour_id)\n",
    "            neighbour_mean_centered_rating = neighbour_rating - neighbour_avg_rating\n",
    "            # Calculate Weighted sum and sum of weights\n",
    "            weighted_sum += neighbour_mean_centered_rating * neighbour_corr\n",
    "            sum_of_weights += neighbour_corr\n",
    "            \n",
    "        if sum_of_weights != 0:\n",
    "            prediction = user_avg_rating + (weighted_sum / sum_of_weights)\n",
    "        else:\n",
    "            prediction = 0  # In this case, none of the neighbours have given rating to 'the movie'\n",
    "\n",
    "        return prediction\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_movies_watched(movie_ratings, user_id, n=10, k=10, time_constraint=None, min_common_elements = 5) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        :param user_id: user of interest\n",
    "        :param n: Number of movies to predict\n",
    "        :param k: k neighbours to take into account\n",
    "        :param time_constraint: When calculating k neighbours,\n",
    "                                only those that comply to time_constraints will be taken into account.\n",
    "        :return: DataFrame of Predictions where columns = ['prediction', 'rating'] index = 'movie_id'\n",
    "        \"\"\"\n",
    "        \n",
    "        movie_ratings = DatasetOperator.apply_time_constraint(movie_ratings)\n",
    "        \n",
    "        # Get all movies watched by a user\n",
    "        movies_watched = DatasetMovieOperator.get_movies_watched(movie_ratings, user_id)[['item_id', 'rating']]\n",
    "\n",
    "        if movies_watched.empty:\n",
    "            return None\n",
    "        \n",
    "        # Cache user correlations because of the repeated predictions\n",
    "        user_corr_matrix = PearsonSimilarity.create_user_corrs(movie_ratings, min_common_elements)\n",
    "        \n",
    "        predictions = list()\n",
    "        number_of_predictions = 0\n",
    "        for row in movies_watched.itertuples(index=False):\n",
    "            knn = PearsonSimilarity.get_knn(movie_ratings, user_id, k, min_common_elements, user_corr_matrix)\n",
    "            prediction = PearsonPredict.predict(movie_ratings, user_id, movie_id=row[0], k_neighbours=knn)\n",
    "            if number_of_predictions == n:\n",
    "                break\n",
    "            predictions.append([prediction, row[1], row[0]])\n",
    "            number_of_predictions += 1\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions, columns=['prediction', 'rating', 'movie_id'])\n",
    "        predictions_df.movie_id = predictions_df.movie_id.astype(int)\n",
    "        return predictions_df.set_index('movie_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TimebinPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimebinPredict:\n",
    "    @staticmethod\n",
    "    def predict_movie(ratings, user_id, movie_id, timebin_size,\n",
    "                      neighbour_min_s=5, neighbour_max_s=50, neighbour_step_size=1,\n",
    "                      k = 10,\n",
    "                      corr_threshold=0.1, \n",
    "                      min_common_neighbour_ratings=3, min_common_elements=3, \n",
    "                      min_neighbour_count=5):\n",
    "        \"\"\"\n",
    "        Predict Movie with given timebin. Last element of the provided timebin is the movie we want to predict.\n",
    "        \"\"\"\n",
    "        timebin_i = len(DatasetMovieOperator.get_movies_watched(ratings, user_id)) - 1 - timebin_size\n",
    "        \n",
    "        if(timebin_i < 0):\n",
    "            print(\"The specified user timebin is not valid.\")\n",
    "            return None\n",
    "        \n",
    "        knn = TimebinSimilarity.get_knn(ratings, \n",
    "                                        user_id,\n",
    "                                        k,\n",
    "                                        timebin_i, \n",
    "                                        timebin_size, \n",
    "                                        neighbour_min_s, \n",
    "                                        neighbour_max_s, \n",
    "                                        neighbour_step_size, \n",
    "                                        corr_threshold, \n",
    "                                        movie_id,\n",
    "                                        min_common_neighbour_ratings, \n",
    "                                        min_common_elements)\n",
    "    \n",
    "        return TimebinPredict.predict(ratings, movie_id, knn, min_neighbour_count)\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_knn_data(knn_data,  min_neighbour_count=5):\n",
    "        \"\"\"\n",
    "        Make prediction by using the neighbour timebin data on the movie of interest.\n",
    "        K nearest neighbours is used only.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sort the data by corr\n",
    "        knn_data.sort(key=lambda data_element: data_element[1])\n",
    "        \n",
    "        # each row of data ->(rating, corr, neighbour_id, n_common, timebin_i, timebin_size, timebin_size_in_days)\n",
    "        weighted_sum = 0\n",
    "        weight_sum = 0\n",
    "        count = 0\n",
    "        for (rating, corr, _, _, _, _) in knn_data:\n",
    "            weighted_sum += rating * corr\n",
    "            weight_sum += corr            \n",
    "            count += 1\n",
    "            \n",
    "        if count < min_neighbour_count:\n",
    "            return 0\n",
    "        \n",
    "        prediction = weighted_sum / weight_sum\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(ratings, movie_id, knn, min_neighbour_count=5):\n",
    "        if knn is None or knn.empty:\n",
    "            print(\"No neighbour found\")\n",
    "            return None\n",
    "        \n",
    "        neighbours_data = knn[['neighbour_id', 'pearson_corr']].groupby('neighbour_id').mean()\n",
    "        neighbours_data.sort_values(by='pearson_corr', ascending=False, inplace=True)\n",
    "        \n",
    "        weighted_sum = 0\n",
    "        weight_sum = 0\n",
    "        count = 0\n",
    "        for index, row in knn[['neighbour_id', 'pearson_corr']].groupby('neighbour_id').mean().iterrows():\n",
    "            neighbour_id = index\n",
    "            corr = row[0]\n",
    "            rating = DatasetMovieOperator.get_movie_rating(ratings, movie_id, neighbour_id)\n",
    "            weighted_sum += rating * corr\n",
    "            weight_sum += corr\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "        if count < min_neighbour_count:\n",
    "            return 0\n",
    "        \n",
    "        prediction = weighted_sum / weight_sum\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AnalizeTimebinSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalizeTimebinSimilarity:\n",
    "    \n",
    "    @staticmethod\n",
    "    def analize_timebin_prediction(movie_ratings, ratings, n, s, k=10, min_neighbour=3, corr_threshold=0.1, \n",
    "                                   neighbour_min_s=5, neighbour_max_s=50, neighbour_step_size=5, \n",
    "                                   min_common_elements=3,\n",
    "                                   min_common_neighbour_ratings=3):\n",
    "        \"\"\"\n",
    "        :param s: Timebin size\n",
    "        :param n: Number of prediction to make where each prediction is for a randomly chosen users' a random movie\n",
    "        :param k: K nearest neighbour of the timebin based neighbourhood will be taken when making prediction.\n",
    "        :param min_neighbour: Minimum number of neighbour timebins when making prediction.\n",
    "        :param corr_threshold: Overrides the default behaviour if you set any value other than None\n",
    "        :param neighbour_min_s: Minimum number of movies to include inside neighbour timebins\n",
    "                                Overrides default behaviour of the class if given any value other than None \n",
    "        :param neighbour_max_s: Maximum number of movies to include inside neighbour timebins\n",
    "                                Overrides default behaviour of the class if given any value other than None\n",
    "        :param neighbour_step_size: Increase of number of movies in the timebin per step\n",
    "                                Overrides default behaviour of the class if given any value other than None\n",
    "        :param min_common_elements: Minimum number of common elements in between timebins in order them to become neighbour.\n",
    "                                              This  value is used to find similar timebins.\n",
    "        :param min_common_ratings: Minimum number of ratings in common between two users neighbour when taking timebins of them.\n",
    "                                   This value is used to create possible timebins.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Correlation threshold should be greater than or equal to 0 for best quality. \n",
    "        if corr_threshold < 0:\n",
    "            corr_threshold = 0\n",
    "        \n",
    "        output = list()\n",
    "        \n",
    "        # collected data count\n",
    "        count = 0\n",
    "        normal_predictions = list()\n",
    "        timebin_predictions = list()\n",
    "        \n",
    "        while count < n:\n",
    "            \n",
    "            # Choose a random user\n",
    "            user_id = DatasetUserOperator.get_random_users(ratings, n=1)[0]\n",
    "            \n",
    "            ### Choose Random timebin\n",
    "            \n",
    "            # Get all movies that has been watched by the user\n",
    "            movies_watched = DatasetMovieOperator.get_movies_watched(ratings, user_id)\n",
    "            movies_watched = movies_watched[['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "            \n",
    "            # In order to simulate that we are making prediction in random point in time\n",
    "            # take a random starting point and take s piece movies starting from that point as timebin.\n",
    "            max_movie_index = ( len(movies_watched) - 1 ) - s\n",
    "            if max_movie_index <= 0:\n",
    "                continue\n",
    "            timebin_i = random.randint(0, max_movie_index)\n",
    "            timebin = TimebinSimilarity.get_timebin(ratings, user_id, timebin_i, s)\n",
    "            \n",
    "            # Make sure we have a valid timebin before moving forward\n",
    "            if timebin is None or s < min_common_elements:\n",
    "                continue\n",
    "            \n",
    "            ### Predict last movie of the timebin with Normal way(KNN with Pearson Correlation)\n",
    "            movie_id = timebin.index[-1]\n",
    "            \n",
    "            # Get max limit time constraint where max limit is the time we rate the last movie inside the timebin\n",
    "            time_constraint = TimeConstraint(end_dt=(timebin.iloc[-1]['timestamp'] - timedelta(milliseconds=1))) \n",
    "            normal_prediction = PearsonPredict.predict_movie(movie_ratings, user_id, movie_id, time_constraint)\n",
    "            \n",
    "            # In order to compare predictions, normal prediction must exists\n",
    "            if normal_prediction == 0:\n",
    "                continue\n",
    "            \n",
    "            ### Predict last movie of the timebin with Timebin Based K Nearest Neighbourhood\n",
    "            similar_timebins = TimebinSimilarity.get_knn(ratings, user_id, k,\n",
    "                                                         timebin_i, s,\n",
    "                                                         neighbour_min_s=neighbour_min_s,\n",
    "                                                         neighbour_max_s=neighbour_max_s,\n",
    "                                                         neighbour_step_size=neighbour_step_size,\n",
    "                                                         corr_threshold=corr_threshold,\n",
    "                                                         movie_id=movie_id, \n",
    "                                                         min_common_elements=min_common_elements, \n",
    "                                                         min_common_neighbour_ratings=min_common_neighbour_ratings)\n",
    "            if similar_timebins.empty:\n",
    "                continue  \n",
    "\n",
    "            prediction = TimebinPredict.predict(ratings, movie_id, similar_timebins, min_neighbour_count=min_neighbour)\n",
    "            actual = DatasetMovieOperator.get_movie_rating(ratings, movie_id, user_id)\n",
    "            \n",
    "            # In order to compare predictions, timebin based prediction must exists\n",
    "            if prediction == 0:\n",
    "                continue\n",
    "            \n",
    "            print(movie_id, normal_prediction, prediction, actual)  # Debug Output\n",
    "            \n",
    "            normal_predictions.append( (normal_prediction, actual) )\n",
    "            timebin_predictions.append( (prediction, actual) )\n",
    "            \n",
    "            #### !!!!!!!!!!! Add movieId to output in order to calculate coverage.\n",
    "            \n",
    "            # Save the data\n",
    "            \n",
    "            output.append( (user_id, timebin_i, s, similar_timebins) )\n",
    "            # we saved a data succesfully, increment collected data cout\n",
    "            count += 1\n",
    "        \n",
    "        # RMSE for debug output\n",
    "        normal_rmse = Accuracy.rmse(normal_predictions)\n",
    "        timebins_rmse = Accuracy.rmse(timebin_predictions)\n",
    "        print(f\"Normal RMSE: {normal_rmse:.2} Timebin RMSE:{timebins_rmse:.2}\")\n",
    "        \n",
    "        result = {\n",
    "            \"output\":output,\n",
    "            \"normal_predictions\":normal_predictions,\n",
    "            \"timebin_predictions\":timebin_predictions\n",
    "        }\n",
    "        \n",
    "        return result         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighbourhoodBasedSimilarity(ABC):\n",
    "    \"\"\"\n",
    "    Similarity class. Its subclasses provides utilities in order to calculate similarities between users.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_neighbours(user_id:int, movie_id:int) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Calculate similarities between neighbours and returns each neighbour and its similarity. \n",
    "        \n",
    "        :return: DataFrame of [rating, corr] \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_weighted_neighbours(user_id:int, movie_id:int) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Apply signifance weighting. \n",
    "        \n",
    "        :return: DataFrame of [rating, corr] \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict(ABC):\n",
    "    \"\"\"\n",
    "    Prediction class. Its subclassses provides utilities for making prediction.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>429</td>\n",
       "      <td>434</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>Cliffhanger</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>434</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1996-04-18 11:46:45</td>\n",
       "      <td>Cliffhanger</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>468</td>\n",
       "      <td>434</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1996-05-06 16:34:24</td>\n",
       "      <td>Cliffhanger</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>353</td>\n",
       "      <td>434</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1996-05-12 22:21:25</td>\n",
       "      <td>Cliffhanger</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107</td>\n",
       "      <td>434</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1996-05-13 16:08:20</td>\n",
       "      <td>Cliffhanger</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100616</th>\n",
       "      <td>331</td>\n",
       "      <td>193609</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-09-17 04:13:26</td>\n",
       "      <td>Andrew Dice Clay: Dice Rules</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1991.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100617</th>\n",
       "      <td>233</td>\n",
       "      <td>78103</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2018-09-20 19:09:26</td>\n",
       "      <td>Shake Hands with the Devil</td>\n",
       "      <td>Drama|War</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100618</th>\n",
       "      <td>210</td>\n",
       "      <td>130444</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-09-22 16:05:10</td>\n",
       "      <td>Ruby Red</td>\n",
       "      <td>Adventure|Children|Fantasy|Sci-Fi</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100619</th>\n",
       "      <td>210</td>\n",
       "      <td>166203</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-09-22 16:05:36</td>\n",
       "      <td>Sapphire Blue</td>\n",
       "      <td>Adventure|Children|Fantasy|Sci-Fi</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100620</th>\n",
       "      <td>210</td>\n",
       "      <td>179511</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-09-22 16:05:38</td>\n",
       "      <td>Emerald Green</td>\n",
       "      <td>Adventure|Drama|Fantasy|Romance</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100621 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id  rating           timestamp  \\\n",
       "0           429      434     4.0 1996-03-29 18:36:55   \n",
       "1            99      434     3.0 1996-04-18 11:46:45   \n",
       "2           468      434     3.0 1996-05-06 16:34:24   \n",
       "3           353      434     3.0 1996-05-12 22:21:25   \n",
       "4           107      434     3.0 1996-05-13 16:08:20   \n",
       "...         ...      ...     ...                 ...   \n",
       "100616      331   193609     4.0 2018-09-17 04:13:26   \n",
       "100617      233    78103     2.5 2018-09-20 19:09:26   \n",
       "100618      210   130444     4.0 2018-09-22 16:05:10   \n",
       "100619      210   166203     4.0 2018-09-22 16:05:36   \n",
       "100620      210   179511     4.0 2018-09-22 16:05:38   \n",
       "\n",
       "                               title                             genres  \\\n",
       "0                        Cliffhanger          Action|Adventure|Thriller   \n",
       "1                        Cliffhanger          Action|Adventure|Thriller   \n",
       "2                        Cliffhanger          Action|Adventure|Thriller   \n",
       "3                        Cliffhanger          Action|Adventure|Thriller   \n",
       "4                        Cliffhanger          Action|Adventure|Thriller   \n",
       "...                              ...                                ...   \n",
       "100616  Andrew Dice Clay: Dice Rules                             Comedy   \n",
       "100617    Shake Hands with the Devil                          Drama|War   \n",
       "100618                      Ruby Red  Adventure|Children|Fantasy|Sci-Fi   \n",
       "100619                 Sapphire Blue  Adventure|Children|Fantasy|Sci-Fi   \n",
       "100620                 Emerald Green    Adventure|Drama|Fantasy|Romance   \n",
       "\n",
       "          year  \n",
       "0       1993.0  \n",
       "1       1993.0  \n",
       "2       1993.0  \n",
       "3       1993.0  \n",
       "4       1993.0  \n",
       "...        ...  \n",
       "100616  1991.0  \n",
       "100617  2007.0  \n",
       "100618  2013.0  \n",
       "100619  2014.0  \n",
       "100620  2016.0  \n",
       "\n",
       "[100621 rows x 7 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movielens = MovieLensDataset()\n",
    "movie_ratings = movielens.create_movie_ratings(movielens.ratings ,movielens.movies)\n",
    "movie_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>448</td>\n",
       "      <td>434</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-04-18 10:13:20</td>\n",
       "      <td>Cliffhanger</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>448</td>\n",
       "      <td>590</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-04-18 10:20:59</td>\n",
       "      <td>Dances with Wolves</td>\n",
       "      <td>Adventure|Drama|Western</td>\n",
       "      <td>1990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>448</td>\n",
       "      <td>432</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-04-18 11:18:14</td>\n",
       "      <td>City Slickers II: The Legend of Curly's Gold</td>\n",
       "      <td>Adventure|Comedy|Western</td>\n",
       "      <td>1994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>448</td>\n",
       "      <td>592</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2005-06-07 17:54:17</td>\n",
       "      <td>Batman</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "      <td>1989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>448</td>\n",
       "      <td>165</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-04-18 15:05:24</td>\n",
       "      <td>Die Hard: With a Vengeance</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99298</th>\n",
       "      <td>448</td>\n",
       "      <td>127132</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2017-07-02 11:30:13</td>\n",
       "      <td>Zipper</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99299</th>\n",
       "      <td>448</td>\n",
       "      <td>173751</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2017-07-02 18:12:19</td>\n",
       "      <td>Tiger Raid</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99300</th>\n",
       "      <td>448</td>\n",
       "      <td>146730</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2017-07-03 17:37:10</td>\n",
       "      <td>Lost in the Sun</td>\n",
       "      <td>Action|Drama|Thriller</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99301</th>\n",
       "      <td>448</td>\n",
       "      <td>152085</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2017-07-03 18:50:29</td>\n",
       "      <td>Desierto</td>\n",
       "      <td>Drama</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99311</th>\n",
       "      <td>448</td>\n",
       "      <td>165347</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2017-07-09 17:18:29</td>\n",
       "      <td>Jack Reacher: Never Go Back</td>\n",
       "      <td>Action|Crime|Drama|Mystery|Thriller</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1863 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  rating           timestamp  \\\n",
       "80         448      434     3.0 2002-04-18 10:13:20   \n",
       "384        448      590     3.0 2002-04-18 10:20:59   \n",
       "487        448      432     3.0 2002-04-18 11:18:14   \n",
       "642        448      592     3.0 2005-06-07 17:54:17   \n",
       "1120       448      165     4.0 2002-04-18 15:05:24   \n",
       "...        ...      ...     ...                 ...   \n",
       "99298      448   127132     2.0 2017-07-02 11:30:13   \n",
       "99299      448   173751     2.0 2017-07-02 18:12:19   \n",
       "99300      448   146730     2.0 2017-07-03 17:37:10   \n",
       "99301      448   152085     2.5 2017-07-03 18:50:29   \n",
       "99311      448   165347     2.0 2017-07-09 17:18:29   \n",
       "\n",
       "                                              title  \\\n",
       "80                                      Cliffhanger   \n",
       "384                              Dances with Wolves   \n",
       "487    City Slickers II: The Legend of Curly's Gold   \n",
       "642                                          Batman   \n",
       "1120                     Die Hard: With a Vengeance   \n",
       "...                                             ...   \n",
       "99298                                        Zipper   \n",
       "99299                                    Tiger Raid   \n",
       "99300                               Lost in the Sun   \n",
       "99301                                      Desierto   \n",
       "99311                   Jack Reacher: Never Go Back   \n",
       "\n",
       "                                    genres    year  \n",
       "80               Action|Adventure|Thriller  1993.0  \n",
       "384                Adventure|Drama|Western  1990.0  \n",
       "487               Adventure|Comedy|Western  1994.0  \n",
       "642                  Action|Crime|Thriller  1989.0  \n",
       "1120                 Action|Crime|Thriller  1995.0  \n",
       "...                                    ...     ...  \n",
       "99298                       Drama|Thriller  2015.0  \n",
       "99299                             Thriller  2016.0  \n",
       "99300                Action|Drama|Thriller  2015.0  \n",
       "99301                                Drama  2016.0  \n",
       "99311  Action|Crime|Drama|Mystery|Thriller  2016.0  \n",
       "\n",
       "[1863 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetMovieOperator.get_movies_watched(movie_ratings, 448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-04-18 09:57:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-04-18 09:57:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-04-18 09:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2002-04-18 09:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-04-18 09:58:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-04-18 09:58:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-04-18 09:58:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-04-18 09:59:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-04-18 09:59:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-04-18 10:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-04-18 10:00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-04-18 10:01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-04-18 10:01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2002-04-18 10:01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-04-18 10:02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-04-18 10:02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-04-18 10:02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-04-18 10:02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-04-18 10:02:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-04-18 10:02:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating           timestamp\n",
       "item_id                            \n",
       "3507        4.0 2002-04-18 09:57:46\n",
       "1210        5.0 2002-04-18 09:57:46\n",
       "1374        3.0 2002-04-18 09:58:00\n",
       "2349        2.0 2002-04-18 09:58:00\n",
       "1961        3.0 2002-04-18 09:58:14\n",
       "1193        5.0 2002-04-18 09:58:48\n",
       "507         3.0 2002-04-18 09:58:48\n",
       "1097        4.0 2002-04-18 09:59:01\n",
       "541         4.0 2002-04-18 09:59:20\n",
       "4499        4.0 2002-04-18 10:00:05\n",
       "3030        5.0 2002-04-18 10:00:45\n",
       "260         5.0 2002-04-18 10:01:11\n",
       "1377        3.0 2002-04-18 10:01:34\n",
       "2408        2.0 2002-04-18 10:01:34\n",
       "1221        5.0 2002-04-18 10:02:27\n",
       "858         5.0 2002-04-18 10:02:27\n",
       "1196        5.0 2002-04-18 10:02:27\n",
       "1198        5.0 2002-04-18 10:02:27\n",
       "1387        4.0 2002-04-18 10:02:40\n",
       "1214        3.0 2002-04-18 10:02:54"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = TimebinSimilarity.get_timebin(movielens.ratings.sort_values(by='timestamp'), 448, 0, 20) ### Sort ratings by timestamp first\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = TimebinSimilarity.get_timebin(movielens.ratings.sort_values(by='timestamp'), 334, 0, 50) ### Sort ratings by timestamp first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9725947945938099, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TimebinSimilarity.timebin_similarity(t1, DatasetUserOperator.get_user_avg(movielens.ratings, 448),\n",
    "                                     t2, DatasetUserOperator.get_user_avg(movielens.ratings, 334))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(         rating           timestamp\n",
       "  item_id                            \n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54,\n",
       "  15,\n",
       "  5),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  3030        5.0 2002-04-18 10:00:45\n",
       "  260         5.0 2002-04-18 10:01:11\n",
       "  2408        2.0 2002-04-18 10:01:34\n",
       "  1377        3.0 2002-04-18 10:01:34\n",
       "  1221        5.0 2002-04-18 10:02:27\n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54,\n",
       "  10,\n",
       "  10),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54\n",
       "  589         3.0 2002-04-18 10:03:14\n",
       "  1201        5.0 2002-04-18 10:03:14\n",
       "  1240        3.0 2002-04-18 10:03:14\n",
       "  1036        5.0 2002-04-18 10:03:31\n",
       "  1953        5.0 2002-04-18 10:03:31\n",
       "  1610        4.0 2002-04-18 10:03:31\n",
       "  2366        5.0 2002-04-18 10:03:51\n",
       "  474         4.0 2002-04-18 10:03:51\n",
       "  1291        4.0 2002-04-18 10:03:51\n",
       "  21          2.0 2002-04-18 10:03:51,\n",
       "  15,\n",
       "  15),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  1193        5.0 2002-04-18 09:58:48\n",
       "  507         3.0 2002-04-18 09:58:48\n",
       "  1097        4.0 2002-04-18 09:59:01\n",
       "  541         4.0 2002-04-18 09:59:20\n",
       "  4499        4.0 2002-04-18 10:00:05\n",
       "  3030        5.0 2002-04-18 10:00:45\n",
       "  260         5.0 2002-04-18 10:01:11\n",
       "  2408        2.0 2002-04-18 10:01:34\n",
       "  1377        3.0 2002-04-18 10:01:34\n",
       "  1221        5.0 2002-04-18 10:02:27\n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54,\n",
       "  0,\n",
       "  20),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  1193        5.0 2002-04-18 09:58:48\n",
       "  507         3.0 2002-04-18 09:58:48\n",
       "  1097        4.0 2002-04-18 09:59:01\n",
       "  541         4.0 2002-04-18 09:59:20\n",
       "  4499        4.0 2002-04-18 10:00:05\n",
       "  3030        5.0 2002-04-18 10:00:45\n",
       "  260         5.0 2002-04-18 10:01:11\n",
       "  2408        2.0 2002-04-18 10:01:34\n",
       "  1377        3.0 2002-04-18 10:01:34\n",
       "  1221        5.0 2002-04-18 10:02:27\n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54\n",
       "  589         3.0 2002-04-18 10:03:14\n",
       "  1201        5.0 2002-04-18 10:03:14\n",
       "  1240        3.0 2002-04-18 10:03:14\n",
       "  1036        5.0 2002-04-18 10:03:31\n",
       "  1953        5.0 2002-04-18 10:03:31,\n",
       "  0,\n",
       "  25),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  1193        5.0 2002-04-18 09:58:48\n",
       "  507         3.0 2002-04-18 09:58:48\n",
       "  1097        4.0 2002-04-18 09:59:01\n",
       "  541         4.0 2002-04-18 09:59:20\n",
       "  4499        4.0 2002-04-18 10:00:05\n",
       "  3030        5.0 2002-04-18 10:00:45\n",
       "  260         5.0 2002-04-18 10:01:11\n",
       "  2408        2.0 2002-04-18 10:01:34\n",
       "  1377        3.0 2002-04-18 10:01:34\n",
       "  1221        5.0 2002-04-18 10:02:27\n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54\n",
       "  589         3.0 2002-04-18 10:03:14\n",
       "  1201        5.0 2002-04-18 10:03:14\n",
       "  1240        3.0 2002-04-18 10:03:14\n",
       "  1036        5.0 2002-04-18 10:03:31\n",
       "  1953        5.0 2002-04-18 10:03:31\n",
       "  1610        4.0 2002-04-18 10:03:31\n",
       "  2366        5.0 2002-04-18 10:03:51\n",
       "  474         4.0 2002-04-18 10:03:51\n",
       "  1291        4.0 2002-04-18 10:03:51\n",
       "  21          2.0 2002-04-18 10:03:51,\n",
       "  0,\n",
       "  30),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  1193        5.0 2002-04-18 09:58:48\n",
       "  507         3.0 2002-04-18 09:58:48\n",
       "  1097        4.0 2002-04-18 09:59:01\n",
       "  541         4.0 2002-04-18 09:59:20\n",
       "  4499        4.0 2002-04-18 10:00:05\n",
       "  3030        5.0 2002-04-18 10:00:45\n",
       "  260         5.0 2002-04-18 10:01:11\n",
       "  2408        2.0 2002-04-18 10:01:34\n",
       "  1377        3.0 2002-04-18 10:01:34\n",
       "  1221        5.0 2002-04-18 10:02:27\n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54\n",
       "  589         3.0 2002-04-18 10:03:14\n",
       "  1201        5.0 2002-04-18 10:03:14\n",
       "  1240        3.0 2002-04-18 10:03:14\n",
       "  1036        5.0 2002-04-18 10:03:31\n",
       "  1953        5.0 2002-04-18 10:03:31\n",
       "  1610        4.0 2002-04-18 10:03:31\n",
       "  2366        5.0 2002-04-18 10:03:51\n",
       "  474         4.0 2002-04-18 10:03:51\n",
       "  1291        4.0 2002-04-18 10:03:51\n",
       "  21          2.0 2002-04-18 10:03:51\n",
       "  2951        5.0 2002-04-18 10:03:51\n",
       "  4855        4.0 2002-04-18 10:04:04\n",
       "  1954        4.0 2002-04-18 10:04:04\n",
       "  3104        4.0 2002-04-18 10:04:39\n",
       "  2406        3.0 2002-04-18 10:04:39,\n",
       "  0,\n",
       "  35),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  1193        5.0 2002-04-18 09:58:48\n",
       "  507         3.0 2002-04-18 09:58:48\n",
       "  1097        4.0 2002-04-18 09:59:01\n",
       "  541         4.0 2002-04-18 09:59:20\n",
       "  4499        4.0 2002-04-18 10:00:05\n",
       "  3030        5.0 2002-04-18 10:00:45\n",
       "  260         5.0 2002-04-18 10:01:11\n",
       "  2408        2.0 2002-04-18 10:01:34\n",
       "  1377        3.0 2002-04-18 10:01:34\n",
       "  1221        5.0 2002-04-18 10:02:27\n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54\n",
       "  589         3.0 2002-04-18 10:03:14\n",
       "  1201        5.0 2002-04-18 10:03:14\n",
       "  1240        3.0 2002-04-18 10:03:14\n",
       "  1036        5.0 2002-04-18 10:03:31\n",
       "  1953        5.0 2002-04-18 10:03:31\n",
       "  1610        4.0 2002-04-18 10:03:31\n",
       "  2366        5.0 2002-04-18 10:03:51\n",
       "  474         4.0 2002-04-18 10:03:51\n",
       "  1291        4.0 2002-04-18 10:03:51\n",
       "  21          2.0 2002-04-18 10:03:51\n",
       "  2951        5.0 2002-04-18 10:03:51\n",
       "  4855        4.0 2002-04-18 10:04:04\n",
       "  1954        4.0 2002-04-18 10:04:04\n",
       "  3104        4.0 2002-04-18 10:04:39\n",
       "  2406        3.0 2002-04-18 10:04:39\n",
       "  2991        5.0 2002-04-18 10:04:39\n",
       "  480         3.0 2002-04-18 10:04:52\n",
       "  3578        4.0 2002-04-18 10:04:52\n",
       "  1722        4.0 2002-04-18 10:04:52\n",
       "  1275        5.0 2002-04-18 10:05:06,\n",
       "  0,\n",
       "  40),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  1193        5.0 2002-04-18 09:58:48\n",
       "  507         3.0 2002-04-18 09:58:48\n",
       "  1097        4.0 2002-04-18 09:59:01\n",
       "  541         4.0 2002-04-18 09:59:20\n",
       "  4499        4.0 2002-04-18 10:00:05\n",
       "  3030        5.0 2002-04-18 10:00:45\n",
       "  260         5.0 2002-04-18 10:01:11\n",
       "  2408        2.0 2002-04-18 10:01:34\n",
       "  1377        3.0 2002-04-18 10:01:34\n",
       "  1221        5.0 2002-04-18 10:02:27\n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54\n",
       "  589         3.0 2002-04-18 10:03:14\n",
       "  1201        5.0 2002-04-18 10:03:14\n",
       "  1240        3.0 2002-04-18 10:03:14\n",
       "  1036        5.0 2002-04-18 10:03:31\n",
       "  1953        5.0 2002-04-18 10:03:31\n",
       "  1610        4.0 2002-04-18 10:03:31\n",
       "  2366        5.0 2002-04-18 10:03:51\n",
       "  474         4.0 2002-04-18 10:03:51\n",
       "  1291        4.0 2002-04-18 10:03:51\n",
       "  21          2.0 2002-04-18 10:03:51\n",
       "  2951        5.0 2002-04-18 10:03:51\n",
       "  4855        4.0 2002-04-18 10:04:04\n",
       "  1954        4.0 2002-04-18 10:04:04\n",
       "  3104        4.0 2002-04-18 10:04:39\n",
       "  2406        3.0 2002-04-18 10:04:39\n",
       "  2991        5.0 2002-04-18 10:04:39\n",
       "  480         3.0 2002-04-18 10:04:52\n",
       "  3578        4.0 2002-04-18 10:04:52\n",
       "  1722        4.0 2002-04-18 10:04:52\n",
       "  1275        5.0 2002-04-18 10:05:06\n",
       "  1408        2.0 2002-04-18 10:05:06\n",
       "  377         4.0 2002-04-18 10:05:24\n",
       "  380         3.0 2002-04-18 10:05:34\n",
       "  733         3.0 2002-04-18 10:05:54\n",
       "  2640        3.0 2002-04-18 10:05:54,\n",
       "  0,\n",
       "  45),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  1193        5.0 2002-04-18 09:58:48\n",
       "  507         3.0 2002-04-18 09:58:48\n",
       "  1097        4.0 2002-04-18 09:59:01\n",
       "  541         4.0 2002-04-18 09:59:20\n",
       "  4499        4.0 2002-04-18 10:00:05\n",
       "  3030        5.0 2002-04-18 10:00:45\n",
       "  260         5.0 2002-04-18 10:01:11\n",
       "  2408        2.0 2002-04-18 10:01:34\n",
       "  1377        3.0 2002-04-18 10:01:34\n",
       "  1221        5.0 2002-04-18 10:02:27\n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54\n",
       "  589         3.0 2002-04-18 10:03:14\n",
       "  1201        5.0 2002-04-18 10:03:14\n",
       "  1240        3.0 2002-04-18 10:03:14\n",
       "  1036        5.0 2002-04-18 10:03:31\n",
       "  1953        5.0 2002-04-18 10:03:31\n",
       "  1610        4.0 2002-04-18 10:03:31\n",
       "  2366        5.0 2002-04-18 10:03:51\n",
       "  474         4.0 2002-04-18 10:03:51\n",
       "  1291        4.0 2002-04-18 10:03:51\n",
       "  21          2.0 2002-04-18 10:03:51\n",
       "  2951        5.0 2002-04-18 10:03:51\n",
       "  4855        4.0 2002-04-18 10:04:04\n",
       "  1954        4.0 2002-04-18 10:04:04\n",
       "  3104        4.0 2002-04-18 10:04:39\n",
       "  2406        3.0 2002-04-18 10:04:39\n",
       "  2991        5.0 2002-04-18 10:04:39\n",
       "  480         3.0 2002-04-18 10:04:52\n",
       "  3578        4.0 2002-04-18 10:04:52\n",
       "  1722        4.0 2002-04-18 10:04:52\n",
       "  1275        5.0 2002-04-18 10:05:06\n",
       "  1408        2.0 2002-04-18 10:05:06\n",
       "  377         4.0 2002-04-18 10:05:24\n",
       "  380         3.0 2002-04-18 10:05:34\n",
       "  733         3.0 2002-04-18 10:05:54\n",
       "  2640        3.0 2002-04-18 10:05:54\n",
       "  3256        4.0 2002-04-18 10:06:05\n",
       "  4963        4.0 2002-04-18 10:06:20\n",
       "  1356        4.0 2002-04-18 10:06:40\n",
       "  2006        3.0 2002-04-18 10:06:40\n",
       "  10          4.0 2002-04-18 10:06:40,\n",
       "  0,\n",
       "  50),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  1193        5.0 2002-04-18 09:58:48\n",
       "  507         3.0 2002-04-18 09:58:48\n",
       "  1097        4.0 2002-04-18 09:59:01\n",
       "  541         4.0 2002-04-18 09:59:20\n",
       "  4499        4.0 2002-04-18 10:00:05\n",
       "  3030        5.0 2002-04-18 10:00:45\n",
       "  260         5.0 2002-04-18 10:01:11\n",
       "  2408        2.0 2002-04-18 10:01:34\n",
       "  1377        3.0 2002-04-18 10:01:34\n",
       "  1221        5.0 2002-04-18 10:02:27\n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54\n",
       "  589         3.0 2002-04-18 10:03:14\n",
       "  1201        5.0 2002-04-18 10:03:14\n",
       "  1240        3.0 2002-04-18 10:03:14\n",
       "  1036        5.0 2002-04-18 10:03:31\n",
       "  1953        5.0 2002-04-18 10:03:31\n",
       "  1610        4.0 2002-04-18 10:03:31\n",
       "  2366        5.0 2002-04-18 10:03:51\n",
       "  474         4.0 2002-04-18 10:03:51\n",
       "  1291        4.0 2002-04-18 10:03:51\n",
       "  21          2.0 2002-04-18 10:03:51\n",
       "  2951        5.0 2002-04-18 10:03:51\n",
       "  4855        4.0 2002-04-18 10:04:04\n",
       "  1954        4.0 2002-04-18 10:04:04\n",
       "  3104        4.0 2002-04-18 10:04:39\n",
       "  2406        3.0 2002-04-18 10:04:39\n",
       "  2991        5.0 2002-04-18 10:04:39\n",
       "  480         3.0 2002-04-18 10:04:52\n",
       "  3578        4.0 2002-04-18 10:04:52\n",
       "  1722        4.0 2002-04-18 10:04:52\n",
       "  1275        5.0 2002-04-18 10:05:06\n",
       "  1408        2.0 2002-04-18 10:05:06\n",
       "  377         4.0 2002-04-18 10:05:24\n",
       "  380         3.0 2002-04-18 10:05:34\n",
       "  733         3.0 2002-04-18 10:05:54\n",
       "  2640        3.0 2002-04-18 10:05:54\n",
       "  3256        4.0 2002-04-18 10:06:05\n",
       "  4963        4.0 2002-04-18 10:06:20\n",
       "  1356        4.0 2002-04-18 10:06:40\n",
       "  2006        3.0 2002-04-18 10:06:40\n",
       "  10          4.0 2002-04-18 10:06:40\n",
       "  2403        2.0 2002-04-18 10:06:40\n",
       "  1580        4.0 2002-04-18 10:07:35\n",
       "  349         4.0 2002-04-18 10:07:35\n",
       "  145         4.0 2002-04-18 10:07:45\n",
       "  2405        3.0 2002-04-18 10:08:01,\n",
       "  0,\n",
       "  55),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  1193        5.0 2002-04-18 09:58:48\n",
       "  507         3.0 2002-04-18 09:58:48\n",
       "  1097        4.0 2002-04-18 09:59:01\n",
       "  541         4.0 2002-04-18 09:59:20\n",
       "  4499        4.0 2002-04-18 10:00:05\n",
       "  3030        5.0 2002-04-18 10:00:45\n",
       "  260         5.0 2002-04-18 10:01:11\n",
       "  2408        2.0 2002-04-18 10:01:34\n",
       "  1377        3.0 2002-04-18 10:01:34\n",
       "  1221        5.0 2002-04-18 10:02:27\n",
       "  858         5.0 2002-04-18 10:02:27\n",
       "  1196        5.0 2002-04-18 10:02:27\n",
       "  1198        5.0 2002-04-18 10:02:27\n",
       "  1387        4.0 2002-04-18 10:02:40\n",
       "  1214        3.0 2002-04-18 10:02:54\n",
       "  589         3.0 2002-04-18 10:03:14\n",
       "  1201        5.0 2002-04-18 10:03:14\n",
       "  1240        3.0 2002-04-18 10:03:14\n",
       "  1036        5.0 2002-04-18 10:03:31\n",
       "  1953        5.0 2002-04-18 10:03:31\n",
       "  1610        4.0 2002-04-18 10:03:31\n",
       "  2366        5.0 2002-04-18 10:03:51\n",
       "  474         4.0 2002-04-18 10:03:51\n",
       "  1291        4.0 2002-04-18 10:03:51\n",
       "  21          2.0 2002-04-18 10:03:51\n",
       "  2951        5.0 2002-04-18 10:03:51\n",
       "  4855        4.0 2002-04-18 10:04:04\n",
       "  1954        4.0 2002-04-18 10:04:04\n",
       "  3104        4.0 2002-04-18 10:04:39\n",
       "  2406        3.0 2002-04-18 10:04:39\n",
       "  2991        5.0 2002-04-18 10:04:39\n",
       "  480         3.0 2002-04-18 10:04:52\n",
       "  3578        4.0 2002-04-18 10:04:52\n",
       "  1722        4.0 2002-04-18 10:04:52\n",
       "  1275        5.0 2002-04-18 10:05:06\n",
       "  1408        2.0 2002-04-18 10:05:06\n",
       "  377         4.0 2002-04-18 10:05:24\n",
       "  380         3.0 2002-04-18 10:05:34\n",
       "  733         3.0 2002-04-18 10:05:54\n",
       "  2640        3.0 2002-04-18 10:05:54\n",
       "  3256        4.0 2002-04-18 10:06:05\n",
       "  4963        4.0 2002-04-18 10:06:20\n",
       "  1356        4.0 2002-04-18 10:06:40\n",
       "  2006        3.0 2002-04-18 10:06:40\n",
       "  10          4.0 2002-04-18 10:06:40\n",
       "  2403        2.0 2002-04-18 10:06:40\n",
       "  1580        4.0 2002-04-18 10:07:35\n",
       "  349         4.0 2002-04-18 10:07:35\n",
       "  145         4.0 2002-04-18 10:07:45\n",
       "  2405        3.0 2002-04-18 10:08:01\n",
       "  3274        4.0 2002-04-18 10:08:01\n",
       "  2013        5.0 2002-04-18 10:08:01\n",
       "  2105        4.0 2002-04-18 10:08:13\n",
       "  1378        3.0 2002-04-18 10:08:48\n",
       "  2393        4.0 2002-04-18 10:09:06,\n",
       "  0,\n",
       "  60),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  ...         ...                 ...\n",
       "  4203        2.0 2002-04-18 10:09:06\n",
       "  2002        3.0 2002-04-18 10:09:35\n",
       "  288         3.0 2002-04-18 10:09:35\n",
       "  3020        4.0 2002-04-18 10:09:35\n",
       "  1372        3.0 2002-04-18 10:09:35\n",
       "  \n",
       "  [65 rows x 2 columns],\n",
       "  0,\n",
       "  65),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  ...         ...                 ...\n",
       "  3755        3.0 2002-04-18 10:09:48\n",
       "  1676        4.0 2002-04-18 10:10:00\n",
       "  1371        3.0 2002-04-18 10:10:21\n",
       "  2881        4.0 2002-04-18 10:10:21\n",
       "  494         3.0 2002-04-18 10:10:21\n",
       "  \n",
       "  [70 rows x 2 columns],\n",
       "  0,\n",
       "  70),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  ...         ...                 ...\n",
       "  533         3.0 2002-04-18 10:10:32\n",
       "  1379        4.0 2002-04-18 10:10:32\n",
       "  95          2.0 2002-04-18 10:10:44\n",
       "  4396        5.0 2002-04-18 10:10:44\n",
       "  2699        3.0 2002-04-18 10:10:55\n",
       "  \n",
       "  [75 rows x 2 columns],\n",
       "  0,\n",
       "  75),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  ...         ...                 ...\n",
       "  376         3.0 2002-04-18 10:11:15\n",
       "  4941        5.0 2002-04-18 10:11:15\n",
       "  736         3.0 2002-04-18 10:11:15\n",
       "  1391        4.0 2002-04-18 10:11:15\n",
       "  4084        3.0 2002-04-18 10:11:26\n",
       "  \n",
       "  [80 rows x 2 columns],\n",
       "  0,\n",
       "  80),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  ...         ...                 ...\n",
       "  3623        2.0 2002-04-18 10:11:43\n",
       "  466         3.0 2002-04-18 10:11:43\n",
       "  2616        2.0 2002-04-18 10:11:54\n",
       "  1917        3.0 2002-04-18 10:12:05\n",
       "  2735        2.0 2002-04-18 10:12:16\n",
       "  \n",
       "  [85 rows x 2 columns],\n",
       "  0,\n",
       "  85),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  ...         ...                 ...\n",
       "  2410        3.0 2002-04-18 10:12:30\n",
       "  3208        3.0 2002-04-18 10:12:30\n",
       "  1918        3.0 2002-04-18 10:12:30\n",
       "  4673        2.0 2002-04-18 10:12:54\n",
       "  4638        4.0 2002-04-18 10:13:09\n",
       "  \n",
       "  [90 rows x 2 columns],\n",
       "  0,\n",
       "  90),\n",
       " (         rating           timestamp\n",
       "  item_id                            \n",
       "  1210        5.0 2002-04-18 09:57:46\n",
       "  3507        4.0 2002-04-18 09:57:46\n",
       "  1374        3.0 2002-04-18 09:58:00\n",
       "  2349        2.0 2002-04-18 09:58:00\n",
       "  1961        3.0 2002-04-18 09:58:14\n",
       "  ...         ...                 ...\n",
       "  434         3.0 2002-04-18 10:13:20\n",
       "  2411        3.0 2002-04-18 10:13:20\n",
       "  4915        2.0 2002-04-18 10:13:42\n",
       "  1388        3.0 2002-04-18 10:13:42\n",
       "  3257        1.0 2002-04-18 10:13:56\n",
       "  \n",
       "  [95 rows x 2 columns],\n",
       "  0,\n",
       "  95)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TimebinSimilarity.get_neighbour_timebins(movielens.ratings, \n",
    "                                         448, 1214,\n",
    "                                         5, 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbour_id</th>\n",
       "      <th>n_common</th>\n",
       "      <th>pearson_corr</th>\n",
       "      <th>timebin_i</th>\n",
       "      <th>timebin_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.470784</td>\n",
       "      <td>320</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.764283</td>\n",
       "      <td>325</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>0.534690</td>\n",
       "      <td>315</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>0.531535</td>\n",
       "      <td>320</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>0.531535</td>\n",
       "      <td>315</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>0.470014</td>\n",
       "      <td>210</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>0.778826</td>\n",
       "      <td>225</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "      <td>0.685565</td>\n",
       "      <td>315</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>91</td>\n",
       "      <td>5</td>\n",
       "      <td>0.683138</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>91</td>\n",
       "      <td>5</td>\n",
       "      <td>0.676055</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "      <td>0.785946</td>\n",
       "      <td>315</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "0             42         4      0.470784        320            20\n",
       "1             42         4      0.764283        325            25\n",
       "2             42         5      0.534690        315            35\n",
       "3             42         5      0.531535        320            40\n",
       "4             42         5      0.531535        315            45\n",
       "5             45         3      0.470014        210            35\n",
       "6             45         3      0.778826        225            45\n",
       "7             91         3      0.685565        315            15\n",
       "8             91         5      0.683138        300            25\n",
       "9             91         5      0.676055        300            30\n",
       "10            91         7      0.785946        315            35"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TimebinSimilarity.get_knn(ratings=movielens.ratings, \n",
    "                                   user_id=448,\n",
    "                                   k=10,\n",
    "                                   timebin_i=60, \n",
    "                                   timebin_size=150, \n",
    "                                   neighbour_min_s=5, \n",
    "                                   neighbour_max_s=50, \n",
    "                                   neighbour_step_size=5, \n",
    "                                   corr_threshold=0.1, \n",
    "                                   movie_id=1961,\n",
    "                                   min_common_neighbour_ratings=3, \n",
    "                                   min_common_elements=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356 4.975845410628019 4.050626795326627 5.0\n",
      "Normal RMSE: 0.001 Timebin RMSE:1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': [(464,\n",
       "   11,\n",
       "   43,\n",
       "       neighbour_id  n_common  pearson_corr  timebin_i  timebin_size\n",
       "   0             68         4      0.166667         15            15\n",
       "   1             68         8      0.166667          0            35\n",
       "   3            239         4      0.376020         40            20\n",
       "   4            239         6      0.924269         25            25\n",
       "   5            239         6      0.594728         30            30\n",
       "   6            239         8      0.382993         35            35\n",
       "   7            239         9      0.879978          0            45\n",
       "   8            246         3      1.000000        100            10\n",
       "   9            246         4      1.000000        100            20\n",
       "   10           246         6      0.766942        100            25)],\n",
       " 'normal_predictions': [(4.975845410628019, 5.0)],\n",
       " 'timebin_predictions': [(4.050626795326627, 5.0)]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnalizeTimebinSimilarity.analize_timebin_prediction(movie_ratings, movielens.ratings, n=1, s=43, k=10,\n",
    "                           min_neighbour=3, corr_threshold=0.1, \n",
    "                           neighbour_min_s=5, neighbour_max_s=50, neighbour_step_size=5, \n",
    "                           min_common_elements=3,\n",
    "                           min_common_neighbour_ratings=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualInformationSimilarity:\n",
    "    \"\"\"\n",
    "    Mutual information is used to give recommendations.\n",
    "    It is based on entropy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lowest_rating, highest_rating, rating_increment, ratings):\n",
    "        self.lowest_rating = lowest_rating\n",
    "        self.highest_rating = highest_rating\n",
    "        self.rating_increment = rating_increment\n",
    "        self.ratings = ratings\n",
    "    \n",
    "    def get_n_rating(self, common_ratings, isFirstUser:bool, rating):\n",
    "        if isFirstUser == True:\n",
    "            return common_ratings.loc[common_ratings['rating_x'] == rating].count()[0]\n",
    "        else:\n",
    "            return common_ratings.loc[common_ratings['rating_y'] == rating].count()[0]\n",
    "    \n",
    "    def get_n_rating_pair(self, common_ratings, user1_rating, user2_rating):\n",
    "        return common_ratings.loc[(common_ratings['rating_x'] == user1_rating) & \n",
    "                                  (common_ratings['rating_y'] == user2_rating) ].count()[0]\n",
    "    \n",
    "    def mutual_information(self, user1_id, user2_id):\n",
    "        common_ratings = pd.merge(self.ratings.loc[(self.ratings['user_id'] == user1_id)],\n",
    "                                  self.ratings.loc[(self.ratings['user_id'] == user2_id)],\n",
    "                                  on=\"item_id\")\n",
    "        n_common = common_ratings.count()[0]\n",
    "        if(n_common == 0):\n",
    "            return 0.0\n",
    "        # Calculate entropies\n",
    "        user1_entropy, n1 = self.user_entropy(common_ratings, True, n_common)\n",
    "        user2_entropy, n2 = self.user_entropy(common_ratings, False, n_common)\n",
    "        user1_and_2_entropy, n1_2 = self.cross_entropy(common_ratings, n_common)\n",
    "        \n",
    "        # Handle bias of the entropy values and calculate mutual information\n",
    "        I  = user1_entropy + ((n1 - 1) / (2*n_common))         # User1 entropy \n",
    "        I += user2_entropy + ((n2 - 1) / (2*n_common))         # Plus User2 entropy\n",
    "        I -= user1_and_2_entropy + ((n1_2 - 1) / (2*n_common)) # Plus Cross entropy\n",
    "        return I\n",
    "    \n",
    "    def user_entropy(self, common_ratings, isFirstUser:bool, n_common):\n",
    "        entropy = 0.0\n",
    "        rating = self.lowest_rating\n",
    "        n = 0\n",
    "        while(rating <= self.highest_rating):\n",
    "            n_rating = self.get_n_rating(common_ratings, isFirstUser, rating)\n",
    "            Pi = n_rating / n_common\n",
    "            if(Pi > 0):\n",
    "                entropy -= Pi * math.log(Pi)\n",
    "                n += 1\n",
    "            rating += self.rating_increment\n",
    "            \n",
    "        return entropy, n\n",
    "    \n",
    "    def cross_entropy(self, common_ratings, n_common):\n",
    "        entropy = 0.0\n",
    "        user1_rating = self.lowest_rating\n",
    "        n = 0\n",
    "        while(user1_rating <= self.highest_rating):\n",
    "            user2_rating = self.lowest_rating\n",
    "            while(user2_rating <= self.highest_rating):\n",
    "                n_rating = self.get_n_rating_pair(common_ratings, user1_rating, user2_rating)\n",
    "                Pi = n_rating / n_common\n",
    "                if(Pi > 0):\n",
    "                    entropy -= Pi * math.log(Pi)\n",
    "                    n += 1\n",
    "                user2_rating += self.rating_increment\n",
    "            user1_rating += self.rating_increment\n",
    "                    \n",
    "        return entropy, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualInformationPredict:\n",
    "    \n",
    "    def __init__(self, lowest_rating, highest_rating, rating_increment, ratings):\n",
    "        self.MI = MutualInformationSimilarity(lowest_rating, highest_rating, rating_increment, ratings)\n",
    "    \n",
    "    def get_k_neighbors(self, user_id, k):\n",
    "        mi_data = list()\n",
    "        for i in range(0,610):\n",
    "            corr = MI.mutual_information(448, i)\n",
    "            if(corr != 0):\n",
    "                mi_data.append( (i, corr) )\n",
    "        \n",
    "        mi_df = pd.DataFrame(mi_data)\n",
    "        mi_df.columns=['NeighbourId', 'corr']\n",
    "        mi_df.set_index('NeighbourId', inplace=True)\n",
    "        mi_df.sort_values(by='corr', ascending=False, inplace=True)\n",
    "        return mi_df.drop(user_id, errors='ignore').head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI = MutualInformation(0.5, 5.0, 0.5, movielens.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2924618091542328"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI.mutual_information(448, 336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi = PredictWithMI(0.5, 5, 0.5, movielens.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeighbourId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2.018405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.431946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>1.379854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1.306996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1.243255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.221849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>1.161607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1.150271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1.052595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1.034230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 corr\n",
       "NeighbourId          \n",
       "448          2.018405\n",
       "175          1.431946\n",
       "506          1.379854\n",
       "207          1.306996\n",
       "478          1.243255\n",
       "87           1.221849\n",
       "549          1.161607\n",
       "253          1.150271\n",
       "576          1.052595\n",
       "138          1.034230"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmi.get_k_neighbors(448, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeighbourId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.431946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>1.379854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1.306996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1.243255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.221849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>1.161607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1.150271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1.052595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1.034230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1.003840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 corr\n",
       "NeighbourId          \n",
       "175          1.431946\n",
       "506          1.379854\n",
       "207          1.306996\n",
       "478          1.243255\n",
       "87           1.221849\n",
       "549          1.161607\n",
       "253          1.150271\n",
       "576          1.052595\n",
       "138          1.034230\n",
       "285          1.003840"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_df.drop(448).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

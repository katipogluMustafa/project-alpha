{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeConstraint:\n",
    "    \"\"\"\n",
    "    TimeConstraint is a constraint on the timestamp of the movie ratings.\n",
    "    We classify a TimeConstraint as either max_time_constraint or time_bin_constraint.\n",
    "    max_time_constraint is used to simulate real life in which we do not know the future but all the data up until one point in time.\n",
    "    time_bin_constraint is used to grab a portion of a time interval where starting and ending points are strictly defined and data is well known.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, end_dt, start_dt=None):\n",
    "        \"\"\"\n",
    "        When end_dt is only given, system will have a max time constraint only.\n",
    "\n",
    "        When end_dt and start_dt are given, system will have beginning end ending boundary.\n",
    "\n",
    "        :param end_dt: The maximum limit of the time constraint.\n",
    "        :param start_dt: The minimum limit of the time constraint.\n",
    "            Always set start_dt to None if you change the object from time_bin to max_limit.\n",
    "        \"\"\"\n",
    "        self.end_dt = end_dt\n",
    "        self.start_dt = start_dt\n",
    "\n",
    "    def is_valid_time_bin(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether this TimeConstraint object represents a valid time bin.\n",
    "        \"\"\"\n",
    "        if self.is_time_bin() and (self._end_dt > self._start_dt):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_valid_max_limit(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether this TimeConstraint represents a valid max time limit.\n",
    "        \"\"\"\n",
    "        if (self._end_dt is not None) and (self._start_dt is None):\n",
    "            return True\n",
    "\n",
    "    def is_time_bin(self) -> bool:\n",
    "        if (self._start_dt is not None) and (self._end_dt is not None):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Comparing TimeConstraints\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return self._start_dt == other.start_dt and self._end_dt == other.end_dt\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return self._start_dt != other.start_dt or self._end_dt != other.end_dt\n",
    "\n",
    "    # Properties\n",
    "\n",
    "    @property\n",
    "    def end_dt(self):\n",
    "        return self._end_dt\n",
    "\n",
    "    @end_dt.setter\n",
    "    def end_dt(self, value):\n",
    "        self._end_dt = value\n",
    "\n",
    "    @property\n",
    "    def start_dt(self):\n",
    "        return self._start_dt\n",
    "\n",
    "    @start_dt.setter\n",
    "    def start_dt(self, value):\n",
    "        self._start_dt = value\n",
    "\n",
    "    # Printing TimeConstraints\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"(start = {self._start_dt}, end= {self._end_dt})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"(start = {self._start_dt}, end= {self._end_dt})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    \"\"\"\n",
    "    Cache is introduced as a way to speed up the bulk analysis. In a normal recommendations, this class should not exist.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 is_ratings_cached=False,\n",
    "                 ratings=None,\n",
    "                 is_movies_cached=False,\n",
    "                 movies=None,\n",
    "                 is_movie_ratings_cached=False,\n",
    "                 movie_ratings=None,\n",
    "                 is_user_movie_matrix_cached=False,\n",
    "                 user_movie_matrix=None,\n",
    "                 is_user_correlations_cached=False,\n",
    "                 user_correlations=None,\n",
    "                 min_common_elements=5,\n",
    "                 use_avg_ratings_cache=True):\n",
    "        \"\"\" Cached data is only valid when the boolean specifier is True \"\"\"\n",
    "\n",
    "        # 30% performance\n",
    "        self.is_ratings_cached = is_ratings_cached\n",
    "        self.ratings = ratings\n",
    "\n",
    "        # 7 fold performance gain on 'movie' related queries\n",
    "        self.is_movies_cached = is_movies_cached\n",
    "        self.movies = movies\n",
    "\n",
    "        self.is_movie_ratings_cached = is_movie_ratings_cached\n",
    "        self.movie_ratings = movie_ratings\n",
    "\n",
    "        self.is_user_movie_matrix_cached = is_user_movie_matrix_cached\n",
    "        self.user_movie_matrix = user_movie_matrix\n",
    "\n",
    "        self.is_user_correlations_cached = is_user_correlations_cached\n",
    "        self.user_correlations = user_correlations\n",
    "\n",
    "        self.min_common_elements = min_common_elements\n",
    "\n",
    "        # if we use avg ratings cache, on average 10 fold performance gain\n",
    "        self.use_avg_ratings_cache = use_avg_ratings_cache\n",
    "        if self.use_avg_ratings_cache:\n",
    "            self.avg_user_ratings = self.create_user_avg_rating_cache()\n",
    "        else:\n",
    "            self.avg_user_ratings = None\n",
    "\n",
    "    def create_user_avg_rating_cache(self):\n",
    "        if self.is_ratings_cached:\n",
    "            data = self.ratings\n",
    "        else:\n",
    "            data = self.movie_ratings\n",
    "        return data.groupby('user_id')[['rating']].mean()\n",
    "\n",
    "    def get_user_corrs(self, min_common_elements, time_constraint=None):\n",
    "        \"\"\"\n",
    "        If cache enabled and matches, returns the cache, else none\n",
    "        :param min_common_elements: min common element in between users in order them to become neighbours\n",
    "        :param time_constraint: used in temporal caches only(used in subclass)\n",
    "        :return: user correlation matrix if cache hit, else None\n",
    "        \"\"\"\n",
    "        if self.is_user_correlations_cached:\n",
    "            if self.min_common_elements == min_common_elements:\n",
    "                return self.user_correlations\n",
    "        return None\n",
    "\n",
    "    # Properties\n",
    "    @property\n",
    "    def ratings(self):\n",
    "        return self._ratings\n",
    "\n",
    "    @ratings.setter\n",
    "    def ratings(self, value):\n",
    "        self._ratings = value\n",
    "\n",
    "    @property\n",
    "    def movies(self):\n",
    "        return self._movies\n",
    "\n",
    "    @movies.setter\n",
    "    def movies(self, value):\n",
    "        self._movies = value\n",
    "\n",
    "    @property\n",
    "    def movie_ratings(self):\n",
    "        return self._movie_ratings\n",
    "\n",
    "    @movie_ratings.setter\n",
    "    def movie_ratings(self, value):\n",
    "        self._movie_ratings = value\n",
    "\n",
    "    @property\n",
    "    def user_movie_matrix(self):\n",
    "        return self._user_movie_matrix\n",
    "\n",
    "    @user_movie_matrix.setter\n",
    "    def user_movie_matrix(self, value):\n",
    "        self._user_movie_matrix = value\n",
    "\n",
    "    @property\n",
    "    def user_correlations(self):\n",
    "        return self._user_correlations\n",
    "\n",
    "    @user_correlations.setter\n",
    "    def user_correlations(self, value):\n",
    "        self._user_correlations = value\n",
    "\n",
    "    @property\n",
    "    def min_common_elements(self):\n",
    "        return self._min_common_elements\n",
    "\n",
    "    @min_common_elements.setter\n",
    "    def min_common_elements(self, value):\n",
    "        self._min_common_elements = value\n",
    "\n",
    "\n",
    "class TemporalCache(Cache):\n",
    "\n",
    "    def __init__(self,\n",
    "                 time_constraint: TimeConstraint,\n",
    "                 is_ratings_cached=False,\n",
    "                 ratings=None,\n",
    "                 is_movies_cached=False,\n",
    "                 movies=None,\n",
    "                 is_movie_ratings_cached=False,\n",
    "                 movie_ratings=None,\n",
    "                 is_user_movie_matrix_cached=False,\n",
    "                 user_movie_matrix=None,\n",
    "                 is_user_correlations_cached=False,\n",
    "                 user_correlations=None,\n",
    "                 min_common_elements=5,\n",
    "                 use_avg_ratings_cache=True,\n",
    "                 use_bulk_corr_cache=True):\n",
    "\n",
    "        super().__init__(is_ratings_cached=is_ratings_cached,\n",
    "                         ratings=ratings,\n",
    "                         is_movies_cached=is_movies_cached,\n",
    "                         movies=movies,\n",
    "                         is_movie_ratings_cached=is_movie_ratings_cached,\n",
    "                         movie_ratings=movie_ratings,\n",
    "                         is_user_movie_matrix_cached=is_user_movie_matrix_cached,\n",
    "                         user_movie_matrix=user_movie_matrix,\n",
    "                         is_user_correlations_cached=is_user_correlations_cached,\n",
    "                         user_correlations=user_correlations,\n",
    "                         min_common_elements=min_common_elements,\n",
    "                         use_avg_ratings_cache=use_avg_ratings_cache)\n",
    "\n",
    "        self.time_constraint = time_constraint\n",
    "        self.use_bulk_corr_cache = use_bulk_corr_cache\n",
    "        self.user_corrs_in_bulk = None\n",
    "\n",
    "    def is_temporal_cache_valid(self):\n",
    "        # No TimeConstraint, valid\n",
    "        if self._time_constraint is None:\n",
    "            return True\n",
    "        # Bin TimeConstraint or Max Limit TimeConstraint, valid\n",
    "        if self._time_constraint.is_valid_time_bin() or self._time_constraint.is_valid_max_limit():\n",
    "            return True\n",
    "        # Else, Not Valid\n",
    "        return False\n",
    "\n",
    "    def get_user_corrs_from_bulk(self, min_common_elements, time_constraint, bin_size):\n",
    "        if ((self.user_corrs_in_bulk is None) or (self.user_corrs_in_bulk is None)\n",
    "                or (time_constraint is None) or self.min_common_elements != min_common_elements):\n",
    "            return None\n",
    "\n",
    "        if time_constraint.is_valid_max_limit():\n",
    "            return self.user_corrs_in_bulk.get(time_constraint.end_dt.year)\n",
    "\n",
    "        if bin_size == -1:\n",
    "            return None\n",
    "\n",
    "        bins = self.user_corrs_in_bulk.get(bin_size)\n",
    "        if bins is not None:\n",
    "            return bins.get(time_constraint.start_dt.year)\n",
    "\n",
    "    def get_user_corrs(self, min_common_elements, time_constraint=None):\n",
    "        \"\"\"\n",
    "        If corrs are cached returns the cache, else None\n",
    "\n",
    "        :param min_common_elements: min common element in between users in order them to become neighbours\n",
    "        :param time_constraint: time constraint on user correlations\n",
    "        :return: user correlation matrix if cache found, else None\n",
    "        \"\"\"\n",
    "        if self.is_user_correlations_cached:\n",
    "            if self.time_constraint == time_constraint and self.min_common_elements == min_common_elements:\n",
    "                return self.user_correlations\n",
    "        return None\n",
    "\n",
    "    def set_user_corrs(self, user_corrs, min_common_elements, time_constraint):\n",
    "        # Only set when caching is open for user_correlations\n",
    "        if self.is_user_correlations_cached:\n",
    "            self._time_constraint = time_constraint\n",
    "            self.min_common_elements = min_common_elements\n",
    "            self.user_correlations = user_corrs\n",
    "\n",
    "    @property\n",
    "    def time_constraint(self):\n",
    "        return self._time_constraint\n",
    "\n",
    "    @time_constraint.setter\n",
    "    def time_constraint(self, value):\n",
    "        self._time_constraint = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    \"\"\"\n",
    "    Accuracy class provides utility methods in order to measure accuracy of our analysis.\n",
    "    \n",
    "    Supported Measures:\n",
    "    rmse, accuracy, balanced accuracy, informedness, markedness, \n",
    "    f1, mcc, precision, recall, specificity, NPV and \n",
    "    other threshold measures where we round ratings less than 3.5 to min rating, upper to max rating and use supported measures on this data.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def rmse(predictions) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Root Mean Square Error of given list or Dataframe of (prediction, actual) data.\n",
    "        \n",
    "        In case rmse value is found 0, it is returned as 0.001 to differentiate between successfull rmse\n",
    "        calculation and erroneous calculations where number of predictions in data is zero.\n",
    "        \"\"\"\n",
    "        \n",
    "        # In case dataframe of predictions wher each row[0]=prediction, row[1]=actual rating\n",
    "        if type(predictions) is pd.DataFrame:\n",
    "            number_of_predictions = 0\n",
    "            sum_of_square_differences = 0.0\n",
    "            for row in predictions.itertuples(index=False):\n",
    "                prediction = row[0]\n",
    "                # In case valid prediction is made(0 is invalid, minimum 0.5 in movielens dataset)\n",
    "                if prediction != 0:\n",
    "                    # Round the ratings to the closest half or exact number\n",
    "                    # since movielens dataset only containst ratings 0.5, 1, 1.5,..., 4, 4.5, 5\n",
    "                    actual = Accuracy.half_round_rating(row[1])\n",
    "                    prediction = Accuracy.half_round_rating(prediction)\n",
    "                    \n",
    "                    sum_of_square_differences += (actual - prediction) ** 2\n",
    "                    number_of_predictions += 1\n",
    "                \n",
    "                if number_of_predictions == 0:\n",
    "                    return 0 \n",
    "                rmse_value = sum_of_square_differences / number_of_predictions\n",
    "            return rmse_value if rmse_value != 0 else 0.001\n",
    "        # In case list of predictions where each element is (prediction, actual)\n",
    "        elif type(predictions) is list:\n",
    "            number_of_predictions = 0\n",
    "            sum_of_square_differences = 0.0\n",
    "            for prediction, actual in predictions:\n",
    "                if prediction != 0:                  # if the prediction is valid\n",
    "                    actual = Accuracy.half_round_rating(actual)\n",
    "                    prediction = Accuracy.half_round_rating(prediction)\n",
    "                    \n",
    "                    sum_of_square_differences += (actual - prediction) ** 2\n",
    "                    number_of_predictions += 1\n",
    "                \n",
    "            if number_of_predictions == 0:\n",
    "                return 0\n",
    "        \n",
    "            rmse_value = sum_of_square_differences / number_of_predictions \n",
    "            return rmse_value if rmse_value != 0 else 0.001    \n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_accuracy(predictions) -> float:\n",
    "        \"\"\"\n",
    "        Threshold accuracy is the rate of sucessful prediction when we round \n",
    "        ratings between 0.5 and 3.5 to the lowest rating(0.5) ,\n",
    "        ratings between 3.5 and 5 to the highest rating(5)\n",
    "        \n",
    "        Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if type(predictions) is pd.DataFrame:\n",
    "            number_of_predictions = 0\n",
    "            number_of_hit = 0\n",
    "            for row in predictions.itertuples(index=False):\n",
    "                # row[1] : actual rating, row[0] : prediction\n",
    "                prediction = row[0]\n",
    "                if prediction != 0:\n",
    "                    actual = Accuracy.threshold_round_rating(row[1])\n",
    "                    prediction = Accuracy.threshold_round_rating(prediction)\n",
    "                    \n",
    "                    if actual == prediction:\n",
    "                        number_of_hit += 1\n",
    "                    number_of_predictions += 1\n",
    "            return number_of_hit / number_of_predictions if number_of_predictions != 0 else 0\n",
    "        elif type(predictions) is list:            \n",
    "            number_of_predictions = 0\n",
    "            number_of_hit = 0\n",
    "            for prediction, actual in predictions:\n",
    "                if prediction != 0:\n",
    "                    actual = Accuracy.threshold_round_rating(actual)\n",
    "                    prediction = Accuracy.threshold_round_rating(prediction)\n",
    "                    \n",
    "                    if actual == prediction:\n",
    "                        number_of_hit += 1\n",
    "                        \n",
    "                    number_of_predictions += 1\n",
    "            return number_of_hit / number_of_predictions if number_of_predictions != 0 else 0\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_analize(predictions):\n",
    "        \"\"\"\n",
    "        Analize the threshold predictions with all metrics found in the Accuracy class.\n",
    "        \"\"\"\n",
    "        \n",
    "        TP, FN, FP, TN = Accuracy.threshold_confusion_matrix(predictions)\n",
    "        precision = Accuracy.precision(TP, FP)     # also called PPV\n",
    "        recall = Accuracy.recall(TP, FN)           # also called TPR\n",
    "        specificity = Accuracy.specificity(FP, TN) # also called TNR\n",
    "        NPV = Accuracy.negative_predictive_value(FN, TN)\n",
    "        \n",
    "        accuracy = Accuracy.accuracy(TP, FN, FP, TN)\n",
    "        balanced_accuracy = Accuracy.balanced_accuracy(TPR=recall, TNR=specificity)\n",
    "        informedness = Accuracy.informedness(TPR=recall, TNR=specificity)\n",
    "        markedness = Accuracy.markedness(PPV=precision, NPV=NPV)\n",
    "        \n",
    "        f1 = Accuracy.f_measure(precision, recall)\n",
    "        mcc = Accuracy.mcc(TP, FN, FP, TN)\n",
    "        \n",
    "                \n",
    "        output = {\n",
    "                  \"accuracy\"         :round(accuracy, 3),\n",
    "                  \"balanced_accuracy\":round(balanced_accuracy, 3),\n",
    "                  \"informedness\"     :round(informedness, 3),\n",
    "                  \"markedness\"       :round(markedness, 3),\n",
    "                  \"f1\"               :round(f1, 3),\n",
    "                  \"mcc\"              :round(mcc, 3),\n",
    "                  \"precision\"        :round(precision, 3),\n",
    "                  \"recall\"           :round(recall, 3),\n",
    "                  \"specificity\"      :round(specificity, 3),\n",
    "                  \"NPV\"              :round(NPV, 3)\n",
    "                 }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def analize(predictions):\n",
    "        \"\"\"\n",
    "        Analize the threshold predictions with all metrics found in the Accuracy class.\n",
    "        \n",
    "        https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "        \n",
    "        Returns analysis for each class as list\n",
    "        :return: accuracy, balanced_accuracy, informedness, markedness, f1, mcc, precision, recall, specificity, NPV\n",
    "        \"\"\"\n",
    "        confusion_mtr = Accuracy.confusion_matrix(predictions)\n",
    "        \n",
    "        # Use macro averaging (https://stats.stackexchange.com/questions/187768/matthews-correlation-coefficient-with-multi-class)\n",
    "        precision = [0] * 10   # 10 is the number of classes found \n",
    "        recall = [0] * 10      # 0.5 -> Class 0 , 1 -> Class 1, 1.5 -> Class 2 .... \n",
    "        specificity = [0] * 10\n",
    "        NPV = [0] * 10 \n",
    "        \n",
    "        accuracy = [0] * 10 \n",
    "        balanced_accuracy = [0] * 10 \n",
    "        informedness = [0] * 10 \n",
    "        markedness = [0] * 10 \n",
    "        \n",
    "        f1 = [0] * 10 \n",
    "        mcc = [0] * 10\n",
    "        \n",
    "        for i in range(0, 10): # For Each Class\n",
    "            TP, FN, FP, TN = Accuracy.confusion_matrix_one_against_all(confusion_mtr, i)\n",
    "            precision[i] = Accuracy.precision(TP, FP)     # also called PPV\n",
    "            recall[i] = Accuracy.recall(TP, FN)           # also called TPR\n",
    "            specificity[i] = Accuracy.specificity(FP, TN) # also called TNR\n",
    "            NPV[i] = Accuracy.negative_predictive_value(FN, TN)\n",
    "\n",
    "            accuracy[i] = Accuracy.accuracy(TP, FN, FP, TN)\n",
    "            balanced_accuracy[i] = Accuracy.balanced_accuracy(TPR=recall[i], TNR=specificity[i])\n",
    "            informedness[i] = Accuracy.informedness(TPR=recall[i], TNR=specificity[i])\n",
    "            markedness[i] = Accuracy.markedness(PPV=precision[i], NPV=NPV[i])\n",
    "\n",
    "            f1[i] = Accuracy.f_measure(precision[i], recall[i])\n",
    "            mcc[i] = Accuracy.mcc(TP, FN, FP, TN)\n",
    "        \n",
    "        output = {\n",
    "                  \"accuracy\"         :Accuracy.round_list_elements(accuracy, 3),\n",
    "                  \"balanced_accuracy\":Accuracy.round_list_elements(balanced_accuracy, 3),\n",
    "                  \"informedness\"     :Accuracy.round_list_elements(informedness, 3),\n",
    "                  \"markedness\"       :Accuracy.round_list_elements(markedness, 3),\n",
    "                  \"f1\"               :Accuracy.round_list_elements(f1, 3),\n",
    "                  \"mcc\"              :Accuracy.round_list_elements(mcc, 3),\n",
    "                  \"precision\"        :Accuracy.round_list_elements(precision, 3),\n",
    "                  \"recall\"           :Accuracy.round_list_elements(recall, 3),\n",
    "                  \"specificity\"      :Accuracy.round_list_elements(specificity, 3),\n",
    "                  \"NPV\"              :Accuracy.round_list_elements(NPV, 3)\n",
    "                 }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def round_list_elements(l, precision):\n",
    "        \"\"\"\n",
    "        :param l: list of floats\n",
    "        :param precision: precision after dot\n",
    "        \"\"\"\n",
    "        return [ round(x, precision) for x in l ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy_multi_class(confusion_mtr):\n",
    "        length = len(confusion_mtr)\n",
    "        numenator = 0\n",
    "        denuminator = 0 \n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                temp = confusion_mtr[i][j]\n",
    "                denuminator += temp\n",
    "                if i == j:\n",
    "                    numenator += temp\n",
    "        return numenator / denominator\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(TP, FN, FP, TN):\n",
    "        return  (TP + TN) / (TP + FN + FP + TN)\n",
    "    \n",
    "    @staticmethod\n",
    "    def balanced_accuracy(TPR, TNR):\n",
    "        \"\"\"\n",
    "        :param TPR : True Positive Rate or recall or sensitivity\n",
    "        :param TNR : True Negative Rate or specificity or  selectivity\n",
    "        \"\"\"\n",
    "        return (TPR + TNR) / 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def informedness(TPR, TNR):\n",
    "        \"\"\"\n",
    "        :param TPR : True Positive Rate or recall or sensitivity\n",
    "        :param TNR : True Negative Rate or specificity or  selectivity\n",
    "        \"\"\"\n",
    "        return TPR + TNR - 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def markedness(PPV, NPV):\n",
    "        \"\"\"\n",
    "        :param PPV: Positive Predictive Value also known as precision\n",
    "        :param NPV: Negative Predictive Value\n",
    "        \"\"\"\n",
    "        return PPV + NPV - 1 \n",
    "    \n",
    "    @staticmethod\n",
    "    def precision(TP, FP):\n",
    "        \"\"\"\n",
    "        Also called as precision or positive predictive value (PPV)\n",
    "        \n",
    "        Precision = TP / (TP + FP) for binary class\n",
    "        Precision = TP / (All Predicted Positive) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TP + FP\n",
    "        return TP / denuminator if denuminator != 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def negative_predictive_value(FN, TN):\n",
    "        \"\"\"     \n",
    "        NPV = TN / (TN + FN) for binary class\n",
    "        NPV = TN / (All Predicted Negative) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TN + FN\n",
    "        return TN / denuminator if denuminator != 0 else 0    \n",
    "    \n",
    "    @staticmethod\n",
    "    def recall(TP, FN):\n",
    "        \"\"\"\n",
    "        Also called as sensitivity, recall, hitrate, or true positive rate(TPR)\n",
    "        Recall = TP / (TP + FN) for binary class\n",
    "        Recall = TP / (All Actual Positive) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = TP + FN\n",
    "        return TP / denuminator if denuminator != 0 else 0    \n",
    "    \n",
    "    @staticmethod\n",
    "    def specificity(FP, TN):\n",
    "        \"\"\"\n",
    "        Also called as specificity, selectivity or true negative rate (TNR)\n",
    "        specificity = TN / (TP + FN) for binary class\n",
    "        specificity = TN / (All Actual Negative) for multi class\n",
    "        \"\"\"\n",
    "        denuminator = FP + TN \n",
    "        return TN / denuminator if denuminator != 0 else 0 \n",
    "    \n",
    "    @staticmethod\n",
    "    def f_measure(precision, recall):\n",
    "        \"\"\"\n",
    "        F-Measure is the harmonic mean of the precision and recall.\n",
    "        \"\"\"\n",
    "        sum_of_both = precision + recall\n",
    "        return (2 * precision * recall) / sum_of_both if sum_of_both != 0 else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mcc(TP, FN, FP, TN):\n",
    "        \"\"\"\n",
    "        MCC(Matthews Correlation Coefficient)\n",
    "        \"\"\"\n",
    "        # Calulate Matthews Correlation Coefficient\n",
    "        numenator   = (TP * TN) - (FP * FN) \n",
    "        denominator = (TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)\n",
    "        denominator = math.sqrt(denominator) if denominator > 0 else 0\n",
    "        return numenator / denominator if denominator != 0 else 0\n",
    "  \n",
    "# deprecated -> now, we use macro averaging instad of multi class versions of metrics\n",
    "#     @staticmethod\n",
    "#     def mcc_multi_class(confusion_mtr):\n",
    "#         # https://en.wikipedia.org/wiki/Matthews_correlation_coefficient#Multiclass_case\n",
    "#         # https://stats.stackexchange.com/questions/187768/matthews-correlation-coefficient-with-multi-class\n",
    "#         length = len(confusion_mtr)\n",
    "#         numenator = 0\n",
    "#         for k in range(length):\n",
    "#             for l in range(length):\n",
    "#                 for m in range(length):\n",
    "#                     numenator += confusion_mtr[k][k] * confusion_mtr[l][m]\n",
    "#                     numenator -= confusion_mtr[k][l] * confusion_mtr[m][k]\n",
    "\n",
    "#         denuminator_1 = 0\n",
    "#         for k in range(length):\n",
    "#             denuminator_part_1 = 0\n",
    "#             for l in range(length):\n",
    "#                 denuminator_part_1 += confusion_mtr[k][l]\n",
    "\n",
    "#             denuminator_part_2 = 0\n",
    "#             for f in range(length):\n",
    "#                 if f != k:\n",
    "#                     for g in range(length):\n",
    "#                         denuminator_part_2 += confusion_mtr[f][g]\n",
    "#             denuminator_1 += denuminator_part_1 * denuminator_part_2\n",
    "\n",
    "#         denuminator_2 = 0\n",
    "#         for k in range(length):\n",
    "#             denuminator2_part_1 = 0\n",
    "#             for l in range(length):\n",
    "#                 denuminator2_part_1 += confusion_mtr[l][k]\n",
    "\n",
    "#             denuminator2_part_2 = 0\n",
    "#             for f in range(length):\n",
    "#                 if f != k:\n",
    "#                     for g in range(length):\n",
    "#                         denuminator2_part_2 += confusion_mtr[g][f]\n",
    "#             denuminator_2 += denuminator2_part_1 * denuminator2_part_2\n",
    "#         return numenator / math.sqrt(denuminator_1) * math.sqrt(denuminator_2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def confusion_matrix_one_against_all(confusion_mtr, class_i):\n",
    "        \"\"\"\n",
    "        Create binary confusion matrix out of multi-class confusion matrix\n",
    "        \n",
    "        Positive Class: class_i\n",
    "        Negative Class: non class_i\n",
    "                \n",
    "        TP: True Positive   FN: False Negative\n",
    "        FP: False Positive  TN: True Negative\n",
    "        \n",
    "        \"TP of Class_1\" is all Class_1 instances that are classified as Class_1.\n",
    "        \"TN of Class_1\" is all non-Class_1 instances that are not classified as Class_1.\n",
    "        \"FP of Class_1\" is all non-Class_1 instances that are classified as Class_1.\n",
    "        \"FN of Class_1\" is all Class_1 instances that are not classified as Class_1.\n",
    "        # https://www.researchgate.net/post/How_do_you_measure_specificity_and_sensitivity_in_a_multiple_class_classification_problem\n",
    "        \n",
    "        --> Input matrix\n",
    "                 | 0 Prediction | 1 Prediction | 2 Prediction | .....\n",
    "        0 Class  |     T0       |     ..       |      ..      |\n",
    "        1 Class  |     ..       |     T1       |      ..      | \n",
    "        2 Class  |     ..       |     ..       |      T2      |\n",
    "        \n",
    "        --> Output matrix\n",
    "        \n",
    "                        | Positive Prediction | Negative Prediction\n",
    "        Positive Class  |       TP            |       FN\n",
    "        Negative Class  |       FP            |       TN\n",
    "        \n",
    "        :param confusion_mtr: 10 class confusion matrix designed for movielens\n",
    "        :param class_i: index of the class we are interested in(0-9)\n",
    "        :return: TP, FN, FP, TN\n",
    "        \"\"\"\n",
    "        length = len(confusion_mtr)\n",
    "\n",
    "        TP = confusion_mtr[class_i][class_i] \n",
    "        \n",
    "        actual_class_i_count = 0 \n",
    "        for i in range(length):  # sum of the row\n",
    "            actual_class_i_count += confusion_mtr[class_i][i]\n",
    "        FN = actual_class_i_count - TP\n",
    "        \n",
    "        predicted_class_i_count = 0\n",
    "        for i in range(length): # sum of the column\n",
    "            predicted_class_i_count += confusion_mtr[i][class_i]\n",
    "        FP = predicted_class_i_count - TP\n",
    "        \n",
    "        # sum of matrix\n",
    "        sum_of_matrix = np.sum(confusion_mtr)\n",
    "        # TN is found by summing up all values except the row and column of the class \n",
    "        TN = sum_of_matrix - predicted_class_i_count - actual_class_i_count - TP \n",
    "        \n",
    "        return TP, FN, FP, TN\n",
    "        \n",
    "    @staticmethod\n",
    "    def confusion_matrix(predictions):\n",
    "        \"\"\"\n",
    "        Create confusion matrix and then return TP, FN, FP, TN\n",
    "\n",
    "        0 Class: 0.5\n",
    "        1 Class: 1\n",
    "        2 Class: 1.5\n",
    "        3 Class: 2\n",
    "        4 Class: 2.5\n",
    "        5 Class: 3\n",
    "        6 Class: 3.5\n",
    "        7 Class: 4\n",
    "        8 Class: 4.5\n",
    "        9 Class: 5\n",
    "\n",
    "        T0: True 0\n",
    "        F0: False 0\n",
    "        T1: True 1\n",
    "        F1: False 1\n",
    "        ...\n",
    "\n",
    "                 | 0 Prediction | 1 Prediction | 2 Prediction | .....\n",
    "        0 Class  |     T0       |     ..       |      ..      |\n",
    "        1 Class  |     ..       |     T1       |      ..      | \n",
    "        2 Class  |     ..       |     ..       |      T2      |\n",
    "        ...\n",
    "        \"\"\"\n",
    "        # Create multiclass confusion matrix\n",
    "\n",
    "        conf_mtr = np.zeros( (10,10) )\n",
    "\n",
    "        for prediction in predictions:\n",
    "            predicted = Accuracy.half_round_rating(prediction[0])\n",
    "            actual    = Accuracy.half_round_rating(prediction[1])\n",
    "\n",
    "            predicted_class_index = int( (predicted * 2) - 1 )\n",
    "            actual_class_index = int( (actual * 2) - 1 )\n",
    "\n",
    "            conf_mtr[actual_class_index][predicted_class_index] += 1\n",
    "\n",
    "        return conf_mtr\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_confusion_matrix(predictions):\n",
    "        \"\"\"\n",
    "        Create confusion matrix and then return TP, FN, FP, TN\n",
    "\n",
    "        Positive Class: 5\n",
    "        Negative Class: 0.5\n",
    "\n",
    "        TP: True Positive\n",
    "        TN: True Negative\n",
    "        FP: False Positive\n",
    "        FN: False Negative\n",
    "\n",
    "                        | Positive Prediction | Negative Prediction\n",
    "        Positive Class  |       TP            |       FN\n",
    "        Negative Class  |       FP            |       TN\n",
    "\n",
    "        \"\"\"\n",
    "        # Create confusion matrix\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for prediction in predictions:\n",
    "            predicted = Accuracy.threshold_round_rating(prediction[0])\n",
    "            actual = Accuracy.threshold_round_rating(prediction[1])\n",
    "            if predicted == 5 and actual == 5:\n",
    "                TP += 1\n",
    "            elif predicted == 5 and actual == 0.5:\n",
    "                FP += 1\n",
    "            elif predicted == 0.5 and actual == 0.5:\n",
    "                TN += 1\n",
    "            elif predicted == 0.5 and actual == 5:\n",
    "                FN += 1\n",
    "        return TP, FN, FP, TN\n",
    "\n",
    "    @staticmethod\n",
    "    def half_round_rating(rating):\n",
    "        \"\"\"\n",
    "        Round ratings to the closest match in the movielens dataset\n",
    "        For ex.\n",
    "          ratings between 2 and 2.25 -> round to 2\n",
    "          ratings between 2.25 and 2.5 -> round to 2.5\n",
    "          ratings between 2.5 and 2.75 -> round to 2.5\n",
    "          ratings between 2.75 and 3 -> round to 3\n",
    "\n",
    "        \"\"\"\n",
    "        floor_value = math.floor(rating)\n",
    "        if(rating > floor_value + 0.75):\n",
    "            return floor_value + 1\n",
    "        elif(rating > floor_value + 0.5 or rating > floor_value + 0.25):\n",
    "            return floor_value + 0.5\n",
    "        else:\n",
    "            return floor_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_round_rating(rating):\n",
    "        \"\"\"\n",
    "        Round ratings to the closest match in threshold fashion\n",
    "          ratings between 0.5 and 3.5 -> round to 0.5\n",
    "          ratings between 3.5 and 5 -> round to 5\n",
    "        \"\"\"\n",
    "        if (0.5 <= rating < 3.5):\n",
    "            return 0.5\n",
    "        elif (3.5 <= rating <= 5):\n",
    "            return 5\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(ABC):\n",
    "    \"\"\"\n",
    "    Dataset class and its subclasses provides utilities in order to import datasets.\n",
    "    \n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def load():\n",
    "        \"\"\" Every subclass must provide static load method\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp'),\n",
    "                 ratings_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv',\n",
    "                 movies_col_names=('item_id', 'title', 'genres'),\n",
    "                 movies_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv',\n",
    "                 is_ratings_cached=True,\n",
    "                 is_movies_cached=True):\n",
    "        Dataset.__init__(self)\n",
    "        self.is_ratings_cached = is_ratings_cached\n",
    "        self.is_movies_cached = is_movies_cached\n",
    "        self.ratings = MovieLensDataset.load_ratings(ratings_path,\n",
    "                                                     ratings_col_names) if self.is_ratings_cached else None\n",
    "        self.movies = MovieLensDataset.load_movies(movies_path,\n",
    "                                                   movies_col_names) if self.is_movies_cached else None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_movies(movies_path,\n",
    "                    movies_col_names=('item_id', 'title', 'genres')):\n",
    "        if not os.path.isfile(movies_path) or not movies_col_names:\n",
    "            return None\n",
    "\n",
    "        # read movies\n",
    "        movies = pd.read_csv(movies_path, sep=',', header=1, names=movies_col_names)\n",
    "\n",
    "        # Extract Movie Year\n",
    "        movies['year'] = movies.title.str.extract(\"\\((\\d{4})\\)\", expand=True)\n",
    "        movies.year = pd.to_datetime(movies.year, format='%Y')\n",
    "        movies.year = movies.year.dt.year  # As there are some NaN years, resulting type will be float (decimals)\n",
    "\n",
    "        # Remove year part from the title\n",
    "        movies.title = movies.title.str[:-7]\n",
    "\n",
    "        return movies\n",
    "\n",
    "    @staticmethod\n",
    "    def load_ratings(ratings_path,\n",
    "                     ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp')):\n",
    "        if not os.path.isfile(ratings_path) or not ratings_col_names:\n",
    "            return None\n",
    "\n",
    "        # read ratings\n",
    "        ratings = pd.read_csv(ratings_path, sep=',', header=1, names=ratings_col_names)\n",
    "\n",
    "        # Convert timestamp into readable format\n",
    "        ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s', origin='unix')\n",
    "\n",
    "        return ratings\n",
    "\n",
    "    @staticmethod\n",
    "    def create_movie_ratings(ratings, movies):\n",
    "        return pd.merge(ratings, movies, on='item_id')\n",
    "\n",
    "    @staticmethod\n",
    "    def load(ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp'),\n",
    "             ratings_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv',\n",
    "             movies_col_names=('item_id', 'title', 'genres'),\n",
    "             movies_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv'\n",
    "             ):\n",
    "        # Load movies\n",
    "        movies = MovieLensDataset.load_movies(movies_path=movies_path, movies_col_names=movies_col_names)\n",
    "        # Load ratings\n",
    "        ratings = MovieLensDataset.load_ratings(ratings_path=ratings_path, ratings_col_names=ratings_col_names)\n",
    "\n",
    "        # Merge the ratings and movies\n",
    "        movie_ratings = pd.merge(ratings, movies, on='item_id')\n",
    "\n",
    "        return movie_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalPearson:\n",
    "    \"\"\"\n",
    "    Temporal pearson is the classic way of handling recommendations. \n",
    "    We provide pearson method and related cache support for bulk analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, cache: TemporalCache, time_constraint: TimeConstraint = None, min_common_elements: int = 5):\n",
    "        self.time_constraint = time_constraint\n",
    "        self.cache = cache\n",
    "        self.min_common_elements = min_common_elements\n",
    "        #from .trainset import TrainsetUser, TrainsetMovie\n",
    "        self.trainset_user = TrainsetUser(cache=self.cache)\n",
    "        self.trainset_movie = TrainsetMovie(cache=self.cache)\n",
    "\n",
    "    def mean_centered_pearson(self, user_id, movie_id, k_neighbours: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Mean Centered Cosine or in other words Pearson Prediction\n",
    "\n",
    "        :param user_id: user of interest\n",
    "        :param movie_id: the movie's rating is the one we we want to predict\n",
    "        :param k_neighbours: k nearest neighbours in DataFrame where index user_id, column correlation in between.\n",
    "        :return: Prediction rating\n",
    "        \"\"\"\n",
    "        # If a movie with movie_id not exists, predict 0\n",
    "        if self.trainset_movie.get_movie(movie_id=movie_id).empty:\n",
    "            return 0\n",
    "\n",
    "        if k_neighbours is None or k_neighbours.empty:\n",
    "            return 0\n",
    "\n",
    "        user_avg_rating = self.trainset_user.get_user_avg(user_id=user_id)\n",
    "\n",
    "        weighted_sum = 0.0\n",
    "        sum_of_weights = 0.0\n",
    "        for neighbour_id, data in k_neighbours.iterrows():\n",
    "            # Get each neighbour's correlation 'user_id' and her rating to 'movie_id'\n",
    "            neighbour_corr = data['correlation']\n",
    "            neighbour_rating = self.trainset_movie.get_movie_rating(movie_id=movie_id, user_id=neighbour_id)\n",
    "            # If the neighbour doesnt give rating to the movie_id, pass this around of the loop\n",
    "            if neighbour_rating == 0:\n",
    "                continue\n",
    "            neighbour_avg_rating = self.trainset_user.get_user_avg(user_id=neighbour_id)\n",
    "            neighbour_mean_centered_rating = neighbour_rating - neighbour_avg_rating\n",
    "            # Calculate Weighted sum and sum of weights\n",
    "            weighted_sum += neighbour_mean_centered_rating * neighbour_corr\n",
    "            sum_of_weights += neighbour_corr\n",
    "\n",
    "        # Predict\n",
    "        if sum_of_weights != 0:\n",
    "            prediction_rating = user_avg_rating + (weighted_sum / sum_of_weights)\n",
    "        else:\n",
    "            prediction_rating = 0  # In this case, none of the neighbours have given rating to 'the movie'\n",
    "\n",
    "        return prediction_rating\n",
    "\n",
    "    def get_corr_matrix(self, bin_size=-1):\n",
    "        user_corrs = None\n",
    "        # if valid cache found, try to get user corrs from there\n",
    "        if self.cache.is_temporal_cache_valid():\n",
    "            # First check user-correlations\n",
    "            user_corrs = self.cache.get_user_corrs(self.min_common_elements, self.time_constraint)\n",
    "            if user_corrs is not None:\n",
    "                return user_corrs\n",
    "            # Then check bulk-user-correlations\n",
    "            user_corrs = self.cache.get_user_corrs_from_bulk(time_constraint=self.time_constraint,\n",
    "                                                             min_common_elements=self.min_common_elements,\n",
    "                                                             bin_size=bin_size)\n",
    "            if user_corrs is not None:\n",
    "                return user_corrs\n",
    "\n",
    "        # here, if cache not found or no cache match\n",
    "\n",
    "        # Create user correlations\n",
    "        user_corrs = TemporalPearson.create_user_corrs(movie_ratings=self.cache.movie_ratings,\n",
    "                                                       time_constraint=self.time_constraint,\n",
    "                                                       min_common_elements=self.min_common_elements)\n",
    "        # Cache the user_corrs\n",
    "        self.cache.set_user_corrs(user_corrs=user_corrs,\n",
    "                                  min_common_elements=self.min_common_elements,\n",
    "                                  time_constraint=self.time_constraint)\n",
    "\n",
    "        return user_corrs\n",
    "\n",
    "    @staticmethod\n",
    "    def create_user_corrs(movie_ratings, time_constraint: TimeConstraint, min_common_elements):\n",
    "        # by default movie_ratings is for no time constraint\n",
    "        # with these controls change the time constraint of the movie_ratings\n",
    "        if time_constraint is not None:\n",
    "            if time_constraint.is_valid_max_limit():\n",
    "                movie_ratings = movie_ratings[movie_ratings.timestamp < time_constraint.end_dt]\n",
    "            elif time_constraint.is_valid_time_bin():\n",
    "                movie_ratings = movie_ratings[(movie_ratings.timestamp >= time_constraint.start_dt)\n",
    "                                              & (movie_ratings.timestamp < time_constraint.end_dt)]\n",
    "\n",
    "        user_movie_matrix = movie_ratings.pivot_table(index='title', columns='user_id', values='rating')\n",
    "        return user_movie_matrix.corr(method=\"pearson\", min_periods=min_common_elements)\n",
    "\n",
    "    def cache_user_corrs_in_bulk_for_max_limit(self, time_constraint: TimeConstraint, min_year, max_year):\n",
    "        \"\"\"\n",
    "        Cache user correlations by changing year of the time_constraint\n",
    "        for each year in between min_year and max_year(not included)\n",
    "\n",
    "        :param time_constraint: time_constraint apply\n",
    "        :param min_year: start of the range\n",
    "        :param max_year: end of the range\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.use_bulk_corr_cache:\n",
    "            if time_constraint is not None and time_constraint.is_valid_max_limit():\n",
    "                self.cache.user_corrs_in_bulk = dict()\n",
    "                for year in range(min_year, max_year):\n",
    "                    time_constraint.end_dt = time_constraint.end_dt.replace(year=year)\n",
    "                    corrs = TemporalPearson.create_user_corrs(self.cache.movie_ratings, time_constraint,\n",
    "                                                              self.min_common_elements)\n",
    "                    self.cache.user_corrs_in_bulk[year] = corrs\n",
    "            else:\n",
    "                raise Exception(\"Trying to cache user correlations in bulk for max_limit \"\n",
    "                                \"but start time is not max_limit!\")\n",
    "        else:\n",
    "            raise Exception(\"Trying to create bulk corr cache when use_bulk_corr_cache is False\")\n",
    "\n",
    "    def cache_user_corrs_in_bulk_for_time_bins(self, time_constraint: TimeConstraint, min_year, max_year,\n",
    "                                               min_time_bin_size=2, max_time_bin_size=10):\n",
    "        if self.cache.use_bulk_corr_cache:\n",
    "            if time_constraint is not None and time_constraint.is_valid_time_bin():\n",
    "                del self.cache.user_corrs_in_bulk    # invalidate old cache\n",
    "                self.cache.user_corrs_in_bulk = dict()\n",
    "                for time_bin_size in range(min_time_bin_size, max_time_bin_size):\n",
    "                    self.cache.user_corrs_in_bulk[time_bin_size] = dict()\n",
    "                    for shift in range(0, time_bin_size):\n",
    "                        curr_year = min_year + shift\n",
    "                        while (curr_year + time_bin_size) < max_year:\n",
    "                            time_constraint = TimeConstraint(start_dt=datetime(curr_year, 1, 1),\n",
    "                                                             end_dt=datetime(curr_year + time_bin_size, 1, 1))\n",
    "                            corrs = TemporalPearson.create_user_corrs(self.cache.movie_ratings,\n",
    "                                                                      time_constraint,\n",
    "                                                                      self.min_common_elements)\n",
    "                            self.cache.user_corrs_in_bulk[time_bin_size][curr_year] = corrs\n",
    "                            curr_year += time_bin_size\n",
    "        else:\n",
    "            raise Exception(\"Trying to create bulk corr cache when use_bulk_corr_cache is False\")\n",
    "\n",
    "    @property\n",
    "    def time_constraint(self):\n",
    "        return self._time_constraint\n",
    "\n",
    "    @time_constraint.setter\n",
    "    def time_constraint(self, value):\n",
    "        self._time_constraint = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainsetUser:\n",
    "    \"\"\"\n",
    "    TrainsetUser provides user related dataset utility methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, cache: Cache):\n",
    "        \"\"\"\n",
    "        :param cache: Input cache must have movie_ratings not None !\n",
    "        \"\"\"\n",
    "        self.cache = cache\n",
    "\n",
    "        if not self.cache.is_movie_ratings_cached:\n",
    "            raise Exception(\"'movie_ratings' has not been cached !\")\n",
    "\n",
    "    def get_users(self):\n",
    "        \"\"\"\n",
    "        Get list of unique 'user_id's\n",
    "\n",
    "        Since MovieLens Have 'user_id's from 0 to 610 without any missing user, for now sending that directly\n",
    "        Uncomment the other lines later\n",
    "\n",
    "        :return: the ids of the users found in movie_ratings\n",
    "        \"\"\"\n",
    "        #\n",
    "        # if self.cache.is_ratings_cached:\n",
    "        #     data = self.cache.ratings\n",
    "        # else:\n",
    "        #     data = self.cache.movie_ratings\n",
    "        #\n",
    "        # return pd.unique(data['user_id'])\n",
    "        return range(0, 611)\n",
    "\n",
    "    def get_active_users(self, n=10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get Users in sorted order where the first one is the one who has given most ratings.\n",
    "\n",
    "        :param n: Number of users to retrieve.\n",
    "        :return: user DataFrame with index of 'user_id' and columns of ['mean_rating', 'No_of_ratings'] .\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.is_ratings_cached:                         # 30% faster than other choice\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "\n",
    "        active_users = pd.DataFrame(data.groupby('user_id')['rating'].mean())\n",
    "        active_users['No_of_ratings'] = pd.DataFrame(data.groupby('user_id')['rating'].count())\n",
    "        active_users.sort_values(by=['No_of_ratings'], ascending=False, inplace=True)\n",
    "        active_users.columns = ['mean_rating', 'No_of_ratings']\n",
    "        return active_users.head(n)\n",
    "\n",
    "    def get_random_users(self, n=1):\n",
    "        \"\"\"\n",
    "        Get list of random n number of 'user_id's\n",
    "\n",
    "        :param n: Number of random users\n",
    "        :return: List of random 'user_id's\n",
    "        \"\"\"\n",
    "\n",
    "        return random.choices(population=self.get_users(), k=n)\n",
    "\n",
    "    def get_user_ratings(self, user_id: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all the ratings given by of the chosen users\n",
    "\n",
    "        :param user_id: id of the chosen user\n",
    "        :return: Ratings given by the 'user_id'\n",
    "        \"\"\"\n",
    "        if self.cache.is_ratings_cached:                         # 2.2x faster than other choice\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "\n",
    "        return data.loc[data['user_id'] == user_id]\n",
    "\n",
    "    def get_user_avg(self, user_id: int):\n",
    "\n",
    "        if self.cache.use_avg_ratings_cache:\n",
    "            avg_user_rating = self.cache.avg_user_ratings.loc[user_id]\n",
    "            return avg_user_rating[0] if not avg_user_rating.empty else 0\n",
    "\n",
    "        user_ratings = self.get_user_ratings(user_id=user_id)\n",
    "        return user_ratings.rating.mean() if not user_ratings.empty else 0\n",
    "\n",
    "    def get_timestamp(self, user_id: int, movie_id: int):\n",
    "        \"\"\"\n",
    "        Get the timestamp of the given rating\n",
    "\n",
    "        :param user_id: the users whose rating timestamp we are searching\n",
    "        :param movie_id: id of the movie that the user gave the rating\n",
    "        :return: if found the datetime object otherwise None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.is_ratings_cached:\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "\n",
    "        timestamp = data.loc[(data['user_id'] == user_id) & (data['item_id'] == movie_id)]\n",
    "        return timestamp.values[0, 3] if not timestamp.empty else None\n",
    "\n",
    "    def get_first_timestamp(self):\n",
    "        if self.cache.is_ratings_cached:\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "        return data['timestamp'].min()\n",
    "\n",
    "    def get_user_avg_timestamp(self, user_id: int):\n",
    "        user_ratings = self.get_user_ratings(user_id=user_id)\n",
    "        return user_ratings.timestamp.mean() if not user_ratings.empty else 0\n",
    "\n",
    "    # TODO: Later, create TemporalDatasetUser, and put this method into that one\n",
    "    def get_user_ratings_at(self, user_id: int, at: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get user ratings up until the given datetime\n",
    "        :param user_id: id of the chosen user\n",
    "        :param at: only those ratings that are before this date will be taken into account\n",
    "        :return: Ratings given by the 'user_id' before given datetime\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.is_ratings_cached:\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "\n",
    "        return data.loc[(data['user_id'] == user_id) & (data.timestamp < at)]\n",
    "\n",
    "    # TODO: Later, create TemporalDatasetUser, and put this method into that one\n",
    "    def get_user_avg_at(self, user_id: int, at: datetime):\n",
    "        user_ratings = self.get_user_ratings_at(user_id, at)\n",
    "        return user_ratings.rating.mean() if not user_ratings.empty else 0\n",
    "\n",
    "\n",
    "class TrainsetMovie:\n",
    "    \"\"\"\n",
    "    TrainsetMovie provides movie related dataset utility methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, cache: Cache):\n",
    "        \"\"\"\n",
    "        :param cache: Input cache must have movie_ratings not None !\n",
    "        \"\"\"\n",
    "        self.cache = cache\n",
    "\n",
    "        if not self.cache.is_movie_ratings_cached:\n",
    "            raise Exception(\"'movie_ratings' has not been cached !\")\n",
    "\n",
    "    def get_movie(self, movie_id) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get Movie Record\n",
    "\n",
    "        :return: DataFrame which contains the given 'movie_id's details. If not found empty DataFrame .\n",
    "        \"\"\"\n",
    "        if self.cache.is_movies_cached:\n",
    "            return self.cache.movies.loc[self.cache.movies['item_id'] == movie_id]\n",
    "        return self.cache.movie_ratings.loc[self.cache.movie_ratings['item_id'] == movie_id]\n",
    "\n",
    "    def get_movies(self):\n",
    "        \"\"\"\n",
    "        Get list of unique 'item_id's or in other words the movies.\n",
    "\n",
    "        :return: List of movie ids\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.is_movies_cached:\n",
    "            return self.cache.movies['item_id'].values.tolist()\n",
    "\n",
    "        return pd.unique(self.cache.movie_ratings['item_id'])\n",
    "\n",
    "    def get_random_movies(self, n=10):\n",
    "        \"\"\"\n",
    "        Get list of random n number of 'item_id's or in other words the movies\n",
    "\n",
    "        :param n: Number of random movies\n",
    "        :return: List of random 'movie_id's\n",
    "        \"\"\"\n",
    "        return random.choices(population=self.get_movies(), k=n)\n",
    "\n",
    "    def get_movies_watched(self, user_id: int, time_constraint: TimeConstraint = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all the movies watched by the chosen user.\n",
    "\n",
    "        :param user_id: the user that we want to get the movies he-she has watched.\n",
    "        :param time_constraint: type of the time constraint.\n",
    "        :return: DataFrame of all movies watched with 'item_id', 'rating' columns\n",
    "        \"\"\"\n",
    "\n",
    "        movie_ratings = self.cache.movie_ratings\n",
    "\n",
    "        if time_constraint is None:\n",
    "            return movie_ratings.loc[(movie_ratings['user_id'] == user_id)][['item_id', 'rating']]\n",
    "\n",
    "        if time_constraint.is_valid_max_limit():\n",
    "            return movie_ratings.loc[(movie_ratings['user_id'] == user_id)\n",
    "                                     & (movie_ratings.timestamp < time_constraint.end_dt)][['item_id', 'rating']]\n",
    "        elif time_constraint.is_valid_time_bin():\n",
    "            return movie_ratings.loc[(movie_ratings['user_id'] == user_id)\n",
    "                                     & (movie_ratings.timestamp >= time_constraint.start_dt)\n",
    "                                     & (movie_ratings.timestamp < time_constraint.end_dt)][['item_id', 'rating']]\n",
    "        raise Exception(\"Undefined time_constraint is given!\")\n",
    "\n",
    "    def get_movie_rating(self, movie_id: int, user_id: int) -> int:\n",
    "        \"\"\"\n",
    "        Get the movie rating taken by the chosen user\n",
    "\n",
    "        :param movie_id: the movie chosen movie's id\n",
    "        :param user_id: id of the chosen user\n",
    "        :return: Rating given by user. If not found, returns 0\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.is_ratings_cached:\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "\n",
    "        movie_rating = data.loc[(data['user_id'] == user_id) & (data['item_id'] == movie_id)]\n",
    "        return movie_rating.values[0, 2] if not movie_rating.empty else 0\n",
    "\n",
    "    def get_random_movie_watched(self, user_id: int) -> int:\n",
    "        \"\"\"\n",
    "        Get random movie id watched.\n",
    "\n",
    "        :param user_id: User of interest\n",
    "        :return:  movie_id or item_id of the random movie watched by the user.\n",
    "                  In case non-valid user_id supplied then returns 0\n",
    "        \"\"\"\n",
    "        movies_watched = self.get_movies_watched(user_id=user_id)\n",
    "        return random.choice(movies_watched['item_id'].values.tolist()) if not movies_watched.empty else 0\n",
    "\n",
    "    def get_random_movies_watched(self, user_id: int, n=2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get random n movies watched by the user. Only use when n > 2\n",
    "\n",
    "        Use get_random_movie_watched if n=1 since that one 2 fold faster.\n",
    "\n",
    "        :param user_id: the user of interest\n",
    "        :param n: number of random movies to get\n",
    "        :return: DataFrame of movies, if none found then empty DataFrame\n",
    "        \"\"\"\n",
    "        movies_watched = self.get_movies_watched(user_id=user_id)\n",
    "        return random.choices(population=movies_watched['item_id'].values.tolist(),\n",
    "                              k=n) if not movies_watched.empty else movies_watched\n",
    "\n",
    "    def get_random_movie_per_user(self, user_id_list):\n",
    "        \"\"\"\n",
    "        Get random movie for each user given in the 'user_id_list'\n",
    "\n",
    "        :param user_id_list: List of valid user_ids\n",
    "        :return: List of (user_id, movie_id) tuples\n",
    "                where each movie_id is randomly chosen from watched movies of the user_id .\n",
    "                In case any one of the user_id's supplies invalid, then the movie_id will be 0 for that user.\n",
    "        \"\"\"\n",
    "        user_movie_list = list()\n",
    "        for user_id in user_id_list:\n",
    "            user_movie_list.append((user_id, self.get_random_movie_watched(user_id=user_id)))\n",
    "        return user_movie_list\n",
    "\n",
    "\n",
    "class Trainset:\n",
    "    \"\"\"\n",
    "    Trainset class is used to find K nearest neighbours and and predict movies using TemporalPearson class.\n",
    "    \"\"\"\n",
    "    def __init__(self, cache: TemporalCache, min_common_elements: int = 5):\n",
    "        self.cache = cache\n",
    "        self.min_common_elements = min_common_elements\n",
    "        self.similarity = TemporalPearson(time_constraint=None, cache=self.cache)\n",
    "\n",
    "        if not self.cache.is_movie_ratings_cached:\n",
    "            raise Exception(\"'movie_ratings' has not been cached !\")\n",
    "\n",
    "        self.trainset_movie = TrainsetMovie(cache=cache)\n",
    "        self.trainset_user = TrainsetUser(cache=cache)\n",
    "\n",
    "        # if caching is allowed, create user correlations cache\n",
    "        self.similarity.get_corr_matrix()\n",
    "\n",
    "    def predict_movies_watched(self, user_id, n=10, k=10, time_constraint=None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "\n",
    "        :param user_id: user of interest\n",
    "        :param n: Number of movies to predict\n",
    "        :param k: k neighbours to take into account\n",
    "        :param time_constraint: When calculating k neighbours,\n",
    "                                only those that comply to time_constraints will be taken into account.\n",
    "        :return: DataFrame of Predictions where columns = ['prediction', 'rating'] index = 'movie_id'\n",
    "        \"\"\"\n",
    "        # Get all movies watched by a user\n",
    "        movies_watched = self.trainset_movie.get_movies_watched(user_id=user_id)\n",
    "\n",
    "        if movies_watched.empty:\n",
    "            return None\n",
    "\n",
    "        predictions = list()\n",
    "        number_of_predictions = 0\n",
    "        for row in movies_watched.itertuples(index=False):\n",
    "            prediction = self.predict_movie(user_id=user_id, movie_id=row[0],\n",
    "                                            time_constraint=time_constraint, k=k)\n",
    "            if number_of_predictions == n:\n",
    "                break\n",
    "            predictions.append([prediction, row[1], row[0]])\n",
    "            number_of_predictions += 1\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions, columns=['prediction', 'rating', 'movie_id'])\n",
    "        predictions_df.movie_id = predictions_df.movie_id.astype(int)\n",
    "        return predictions_df.set_index('movie_id')\n",
    "\n",
    "    def predict_movie(self, user_id, movie_id, k=10, time_constraint=None, bin_size=-1):\n",
    "        prediction = self.similarity.mean_centered_pearson(user_id=user_id,\n",
    "                                                           movie_id=movie_id,\n",
    "                                                           k_neighbours=\n",
    "                                                           self.get_k_neighbours(user_id, k=k,\n",
    "                                                                                 time_constraint=time_constraint,\n",
    "                                                                                 bin_size=bin_size)\n",
    "                                                           )        \n",
    "        return prediction if prediction <= 5 else 5\n",
    "\n",
    "    def get_k_neighbours(self, user_id, k=10, time_constraint: TimeConstraint = None, bin_size=-1):\n",
    "        \"\"\"\n",
    "        :param user_id: the user of interest\n",
    "        :param k: number of neighbours to retrieve\n",
    "        :param time_constraint: time constraint when choosing neighbours\n",
    "        :param bin_size: Used when using time_bins, in order to select bin from cache\n",
    "        :return: Returns the k neighbours and correlations in between them. If no neighbours found, returns None\n",
    "                 DataFrame which has 'Correlation' column and 'user_id' index.\n",
    "        \"\"\"\n",
    "        self.similarity.time_constraint = time_constraint\n",
    "        user_corr_matrix = self.similarity.get_corr_matrix(bin_size=bin_size)\n",
    "\n",
    "        # Exit if matrix is None, no user found in self.cache.movie_ratings, something is wrong\n",
    "        if user_corr_matrix is None:\n",
    "            return None\n",
    "\n",
    "        # Get the chosen 'user_id's correlations\n",
    "        user_correlations = user_corr_matrix.get(user_id)\n",
    "        if user_correlations is None:\n",
    "            return None\n",
    "\n",
    "        # Drop any null, if found\n",
    "        user_correlations.dropna(inplace=True)\n",
    "        # Create A DataFrame from not-null correlations of the 'user_id'\n",
    "        users_alike = pd.DataFrame(user_correlations)\n",
    "        # Rename the only column to 'correlation'\n",
    "        users_alike.columns = ['correlation']\n",
    "\n",
    "        # Sort the user correlations in descending order\n",
    "        #     so that first one is the most similar, last one least similar\n",
    "        users_alike.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "\n",
    "        # Eliminate Correlation to itself by deleting first row,\n",
    "        #     since biggest corr is with itself it is in first row\n",
    "        return users_alike.iloc[1:k+1] if k is not None else users_alike.iloc[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluator class is used to test our global timebin architecture and create anaylsis whether it works or not.\n",
    "    As a tip, it didnt worked out well.d But this analysis further guided us towards personal timebin architecture which worked well.\n",
    "    \"\"\"\n",
    "    def __init__(self, trainset: Trainset):\n",
    "        self.trainset = trainset\n",
    "\n",
    "    def evaluate_best_max_year_in_bulk(self, n,\n",
    "                                       n_users, n_movies, k=10,\n",
    "                                       min_year=-1,\n",
    "                                       max_year=-1) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate and collect data about best max year constraint which can be put instead of no constraint.\n",
    "\n",
    "        This method calls 'evaluate_best_max_year_constraint' method 'n' times.\n",
    "        Takes required precautions for bulk calling.\n",
    "\n",
    "        :param n: Number of runs that we run the evaluate_best_max_year_constraint() method\n",
    "        :param n_users: Number of users to check\n",
    "        :param n_movies: Number of movies per user to check\n",
    "        :param k: Number of neighbours of each user to take into account when making prediction\n",
    "        :param min_year: First year to evaluate\n",
    "        :param max_year: Last year to evaluate\n",
    "        :return: (no_constrain_rmse_data, best_year_constraint_results)\n",
    "        \"\"\"\n",
    "        if min_year == -1:\n",
    "            min_year = self.trainset.trainset_user.get_first_timestamp().year\n",
    "\n",
    "        if max_year == -1:\n",
    "            max_year = datetime.now().year\n",
    "\n",
    "        time_constraint = TimeConstraint(end_dt=datetime(year=min_year, month=1, day=1))\n",
    "        # Create cache if bulk_corr_cache is allowed\n",
    "        self.trainset.similarity.cache_user_corrs_in_bulk_for_max_limit(time_constraint,\n",
    "                                                                        min_year=min_year,\n",
    "                                                                        max_year=max_year)\n",
    "        \n",
    "        run_results = dict()\n",
    "        for i in range(n):\n",
    "            run_results[i] = self.evaluate_best_max_year_constraint(n_users=n_users, n_movies=n_movies, k=k,\n",
    "                                                                    min_year=min_year, max_year=max_year,\n",
    "                                                                    create_cache=False,)\n",
    "\n",
    "        return run_results\n",
    "\n",
    "    def evaluate_best_max_year_constraint(self, n_users, n_movies, k,\n",
    "                                          max_diff=0.1,\n",
    "                                          min_year=-1, max_year=-1,\n",
    "                                          create_cache=True) -> defaultdict:\n",
    "        \"\"\"\n",
    "        Evaluate the max_year constraint for evaluate_max_year_constraint method.\n",
    "\n",
    "        :param max_diff: maximum difference between rmse when no constraint and with given year constraint.\n",
    "        :param n_users: Number of users to evaluate\n",
    "        :param n_movies: Number of movies per user to evaluate\n",
    "        :param k: Number of neighbours of each user to take into account when making prediction\n",
    "        :param min_year: First year to evaluate\n",
    "        :param max_year: Last year to evaluate\n",
    "        :param create_cache: create cache before running. For bulk callers.\n",
    "        :return: Votes for years where each year got its vote\n",
    "                 when rmse is less than 'max_diff' in between no constraint and year constraint\n",
    "        \"\"\"\n",
    "\n",
    "        if min_year == -1:\n",
    "            min_year = self.trainset.trainset_user.get_first_timestamp().year\n",
    "\n",
    "        if max_year == -1:\n",
    "            max_year = datetime.now().year\n",
    "\n",
    "        if n_users > 600:\n",
    "            user_list = self.trainset.trainset_user.get_users()  # No need to random selection, get all users\n",
    "        else:\n",
    "            user_list = self.trainset.trainset_user.get_random_users(n=n_users)  # Select random n users\n",
    "\n",
    "        # Calculate RMSE With No Constraint\n",
    "        no_constraint_data = dict()\n",
    "        for user_id in user_list:\n",
    "            rmse = Accuracy.rmse(self.trainset.predict_movies_watched(user_id, n_movies, k))\n",
    "            no_constraint_data[user_id] = rmse\n",
    "\n",
    "        # # Calculate RMSE With Time Constraint\n",
    "\n",
    "        # Cache all years before processing\n",
    "        time_constraint = TimeConstraint(end_dt=datetime(year=min_year, month=1, day=1))\n",
    "        # Create cache if bulk_corr_cache is allowed\n",
    "        if create_cache:\n",
    "            self.trainset.similarity.cache_user_corrs_in_bulk_for_max_limit(time_constraint,\n",
    "                                                                            min_year=min_year,\n",
    "                                                                            max_year=max_year)\n",
    "        # Votes to years is stored inside time_constraint_data\n",
    "        time_constraint_data = defaultdict(int)\n",
    "        for year in range(min_year, max_year):\n",
    "            time_constraint.end_dt = time_constraint.end_dt.replace(year=year)\n",
    "\n",
    "            for user_id in user_list:\n",
    "                rmse = Accuracy.rmse(self.trainset.predict_movies_watched(user_id=user_id, n=n_movies, k=k,\n",
    "                                                                          time_constraint=time_constraint))\n",
    "                if abs(rmse - no_constraint_data[user_id]) < max_diff:\n",
    "                    time_constraint_data[year] += 1\n",
    "\n",
    "        return time_constraint_data\n",
    "\n",
    "    def evaluate_max_year_constraint(self, n_users, n_movies, k, time_constraint):\n",
    "        \"\"\"\n",
    "        Compare given time_constraint with normal where no constraint exists.\n",
    "\n",
    "        Time constraint is of type max_year which means the system will be set to a certain year.\n",
    "\n",
    "        :param n_users: Number of users to evaluate\n",
    "        :param n_movies: Number of movies per user to evaluate\n",
    "        :param k: Number of neighbours to take into account when making movie prediction\n",
    "        :param time_constraint: Time constraint which will be applied.\n",
    "        :return: DataFrame of results which contains rmse with constraint and no constraint, as well as runtime.\n",
    "        \"\"\"\n",
    "        trainset = self.trainset\n",
    "        data = list()\n",
    "\n",
    "        for i in range(n_users):\n",
    "            # Get Random User\n",
    "            user_id = random.randint(1, 610)\n",
    "            # Predict movies for user and record runtime\n",
    "            st = default_timer()\n",
    "            rmse = Accuracy.rmse(\n",
    "                trainset.predict_movies_watched(user_id=user_id, n=n_movies, k=k, time_constraint=None))\n",
    "            r1 = default_timer() - st\n",
    "            # Predict movies with time_constraint for user and record runtime\n",
    "            st = default_timer()\n",
    "            time_constrained_rmse = Accuracy.rmse(\n",
    "                trainset.predict_movies_watched(user_id=user_id, n=n_movies, k=k, time_constraint=time_constraint))\n",
    "            r2 = default_timer() - st\n",
    "            # Save iteration data\n",
    "            data.append([user_id, rmse, r1, time_constrained_rmse, r2])\n",
    "\n",
    "        data = pd.DataFrame(data)\n",
    "        data.columns = ['user_id', 'rmse', 'runtime1', 'temporal_rmse', 'runtime2']\n",
    "        data.set_index('user_id', inplace=True)\n",
    "        return data\n",
    "\n",
    "    def evaluate_time_bins_in_bulk(self, n, n_users, k=10,\n",
    "                                   min_year=-1,\n",
    "                                   max_year=-1,\n",
    "                                   min_time_bin_size=2, max_time_bin_size=10):\n",
    "        \"\"\"\n",
    "        Evaluate time bins and return the results.\n",
    "\n",
    "        This method calls 'evaluate_time_bins' method 'n' times. Takes required precautions for bulk calling.\n",
    "\n",
    "        :param n: Number of runs\n",
    "        :param n_users: Number of users\n",
    "        :param k: Number of neighbours will be used when making prediction\n",
    "        :param min_year: First year to start when taking time bins\n",
    "        :param max_year: When to stop when taking time bins, last is not included.\n",
    "        :param min_time_bin_size: Minimum bin size in years\n",
    "        :param max_time_bin_size: Maximum bin size in years\n",
    "        :return: Evaluation results\n",
    "        \"\"\"\n",
    "        if min_year == -1:\n",
    "            min_year = self.trainset.trainset_user.get_first_timestamp().year\n",
    "\n",
    "        if max_year == -1:\n",
    "            max_year = datetime.now().year\n",
    "\n",
    "        # Cache all years before processing\n",
    "        time_constraint = TimeConstraint(start_dt=datetime(year=min_year, month=1, day=1),\n",
    "                                         end_dt=datetime(year=max_year, month=1, day=1))\n",
    "        self.trainset.similarity.cache_user_corrs_in_bulk_for_time_bins(time_constraint,\n",
    "                                                                        min_year=min_year,\n",
    "                                                                        max_year=max_year,\n",
    "                                                                        min_time_bin_size=min_time_bin_size,\n",
    "                                                                        max_time_bin_size=max_time_bin_size)\n",
    "\n",
    "        run_results = dict()\n",
    "        for i in range(n):\n",
    "            run_results[i] = self.evaluate_time_bins(n_users=n_users, k=k, min_year=min_year, max_year=max_year,\n",
    "                                                     min_time_bin_size=min_time_bin_size,\n",
    "                                                     max_time_bin_size=max_time_bin_size,\n",
    "                                                     create_cache=False)\n",
    "\n",
    "        return run_results\n",
    "\n",
    "    def evaluate_time_bins(self, n_users, k, min_year=-1, max_year=-1,\n",
    "                           min_time_bin_size=2, max_time_bin_size=10,\n",
    "                           create_cache=True) -> dict:\n",
    "        \"\"\"\n",
    "\n",
    "        :param n_users: Number of users\n",
    "        :param k: Number of neighbours will be used when making prediction\n",
    "        :param min_year: First year to start when taking time bins\n",
    "        :param max_year: When to stop when taking time bins, last is not included.\n",
    "        :param min_time_bin_size: Minimum bin size in years\n",
    "        :param max_time_bin_size: Maximum bin size in years\n",
    "        :param create_cache: Create cache before calling time bins. For bulk callers.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        trainset = self.trainset\n",
    "\n",
    "        if min_year == -1:\n",
    "            min_year = self.trainset.trainset_user.get_first_timestamp().year\n",
    "\n",
    "        if max_year == -1:\n",
    "            max_year = datetime.now().year\n",
    "\n",
    "        if n_users > 600:\n",
    "            user_list = trainset.trainset_user.get_users()\n",
    "        else:\n",
    "            user_list = trainset.trainset_user.get_random_users(n=n_users)\n",
    "        user_movie_list = trainset.trainset_movie.get_random_movie_per_user(user_list)\n",
    "        data = dict()\n",
    "\n",
    "        result = list()\n",
    "\n",
    "        if create_cache:\n",
    "            # Cache all years before processing\n",
    "            time_constraint = TimeConstraint(start_dt=datetime(year=min_year, month=1, day=1),\n",
    "                                             end_dt=datetime(year=max_year, month=1, day=1))\n",
    "            self.trainset.similarity.cache_user_corrs_in_bulk_for_time_bins(time_constraint,\n",
    "                                                                            min_year=min_year,\n",
    "                                                                            max_year=max_year,\n",
    "                                                                            min_time_bin_size=min_time_bin_size,\n",
    "                                                                            max_time_bin_size=max_time_bin_size)\n",
    "\n",
    "        # Take each bins where first bin 'min_time_bin_size' years, last one 'max_time_bin_size - 1' years\n",
    "        for time_bin_size in range(min_time_bin_size, max_time_bin_size):\n",
    "            # Shift each time_bin starting with 0 years up until (time_bin-1) years\n",
    "            for shift in range(0, time_bin_size):\n",
    "                curr_year = min_year + shift\n",
    "                predictions = list()\n",
    "                start_time = default_timer()\n",
    "                # Scan and make predictions for all the time_bins\n",
    "                while (curr_year + time_bin_size) < max_year:\n",
    "                    for user_id, movie_id in user_movie_list:\n",
    "                        p = trainset.predict_movie(user_id=user_id, movie_id=movie_id, k=k,\n",
    "                                                   time_constraint=TimeConstraint(start_dt=datetime(curr_year, 1, 1),\n",
    "                                                                                  end_dt=datetime(curr_year+time_bin_size, 1, 1)),\n",
    "                                                   bin_size=time_bin_size)\n",
    "                        # if prediction has been done successfully\n",
    "                        if p != 0:\n",
    "                            r = trainset.trainset_movie.get_movie_rating(movie_id=movie_id, user_id=user_id)\n",
    "                            # Append (prediction, actual_rating)\n",
    "                            predictions.append((p, r))\n",
    "                    curr_year += time_bin_size\n",
    "                runtime = default_timer() - start_time\n",
    "                bin_rmse = Accuracy.rmse(predictions=predictions)\n",
    "                iteration_results = {\"bin_size\": time_bin_size,\n",
    "                                     \"start_year\": min_year + shift,\n",
    "                                     \"predictions\": predictions,\n",
    "                                     \"rmse\": bin_rmse,\n",
    "                                     \"runtime\": runtime\n",
    "                                     }\n",
    "                result.append(iteration_results)\n",
    "\n",
    "        data['result'] = result\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = TemporalCache(time_constraint=None, \n",
    "                  is_ratings_cached=True,\n",
    "                  is_movies_cached=True,\n",
    "                  is_movie_ratings_cached=True,\n",
    "                  ratings=MovieLensDataset.load_ratings(r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv'),\n",
    "                  movies=MovieLensDataset.load_movies(r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv'),\n",
    "                  movie_ratings=MovieLensDataset.load(),\n",
    "                  is_user_correlations_cached=True,\n",
    "                  use_bulk_corr_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Trainset(cache=c, min_common_elements=5)\n",
    "e = Evaluator(trainset=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timebin Based Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimebinSimilarity:\n",
    "    \"\"\"\n",
    "    TembinSimilarity is a way to provide personalized timebin based recommendations.\n",
    "    Here we define an alternative to classical pearson by providing temporal timebin aspect.\n",
    "    Here is what we do in summary:\n",
    "      1. Choose a person to make prediction on (as always).\n",
    "      2. Take the last s movie ratings of this person as a timebin from a random timepoint.(user must have at least s movie watched before!)\n",
    "      3. Find neighbour timebins to the selected timebin.(using correlations between them, higher, better)\n",
    "      4. Predict rating for the person using the timebin neighbours the same way as we use in k nearest neighbour.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ratings, trainset, k=5, r=3, s=10, corr_threshold=0.5, \n",
    "                 neighbour_min_s=5, neighbour_max_s=50, neighbour_step_size=5):\n",
    "        \"\"\"\n",
    "        :param k: Minimum number of ratings in common in between neighbour users when taking timebins of them. \n",
    "        :param r: Minimum number of ratings in common between neighbour timebins.\n",
    "        :param s: Maximum number of movies to include inside of the timebin.\n",
    "        :param neighbour_min_s: Minimum number of movies to include inside neighbour timebins\n",
    "        :param neighbour_max_s: Maximum number of movies to include inside neighbour timebins\n",
    "        :param neighbour_step_size: Number of movies to increase the timebin size per step \n",
    "        \"\"\"\n",
    "        # Ratings for each user is sorted by timestamp\n",
    "        self.ratings = ratings\n",
    "        self.ratings.sort_values(['user_id', 'timestamp'], ignore_index=True, inplace=True)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.k = k\n",
    "        self.r = r\n",
    "        self.s = s\n",
    "        self.corr_threshold = corr_threshold\n",
    "        \n",
    "        self.neighbour_min_s = neighbour_min_s\n",
    "        self.neighbour_max_s = neighbour_max_s\n",
    "        self.neighbour_step_size = neighbour_step_size\n",
    "        \n",
    "        if s < 1:\n",
    "            raise Exception(\"Timebin size must be positive.\")\n",
    "        \n",
    "        # For ease of use, save these\n",
    "        self.trainset = trainset\n",
    "        self.trainset_user = trainset.trainset_user\n",
    "        self.trainset_movie = trainset.trainset_movie\n",
    "        self.first_timestamp = ratings['timestamp'].min()\n",
    "                \n",
    "        # Set these values, before calculating any correlations\n",
    "        self.user_id = None\n",
    "        self.time_constraint=None\n",
    "        self.movie_id = None\n",
    "        self.timebin = None\n",
    "        self.timebin_user_avg_rating = None\n",
    "       \n",
    "    def find_timebin_corr(self, merged:pd.DataFrame, neighbour_avg_rating):\n",
    "        \"\"\"\n",
    "        Find the correlation between the given neighbour timebin and the self.timebin\n",
    "        \n",
    "        :param merged: Merged version of neighbour timebin and self.timebin\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate Pearson Correlation in between the self.timebin and given neighbour\n",
    "        numenator = ((merged['rating_x'] - self.timebin_user_avg_rating) * (merged['rating_y'] - neighbour_avg_rating)).sum()\n",
    "        denominator = math.sqrt(((merged['rating_x'] - self.timebin_user_avg_rating) ** 2).sum())\n",
    "        denominator *= math.sqrt(((merged['rating_y'] - neighbour_avg_rating) ** 2).sum())\n",
    "        pearson = numenator / denominator\n",
    "        \n",
    "        return pearson\n",
    "        \n",
    "    \n",
    "    def get_movies_watched(self, user_id: int, time_constraint: TimeConstraint = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all the movies watched by the chosen user.\n",
    "\n",
    "        :param user_id: the user that we want to get the movies he-she has watched.\n",
    "        :param time_constraint: type of the time constraint.\n",
    "        :return: DataFrame of all movies watched with 'item_id', 'rating' and 'timestamp' columns\n",
    "        \"\"\"\n",
    "\n",
    "        movie_ratings = self.ratings\n",
    "\n",
    "        if time_constraint is None:\n",
    "            return movie_ratings.loc[(movie_ratings['user_id'] == user_id)][['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "\n",
    "        if time_constraint.is_valid_max_limit():\n",
    "            return movie_ratings.loc[(movie_ratings['user_id'] == user_id)\n",
    "                                     & (movie_ratings.timestamp < time_constraint.end_dt)][['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "        elif time_constraint.is_valid_time_bin():\n",
    "            return movie_ratings.loc[(movie_ratings['user_id'] == user_id)\n",
    "                                     & (movie_ratings.timestamp >= time_constraint.start_dt)\n",
    "                                     & (movie_ratings.timestamp < time_constraint.end_dt)][['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "        return None   # Means given time_constraint is invalid.\n",
    "    \n",
    "    def get_timebin2(self, user_id, timebin_i, timebin_size):\n",
    "        \"\"\"\n",
    "        Generate the timebin by using given info.\n",
    "        timebin_i is the index the first movie\n",
    "        timebin_size is the number of movies to take starting at timebin_i th index.\n",
    "        \"\"\"\n",
    "        all_movies_watched = self.get_movies_watched(user_id)\n",
    "        return all_movies_watched.iloc[timebin_i:timebin_i+timebin_size]\n",
    "    \n",
    "    def get_timebin3(self, user_id, timebin_i, timebin_size):\n",
    "        \"\"\"\n",
    "        Generate the timebin by using self.time_constraint.\n",
    "        \n",
    "        Used to get a timebin given a timeconstraint.\n",
    "        \"\"\"\n",
    "        all_movies_watched = self.get_movies_watched(user_id, time_constraint=self.time_constraint)\n",
    "        return all_movies_watched.iloc[timebin_i:(timebin_i+timebin_size)]\n",
    "    \n",
    "    \n",
    "    def get_neighbour_timebins_with_the_movie(self, neighbour_id:int, movie_id:int,\n",
    "                                              neighbour_min_s, neighbour_max_s, neighbour_step_size)->list:\n",
    "        \"\"\"\n",
    "        Given a neighbour_id, take the all the timebins of this neighbour which includes the given movie.\n",
    "        \n",
    "        :param neighbour_min_s: Minimum number of movies to include inside neighbour timebins\n",
    "        :param neighbour_max_s: Maximum number of movies to include inside neighbour timebins\n",
    "        :param neighbour_step_size: Number of movies to extend per step the timebin size\n",
    "        \"\"\"\n",
    "        # Start by taking all movies watched by the neighbour\n",
    "        all_movies_watched = self.get_movies_watched(neighbour_id)\n",
    "        n_movies = len(all_movies_watched)\n",
    "        neighbour_timebins = list()\n",
    "\n",
    "        # For each timebin_size\n",
    "        for timebin_size in range(neighbour_min_s, neighbour_max_s, neighbour_step_size):\n",
    "            # Traverse the all movies by taking 'timebin_size' piece of ratings per loop\n",
    "            for i in range(0, n_movies, timebin_size):\n",
    "                timebin = all_movies_watched.iloc[i:i+timebin_size]\n",
    "                # Take only timebins which includes this movie\n",
    "                if not (movie_id in timebin.index):\n",
    "                    continue\n",
    "                # Insert the timebin, its index i, and its size timebin_size into the list as a tuple\n",
    "                neighbour_timebins.append(  (timebin, i, timebin_size)  )\n",
    "        return neighbour_timebins\n",
    "\n",
    "    def get_timebin_neighbours_with_the_movie(self, user_id:int, timebin_i, timebin_size, movie_id):\n",
    "        ratings = self.ratings\n",
    "        # Recreate the user timebin using its identifiers\n",
    "        timebin = self.get_timebin3(user_id, timebin_i, timebin_size)\n",
    "       \n",
    "        # :::: Change filter on ratings.loc by adding time constraint in case we filter future neighbour timebins::::\n",
    "        \n",
    "        # Count number of common ratings with other users\n",
    "        userlist = [0 for i in range(611)]\n",
    "        for movie_id in timebin.index.values.tolist():\n",
    "            users_who_watched = ratings.loc[(ratings['item_id'] == movie_id)][['user_id']].values.tolist()\n",
    "            for user_who_watched in users_who_watched:\n",
    "                userlist[user_who_watched[0]] += 1\n",
    "\n",
    "        # save as neighbour, if common rating count greater than k and given rating to the movie index at i\n",
    "        neighbour_id_list = []\n",
    "        for j in range(0, 611):\n",
    "            if( (userlist[j] > self.k) and \n",
    "               (self.trainset_movie.get_movie_rating(user_id=j, movie_id=movie_id) != 0) and \n",
    "               (j != user_id) ):\n",
    "                neighbour_id_list.append(j)\n",
    "\n",
    "        return neighbour_id_list\n",
    "                 \n",
    "    def get_similar_timebins(self, user_id, timebin_i, timebin_size,\n",
    "                             neighbour_min_s, neighbour_max_s, neighbour_step_size,\n",
    "                             corr_threshold):\n",
    "        \"\"\"\n",
    "        :param timebin_i: timebin identifier, identifies start of timebin\n",
    "        :param timebin_size: timebin identifier, identifies size of the timebin starting at timebin_i index\n",
    "        :param neighbour_min_s: Minimum number of movies to include inside neighbour timebins \n",
    "        :param neighbour_max_s: Maximum number of movies to include inside neighbour timebins\n",
    "        :param neighbour_step_size: Number of movies to extend per step the timebin size\n",
    "        \"\"\"\n",
    "        \n",
    "        # Recreate the user timebin using its identifiers\n",
    "        timebin = self.get_timebin3(user_id, timebin_i, timebin_size)\n",
    "        time_constraint = self.time_constraint\n",
    "        \n",
    "        # Neighbours contains users who has rated self.k movies in common \n",
    "        # and also rated last movie of the timebin by the way, remember we are trying to predict last movie\n",
    "        neighbours = self.get_timebin_neighbours_with_the_movie(user_id, timebin_i, timebin_size, movie_id=self.movie_id)\n",
    "\n",
    "        data = list()\n",
    "        for neighbour_id in neighbours:\n",
    "            # Get all possible timebins of the neighbour that includes the movie with a timebin size with given constraints.\n",
    "            neighbour_timebins = self.get_neighbour_timebins_with_the_movie(neighbour_id=neighbour_id, movie_id=self.movie_id,\n",
    "                                                                            neighbour_min_s=neighbour_min_s,\n",
    "                                                                            neighbour_max_s=neighbour_max_s,\n",
    "                                                                            neighbour_step_size=neighbour_step_size)\n",
    "            \n",
    "            # For each neighbour timebin, calculate pearson between the timebin and neighbour timebin \n",
    "            for neighbour_timebin, timebin_i, timebin_size in neighbour_timebins:\n",
    "                \n",
    "                # Calculate user avg for each timebin by assuming system at the time neighbour rated the last movie\n",
    "                neighbour_time_constraint = TimeConstraint(end_dt=neighbour_timebin.iloc[-1]['timestamp'])\n",
    "                neighbour_avg_rating = self.trainset_user.get_user_avg_at(neighbour_id, neighbour_time_constraint.end_dt)\n",
    "            \n",
    "                # Filter unrelated movie ratings and only keep common movie ratings\n",
    "                merged = neighbour_timebin.merge(self.timebin, on='item_id')\n",
    "                common_elements = len(merged)\n",
    "\n",
    "                # If less than self.r movies in common rated in the neighbour_timebin, dont process this timebin\n",
    "                if common_elements < self.r:\n",
    "                    continue\n",
    "\n",
    "                # Calculate Pearson Correlation between neighbour timebin and self.timebin\n",
    "                corr = self.find_timebin_corr(merged, neighbour_avg_rating)\n",
    "\n",
    "                # Dont include the neighbour timebin if corr is invalid or less than corr_threshold\n",
    "                if math.isnan(corr) or corr < corr_threshold:                     \n",
    "                    continue\n",
    "\n",
    "                binsize_in_days = TimebinSimilarity.get_timebin_size(\n",
    "                                    TimeConstraint(start_dt=neighbour_timebin.iloc[0]['timestamp'], \n",
    "                                                   end_dt=neighbour_timebin.iloc[-1]['timestamp']))\n",
    "\n",
    "                data.append( (neighbour_id, common_elements, corr, timebin_i, timebin_size, binsize_in_days) )\n",
    "\n",
    "        return pd.DataFrame(data, columns=['neighbour_id', 'n_common','pearson_corr', 'timebin_i', 'timebin_size', 'timebin_size_in_days'])\n",
    "\n",
    "    def get_timebin_neighbours_data_for_no_bias(self, similar_timebins, movie_id):\n",
    "        \"\"\"\n",
    "            Collect neighbour ratings to the movie with id 'movie_id'.\n",
    "        \"\"\"\n",
    "        data = list()\n",
    "        \n",
    "        for row in similar_timebins.itertuples(index=False):\n",
    "            # Get neighbour timebin data\n",
    "            neighbour_id = row[0]\n",
    "            n_common = row[1]\n",
    "            corr = row[2]\n",
    "            timebin_i = row[3]\n",
    "            timebin_size = row[4]\n",
    "            timebin_size_in_days = row[5]\n",
    "            \n",
    "            # Create the neighbour timebin from its data\n",
    "            neighbour_bin = self.get_timebin2(neighbour_id, timebin_i, timebin_size)\n",
    "            rating = None\n",
    "            try: \n",
    "                rating = neighbour_bin.loc[movie_id]     # test whether the neighbour has rating for the item\n",
    "            except KeyError:\n",
    "                continue                                 # if neighbour dont have rating for the item, then pass this round of loop\n",
    "            \n",
    "            # insert rating for the neighbour into data \n",
    "            #  and also insert other data identifying the neighbour(for in case we use them in analysis)\n",
    "            if rating is not None:\n",
    "                data.append( (rating[0], corr, neighbour_id, n_common, timebin_i, timebin_size, timebin_size_in_days) )\n",
    "        \n",
    "        return data \n",
    "    \n",
    "    def analize_timebin_prediction(self, n, k=10, min_neighbour=3, corr_threshold=None, \n",
    "                                   neighbour_min_s=None, neighbour_max_s=None, neighbour_step_size=None):\n",
    "        \"\"\"\n",
    "        :param n: Number of prediction to make where each prediction is for a randomly chosen users' a random movie\n",
    "        :param k: K nearest neighbour of the timebin based neighbourhood will be taken when making prediction.\n",
    "        :param min_neighbour: Minimum number of neighbour timebins when making prediction.\n",
    "        :param corr_threshold: Overrides the default behaviour if you set any value other than None\n",
    "        :param neighbour_min_s: Minimum number of movies to include inside neighbour timebins\n",
    "                                Overrides default behaviour of the class if given any value other than None \n",
    "        :param neighbour_max_s: Maximum number of movies to include inside neighbour timebins\n",
    "                                Overrides default behaviour of the class if given any value other than None\n",
    "        :param neighbour_step_size: Increase of number of movies in the timebin per step\n",
    "                                Overrides default behaviour of the class if given any value other than None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Override default hyperparameters if given as parameter\n",
    "        if corr_threshold is None:\n",
    "            corr_threshold = self.corr_threshold\n",
    "            \n",
    "        if neighbour_min_s is None:\n",
    "            neighbour_min_s = self.neighbour_min_s\n",
    "        \n",
    "        if neighbour_max_s is None:\n",
    "            neighbour_max_s = self.neighbour_max_s\n",
    "        \n",
    "        if neighbour_step_size is None:\n",
    "            neighbour_step_size = self.neighbour_step_size\n",
    "        \n",
    "        output = list()\n",
    "        \n",
    "        # collected data count\n",
    "        count = 0\n",
    "        normal_predictions = list()\n",
    "        timebin_predictions = list()\n",
    "        \n",
    "        while count < n:\n",
    "            \n",
    "            # Choose a random user\n",
    "            user_id = random.randint(0,610)    # User Ids are in between 0 and 610 in the movielens dataset.\n",
    "            \n",
    "            ### Choose Random timebin\n",
    "            \n",
    "            # Get all movies that has been watched by the user\n",
    "            movies_watched = self.get_movies_watched(user_id)\n",
    "            \n",
    "            # In order to simulate that we are making prediction in random point in time\n",
    "            # take a random starting point and take self.s piece movies starting from that point as timebin.\n",
    "            max_movie_index = ( len(movies_watched) - 1 ) - self.s\n",
    "            if max_movie_index <= 0:\n",
    "                continue\n",
    "            timebin_i = random.randint(0, max_movie_index)\n",
    "            timebin = self.get_timebin2(user_id, timebin_i, self.s)\n",
    "            \n",
    "            # Get max limit time constraint where max limit is the time we rate the last movie inside the timebin\n",
    "            time_constraint = TimeConstraint(end_dt=(timebin.iloc[-1]['timestamp'] - timedelta(seconds=1))) \n",
    "            self.time_constraint = time_constraint\n",
    "            # This is saved because we need to get the movie_id which is found at index i\n",
    "            self.timebin = timebin\n",
    "            \n",
    "            # Used for correlation calculations\n",
    "            self.user_id = user_id\n",
    "            self.timebin_user_avg_rating = self.trainset_user.get_user_avg_at(user_id, time_constraint.end_dt)\n",
    "            \n",
    "            timebin_size = len(timebin)\n",
    "            \n",
    "            # Make sure we have a valid timebin before moving forward\n",
    "            if timebin is None or timebin_size < self.r:\n",
    "                continue\n",
    "            \n",
    "            ### Predict last movie of the timebin with Normal way(KNN with Pearson Correlation)\n",
    "            movie_id = timebin.index[-1]\n",
    "            self.movie_id = movie_id\n",
    "            \n",
    "            # When making prediction, assume is\n",
    "            normal_prediction = self.trainset.predict_movie(user_id=user_id, movie_id=movie_id,\n",
    "                                                            time_constraint=self.time_constraint)\n",
    "            \n",
    "            # In order to compare predictions, normal prediction must exists\n",
    "            if normal_prediction == 0:\n",
    "                continue\n",
    "            \n",
    "            ### Predict last movie of the timebin with Timebin Based K Nearest Neighbourhood\n",
    "            similar_timebins = self.get_similar_timebins(user_id, timebin_i, timebin_size,\n",
    "                                                         neighbour_min_s=neighbour_min_s,\n",
    "                                                         neighbour_max_s=neighbour_max_s,\n",
    "                                                         neighbour_step_size=neighbour_step_size,\n",
    "                                                         corr_threshold=corr_threshold)\n",
    "            if similar_timebins.empty:\n",
    "                continue  \n",
    "\n",
    "            # Drop duplicate (neighbour_id, n_common, pearson_corr, timebin_i, timebin_size) tuples so that now no repeating timebin exists\n",
    "            similar_timebins.drop_duplicates(['neighbour_id','n_common', 'pearson_corr', 'timebin_i'], inplace=True)\n",
    "\n",
    "            # collect neighbour ratings for the movie as well as neighbour correlations between the timebin\n",
    "            data = self.get_timebin_neighbours_data_for_no_bias(similar_timebins, movie_id)\n",
    "\n",
    "            prediction = TimebinSimilarity.predict_movie(data, k=k, min_neighbour_count=min_neighbour)\n",
    "            actual = self.trainset_movie.get_movie_rating(movie_id=movie_id, user_id=user_id)\n",
    "            \n",
    "            # In order to compare predictions, timebin based prediction must exists\n",
    "            if prediction == 0:\n",
    "                continue\n",
    "            \n",
    "            print(movie_id, normal_prediction, prediction, actual)  # Debug Output\n",
    "            \n",
    "            normal_predictions.append( (normal_prediction, actual) )\n",
    "            timebin_predictions.append( (prediction, actual) )\n",
    "            \n",
    "            # Save the data\n",
    "            output.append( (user_id, timebin_i, timebin_size, data) )\n",
    "            # we saved a data succesfully, increment collected data cout\n",
    "            count += 1\n",
    "        \n",
    "        # RMSE for debug output\n",
    "        normal_rmse = Accuracy.rmse(normal_predictions)\n",
    "        normal_threshold_accuracy = Accuracy.threshold_accuracy(normal_predictions)\n",
    "        \n",
    "        timebins_rmse = Accuracy.rmse(timebin_predictions)\n",
    "        timebin_threshold_accuracy = Accuracy.threshold_accuracy(timebin_predictions)\n",
    "        \n",
    "\n",
    "        # Debug output\n",
    "        print(f\"Normal RMSE: {normal_rmse:.2} Timebin RMSE:{timebins_rmse:.2} \\\n",
    "                Normal Threshold Acc: {int(normal_threshold_accuracy * 100)}% Timebin Threshold Acc: {int(timebin_threshold_accuracy * 100)}%\")\n",
    "        \n",
    "        result = {\n",
    "            \"output\":output,\n",
    "            \"normal_predictions\":normal_predictions,\n",
    "            \"timebin_predictions\":timebin_predictions\n",
    "        }\n",
    "        \n",
    "        return result         \n",
    "            \n",
    "    def get_timebin_neighbours_data(self, timebin, similar_timebins):\n",
    "        \"\"\"\n",
    "        Get timebin possessors' ratings and timebin correlations as the output\n",
    "        \"\"\"\n",
    "        # Store data in order to return as result\n",
    "        data = defaultdict(list)\n",
    "        for row in similar_timebins.itertuples(index=False):\n",
    "            # Get neighbour timebin data\n",
    "            neighbour_id = row[0]\n",
    "            n_common = row[1]\n",
    "            corr = row[2]\n",
    "            timebin_i = row[3]\n",
    "            timebin_size = row[4]\n",
    "            timebin_size_in_days = row[5]\n",
    "            \n",
    "            # Create the neighbour timebin from its data\n",
    "            neighbour_bin = self.get_timebin2(neighbour_id, timebin_i, timebin_size)\n",
    "            \n",
    "            # Get rid of movie ratings of the neighbour that are not found in the timebin\n",
    "            merged_bin = pd.merge(timebin, neighbour_bin, left_index=True, right_index=True)\n",
    "            \n",
    "            # For each common movie rating, store neighbour rating and its correlation to the timebin\n",
    "            for bin_row in merged_bin.itertuples(index=True):\n",
    "                curr_movie = bin_row[0]\n",
    "                neighbour_rating = bin_row[3]\n",
    "                #print(bin_row[0], bin_row[1], bin_row[2], bin_row[3], bin_row[4])\n",
    "                data[curr_movie].append( (neighbour_rating, corr, neighbour_id, n_common, timebin_i, timebin_size, timebin_size_in_days) )\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_movie_with_user_corrs(movie_id, similar_timebins, trainset_movie, k=10, min_neighbour_count=5):\n",
    "        neighbours_data = similar_timebins[['neighbour_id', 'pearson_corr']].groupby('neighbour_id').mean()\n",
    "        neighbours_data.sort_values(by='pearson_corr', ascending=False, inplace=True)\n",
    "        \n",
    "        weighted_sum = 0\n",
    "        weight_sum = 0\n",
    "        count = 0\n",
    "        for index, row in similar_timebins[['neighbour_id', 'pearson_corr']].groupby('neighbour_id').mean().iterrows():\n",
    "            neighbour_id = index\n",
    "            corr = row[0]\n",
    "            rating = trainset_movie.get_movie_rating(neighbour_id, movie_id)\n",
    "            weighted_sum += rating * corr\n",
    "            weight_sum += corr\n",
    "            # Only take k nearest neighbours\n",
    "            if count == k:\n",
    "                break\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "        if count < min_neighbour_count:\n",
    "            return 0\n",
    "        \n",
    "        prediction = weighted_sum / weight_sum\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_movie(data, k=10,  min_neighbour_count=5):\n",
    "        \"\"\"\n",
    "        Make prediction by using the neighbour timebin data on the movie of interest.\n",
    "        K nearest neighbours is used only.\n",
    "        \"\"\"\n",
    "        # Sort the data by corr\n",
    "        data.sort(key=lambda data_element: data_element[1])\n",
    "        \n",
    "        # each row of data ->(rating, corr, neighbour_id, n_common, timebin_i, timebin_size, timebin_size_in_days)\n",
    "        weighted_sum = 0\n",
    "        weight_sum = 0\n",
    "        count = 0\n",
    "        for (rating, corr, _, _, _, _, _) in data:\n",
    "            weighted_sum += rating * corr\n",
    "            weight_sum += corr\n",
    "            \n",
    "            # Only take k neighbours\n",
    "            if count == k:\n",
    "                break\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "        if count < min_neighbour_count:\n",
    "            return 0\n",
    "        \n",
    "        prediction = weighted_sum / weight_sum\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_timebin_size(tc: TimeConstraint):\n",
    "        return abs((tc.start_dt - tc.end_dt).days)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MovieLensDataset(is_movies_cached=True, is_ratings_cached=True)\n",
    "timebin_similarity = TimebinSimilarity(ratings=dataset.ratings, trainset=t, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240 4.504523540876766 2.8552265214726193 3.0\n",
      "260 4.226626653224825 4.453193651060223 5.0\n",
      "39 2.2389049254990385 2.2174702656245504 1.0\n",
      "597 3.7993242227922406 4.584437980378289 4.0\n",
      "637 3.0479463391057333 3.0 3.0\n",
      "Normal RMSE: 0.85 Timebin RMSE:0.3                 Normal Threshold Acc: 80% Timebin Threshold Acc: 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': [(407,\n",
       "   4,\n",
       "   10,\n",
       "   [(0.5, 0.6591483553508753, 393, 6, 0, 30, 1),\n",
       "    (0.5, 0.6831221776796662, 393, 7, 0, 45, 1),\n",
       "    (0.5, 0.6867012199518078, 393, 7, 0, 40, 1),\n",
       "    (4.5, 0.6893528248754103, 590, 3, 300, 30, 1),\n",
       "    (4.5, 0.6928372536305774, 590, 3, 300, 25, 1),\n",
       "    (0.5, 0.7070496990494536, 393, 6, 0, 35, 1),\n",
       "    (4.0, 0.7205699401790749, 249, 5, 120, 10, 1),\n",
       "    (4.0, 0.7230004039630177, 249, 5, 120, 20, 1),\n",
       "    (4.0, 0.7231210337394206, 249, 5, 120, 15, 1),\n",
       "    (4.0, 0.73549027416856, 249, 5, 120, 30, 1),\n",
       "    (4.0, 0.740950183165002, 249, 5, 120, 40, 1),\n",
       "    (0.5, 0.7470098314550189, 393, 6, 0, 20, 1),\n",
       "    (0.5, 0.757774969025682, 393, 6, 0, 25, 1),\n",
       "    (4.0, 0.7848153355232531, 249, 6, 105, 35, 1),\n",
       "    (4.0, 0.7849049569745824, 249, 6, 90, 45, 1),\n",
       "    (4.0, 0.7894436879929796, 249, 3, 120, 5, 1),\n",
       "    (4.0, 0.8562003092819243, 249, 4, 100, 25, 1),\n",
       "    (4.0, 0.9284766908852594, 561, 5, 0, 25, 1),\n",
       "    (4.0, 0.9331257768114828, 561, 3, 15, 15, 1),\n",
       "    (4.0, 0.9440975861072656, 561, 5, 0, 30, 1),\n",
       "    (4.0, 0.9625548819258521, 561, 5, 0, 35, 1),\n",
       "    (4.0, 0.9762653955817026, 561, 5, 0, 40, 1),\n",
       "    (4.0, 0.9781345640797963, 561, 5, 0, 45, 1)]),\n",
       "  (399,\n",
       "   3,\n",
       "   10,\n",
       "   [(4.0, 0.5050670732931236, 560, 3, 20, 10, 1),\n",
       "    (4.0, 0.5050670732931236, 560, 3, 15, 15, 1),\n",
       "    (5.0, 0.5067311397387995, 7, 5, 35, 35, 1),\n",
       "    (5.0, 0.5074297797680238, 7, 5, 0, 40, 1),\n",
       "    (4.0, 0.5079946648246282, 330, 3, 70, 35, 1),\n",
       "    (5.0, 0.5099532969254159, 17, 4, 0, 25, 1),\n",
       "    (4.5, 0.5146040668207171, 477, 6, 30, 30, 1),\n",
       "    (4.5, 0.5147850482322626, 477, 5, 25, 25, 1),\n",
       "    (4.5, 0.5335042250634191, 215, 4, 20, 20, 1),\n",
       "    (4.5, 0.5335042250634191, 215, 4, 0, 40, 1),\n",
       "    (4.0, 0.5373281523563376, 254, 3, 20, 5, 1),\n",
       "    (4.0, 0.5499002544629354, 352, 3, 250, 25, 1),\n",
       "    (5.0, 0.551580386465119, 445, 7, 0, 45, 1),\n",
       "    (5.0, 0.5528240522845205, 445, 7, 0, 35, 1),\n",
       "    (5.0, 0.5564892358644906, 445, 7, 0, 40, 1),\n",
       "    (5.0, 0.5594682139507829, 445, 7, 0, 30, 1),\n",
       "    (4.0, 0.5641935448843325, 330, 3, 75, 25, 1),\n",
       "    (5.0, 0.5701706418388921, 7, 5, 30, 30, 1),\n",
       "    (5.0, 0.575641845675486, 445, 7, 0, 25, 1),\n",
       "    (5.0, 0.5802325864906867, 7, 3, 25, 25, 1),\n",
       "    (4.5, 0.5907633270495044, 477, 4, 30, 15, 1),\n",
       "    (4.5, 0.5907633270495044, 477, 4, 0, 45, 1),\n",
       "    (5.0, 0.5914616601252702, 438, 3, 90, 45, 1),\n",
       "    (5.0, 0.5924807606051967, 438, 3, 105, 35, 1),\n",
       "    (4.5, 0.5935321595963022, 477, 4, 30, 10, 1),\n",
       "    (4.5, 0.5935321595963022, 477, 4, 20, 20, 1),\n",
       "    (4.5, 0.5935321595963022, 477, 4, 0, 40, 1),\n",
       "    (5.0, 0.6039314547859405, 7, 3, 30, 15, 1),\n",
       "    (4.5, 0.6099812277131705, 480, 7, 30, 30, 1),\n",
       "    (4.5, 0.6113538412474933, 480, 7, 25, 25, 1),\n",
       "    (4.5, 0.619684294892914, 166, 5, 35, 35, 4),\n",
       "    (4.0, 0.6241401466099096, 254, 7, 15, 15, 1),\n",
       "    (4.0, 0.6241401466099096, 254, 7, 0, 30, 1),\n",
       "    (4.0, 0.6247747186844101, 254, 7, 0, 35, 1),\n",
       "    (5.0, 0.6315897471896849, 7, 3, 30, 10, 1),\n",
       "    (4.5, 0.6321810566247094, 215, 3, 0, 35, 1),\n",
       "    (4.0, 0.6497597893446749, 254, 8, 0, 40, 1),\n",
       "    (5.0, 0.65813592250622, 7, 4, 20, 20, 1),\n",
       "    (5.0, 0.6598840764947939, 249, 3, 125, 25, 1),\n",
       "    (5.0, 0.6975942942558628, 464, 6, 0, 45, 1),\n",
       "    (4.0, 0.7001525223159017, 254, 5, 20, 10, 1),\n",
       "    (4.0, 0.7012279916456964, 254, 10, 0, 45, 1),\n",
       "    (5.0, 0.715487181682066, 249, 5, 120, 40, 1),\n",
       "    (5.0, 0.7192745510669597, 249, 5, 120, 30, 1),\n",
       "    (4.0, 0.7244196978846288, 254, 6, 20, 20, 1),\n",
       "    (5.0, 0.7407197790917299, 464, 6, 0, 40, 1),\n",
       "    (5.0, 0.7768978980662657, 464, 6, 0, 35, 1),\n",
       "    (4.5, 0.7937266669447165, 480, 3, 45, 15, 1),\n",
       "    (4.5, 0.7937266669447165, 480, 3, 40, 20, 1),\n",
       "    (4.5, 0.7940577157462728, 480, 3, 45, 5, 1),\n",
       "    (4.5, 0.7940577157462728, 480, 3, 40, 10, 1),\n",
       "    (4.5, 0.7951635547199197, 480, 3, 40, 40, 1),\n",
       "    (4.5, 0.7957586807691438, 480, 3, 45, 45, 1),\n",
       "    (5.0, 0.8147454318226199, 464, 5, 0, 30, 1),\n",
       "    (5.0, 0.8166463175472689, 219, 4, 70, 35, 1),\n",
       "    (4.5, 0.8263705801389032, 91, 3, 220, 20, 1),\n",
       "    (4.5, 0.8263705801389032, 91, 3, 210, 30, 1),\n",
       "    (4.5, 0.8263705801389032, 91, 3, 200, 40, 1),\n",
       "    (4.5, 0.8269839257859585, 91, 3, 210, 35, 1),\n",
       "    (5.0, 0.8551861104941365, 445, 3, 5, 5, 1),\n",
       "    (4.5, 0.8710205614930396, 480, 5, 35, 35, 1),\n",
       "    (5.0, 0.8799364222503123, 445, 4, 0, 15, 1),\n",
       "    (5.0, 0.9061423126590729, 445, 4, 0, 20, 1),\n",
       "    (5.0, 0.9180238333274535, 68, 3, 20, 5, 1),\n",
       "    (5.0, 0.9268846834069854, 68, 4, 0, 25, 1),\n",
       "    (5.0, 0.9918101391215652, 219, 3, 60, 30, 1),\n",
       "    (5.0, 0.9918101391215652, 219, 3, 45, 45, 1)]),\n",
       "  (283,\n",
       "   10,\n",
       "   10,\n",
       "   [(3.0, 0.5291611381801313, 570, 3, 70, 35, 1),\n",
       "    (3.0, 0.5311924356846104, 570, 3, 75, 25, 1),\n",
       "    (3.0, 0.6046175369533192, 45, 4, 135, 45, 1),\n",
       "    (3.0, 0.6148500138739208, 45, 4, 120, 40, 5),\n",
       "    (1.5, 0.6674453846766956, 91, 3, 300, 30, 1),\n",
       "    (1.5, 0.6689118579044665, 91, 3, 300, 25, 1),\n",
       "    (2.0, 0.7339273409178716, 266, 4, 60, 30, 1),\n",
       "    (2.0, 0.7400145477965553, 266, 6, 45, 45, 8),\n",
       "    (2.0, 0.7567466272667887, 266, 4, 35, 35, 8),\n",
       "    (2.0, 0.7804364815713684, 266, 5, 50, 25, 1),\n",
       "    (2.0, 0.7835603630174391, 266, 3, 60, 15, 1),\n",
       "    (2.0, 0.786996273178369, 266, 5, 40, 40, 8),\n",
       "    (3.0, 0.7893176570471789, 570, 3, 60, 20, 1),\n",
       "    (3.0, 0.7893176570471789, 570, 3, 40, 40, 1),\n",
       "    (3.0, 0.7894768041099958, 570, 3, 60, 30, 1),\n",
       "    (3.0, 0.7894768041099958, 570, 3, 45, 45, 1),\n",
       "    (2.0, 0.7949746681277818, 266, 3, 60, 20, 1),\n",
       "    (2.5, 0.8485196355541049, 480, 3, 630, 45, 1),\n",
       "    (3.0, 0.9503070859335623, 226, 3, 30, 30, 1)]),\n",
       "  (410,\n",
       "   153,\n",
       "   10,\n",
       "   [(5.0, 0.5289662971091202, 42, 3, 150, 25, 1),\n",
       "    (5.0, 0.5289662971091202, 42, 3, 140, 35, 1),\n",
       "    (5.0, 0.532213604837768, 42, 3, 160, 20, 1),\n",
       "    (5.0, 0.532213604837768, 42, 3, 150, 30, 1),\n",
       "    (5.0, 0.532213604837768, 42, 3, 135, 45, 1),\n",
       "    (5.0, 0.5737230435452118, 42, 3, 160, 40, 1),\n",
       "    (4.0, 0.7506072736322977, 555, 3, 35, 35, 1),\n",
       "    (4.0, 0.7513271195722615, 555, 3, 50, 25, 1),\n",
       "    (4.0, 0.7935314024601676, 555, 4, 40, 40, 1)]),\n",
       "  (277,\n",
       "   4,\n",
       "   10,\n",
       "   [(3.0, 0.6668687412122364, 529, 9, 0, 25, 1),\n",
       "    (3.0, 0.702966521366815, 529, 7, 10, 10, 1),\n",
       "    (3.0, 0.7050173133057521, 529, 9, 0, 20, 1),\n",
       "    (3.0, 0.9632850716341468, 529, 3, 15, 5, 1),\n",
       "    (3.0, 0.9999750009374611, 529, 3, 15, 15, 1)])],\n",
       " 'normal_predictions': [(4.504523540876766, 3.0),\n",
       "  (4.226626653224825, 5.0),\n",
       "  (2.2389049254990385, 1.0),\n",
       "  (3.7993242227922406, 4.0),\n",
       "  (3.0479463391057333, 3.0)],\n",
       " 'timebin_predictions': [(2.8552265214726193, 3.0),\n",
       "  (4.453193651060223, 5.0),\n",
       "  (2.2174702656245504, 1.0),\n",
       "  (4.584437980378289, 4.0),\n",
       "  (3.0, 3.0)]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_data = timebin_similarity.analize_timebin_prediction(5)\n",
    "prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407,\n",
       " 4,\n",
       " 10,\n",
       " [(0.5, 0.6591483553508753, 393, 6, 0, 30, 1),\n",
       "  (0.5, 0.6831221776796662, 393, 7, 0, 45, 1),\n",
       "  (0.5, 0.6867012199518078, 393, 7, 0, 40, 1),\n",
       "  (4.5, 0.6893528248754103, 590, 3, 300, 30, 1),\n",
       "  (4.5, 0.6928372536305774, 590, 3, 300, 25, 1),\n",
       "  (0.5, 0.7070496990494536, 393, 6, 0, 35, 1),\n",
       "  (4.0, 0.7205699401790749, 249, 5, 120, 10, 1),\n",
       "  (4.0, 0.7230004039630177, 249, 5, 120, 20, 1),\n",
       "  (4.0, 0.7231210337394206, 249, 5, 120, 15, 1),\n",
       "  (4.0, 0.73549027416856, 249, 5, 120, 30, 1),\n",
       "  (4.0, 0.740950183165002, 249, 5, 120, 40, 1),\n",
       "  (0.5, 0.7470098314550189, 393, 6, 0, 20, 1),\n",
       "  (0.5, 0.757774969025682, 393, 6, 0, 25, 1),\n",
       "  (4.0, 0.7848153355232531, 249, 6, 105, 35, 1),\n",
       "  (4.0, 0.7849049569745824, 249, 6, 90, 45, 1),\n",
       "  (4.0, 0.7894436879929796, 249, 3, 120, 5, 1),\n",
       "  (4.0, 0.8562003092819243, 249, 4, 100, 25, 1),\n",
       "  (4.0, 0.9284766908852594, 561, 5, 0, 25, 1),\n",
       "  (4.0, 0.9331257768114828, 561, 3, 15, 15, 1),\n",
       "  (4.0, 0.9440975861072656, 561, 5, 0, 30, 1),\n",
       "  (4.0, 0.9625548819258521, 561, 5, 0, 35, 1),\n",
       "  (4.0, 0.9762653955817026, 561, 5, 0, 40, 1),\n",
       "  (4.0, 0.9781345640797963, 561, 5, 0, 45, 1)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_data[\"output\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TimebinSimilarity' object has no attribute 'p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-a1e7c8923925>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTimebinSimilarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_movie\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"output\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_neighbour_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimebin_similarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'TimebinSimilarity' object has no attribute 'p'"
     ]
    }
   ],
   "source": [
    "TimebinSimilarity.predict_movie(prediction_data[\"output\"][0][3], k=10, min_neighbour_count = timebin_similarity.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-6dae06af3e71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"output\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-9fc58bee139c>\u001b[0m in \u001b[0;36mthreshold_accuracy\u001b[1;34m(predictions)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mnumber_of_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mnumber_of_hit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                     \u001b[0mactual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold_round_rating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "Accuracy.threshold_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

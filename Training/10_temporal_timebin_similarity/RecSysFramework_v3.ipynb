{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeConstraint:\n",
    "\n",
    "    def __init__(self, end_dt, start_dt=None):\n",
    "        \"\"\"\n",
    "        When end_dt is only given, system will have a max time constraint only.\n",
    "\n",
    "        When end_dt and start_dt are given, system will have beginning end ending boundary.\n",
    "\n",
    "        :param end_dt: The maximum limit of the time constraint.\n",
    "        :param start_dt: The minimum limit of the time constraint.\n",
    "            Always set start_dt to None if you change the object from time_bin to max_limit.\n",
    "        \"\"\"\n",
    "        self.end_dt = end_dt\n",
    "        self.start_dt = start_dt\n",
    "\n",
    "    def is_valid_time_bin(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether this TimeConstraint object represents a valid time bin.\n",
    "        \"\"\"\n",
    "        if self.is_time_bin() and (self._end_dt > self._start_dt):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_valid_max_limit(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether this TimeConstraint represents a valid max time limit.\n",
    "        \"\"\"\n",
    "        if (self._end_dt is not None) and (self._start_dt is None):\n",
    "            return True\n",
    "\n",
    "    def is_time_bin(self) -> bool:\n",
    "        if (self._start_dt is not None) and (self._end_dt is not None):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Comparing TimeConstraints\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return self._start_dt == other.start_dt and self._end_dt == other.end_dt\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return self._start_dt != other.start_dt or self._end_dt != other.end_dt\n",
    "\n",
    "    # Properties\n",
    "\n",
    "    @property\n",
    "    def end_dt(self):\n",
    "        return self._end_dt\n",
    "\n",
    "    @end_dt.setter\n",
    "    def end_dt(self, value):\n",
    "        self._end_dt = value\n",
    "\n",
    "    @property\n",
    "    def start_dt(self):\n",
    "        return self._start_dt\n",
    "\n",
    "    @start_dt.setter\n",
    "    def start_dt(self, value):\n",
    "        self._start_dt = value\n",
    "\n",
    "    # Printing TimeConstraints\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"(start = {self._start_dt}, end= {self._end_dt})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"(start = {self._start_dt}, end= {self._end_dt})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache:\n",
    "\n",
    "    def __init__(self,\n",
    "                 is_ratings_cached=False,\n",
    "                 ratings=None,\n",
    "                 is_movies_cached=False,\n",
    "                 movies=None,\n",
    "                 is_movie_ratings_cached=False,\n",
    "                 movie_ratings=None,\n",
    "                 is_user_movie_matrix_cached=False,\n",
    "                 user_movie_matrix=None,\n",
    "                 is_user_correlations_cached=False,\n",
    "                 user_correlations=None,\n",
    "                 min_common_elements=5,\n",
    "                 use_avg_ratings_cache=True):\n",
    "        \"\"\" Cached data is only valid when the boolean specifier is True \"\"\"\n",
    "\n",
    "        # 30% performance\n",
    "        self.is_ratings_cached = is_ratings_cached\n",
    "        self.ratings = ratings\n",
    "\n",
    "        # 7 fold performance gain on 'movie' related queries\n",
    "        self.is_movies_cached = is_movies_cached\n",
    "        self.movies = movies\n",
    "\n",
    "        self.is_movie_ratings_cached = is_movie_ratings_cached\n",
    "        self.movie_ratings = movie_ratings\n",
    "\n",
    "        self.is_user_movie_matrix_cached = is_user_movie_matrix_cached\n",
    "        self.user_movie_matrix = user_movie_matrix\n",
    "\n",
    "        self.is_user_correlations_cached = is_user_correlations_cached\n",
    "        self.user_correlations = user_correlations\n",
    "\n",
    "        self.min_common_elements = min_common_elements\n",
    "\n",
    "        # if use avg ratings cache, on average 10 fold performance gain\n",
    "        self.use_avg_ratings_cache = use_avg_ratings_cache\n",
    "        if self.use_avg_ratings_cache:\n",
    "            self.avg_user_ratings = self.create_user_avg_rating_cache()\n",
    "        else:\n",
    "            self.avg_user_ratings = None\n",
    "\n",
    "    def create_user_avg_rating_cache(self):\n",
    "        if self.is_ratings_cached:\n",
    "            data = self.ratings\n",
    "        else:\n",
    "            data = self.movie_ratings\n",
    "        return data.groupby('user_id')[['rating']].mean()\n",
    "\n",
    "    def get_user_corrs(self, min_common_elements, time_constraint=None):\n",
    "        \"\"\"\n",
    "        If cached returns the cache, else none\n",
    "        :param min_common_elements: min common element in between users in order them to become neighbours\n",
    "        :param time_constraint: used in temporal caches only, None in this context\n",
    "        :return: user correlation matrix if cache found, else None\n",
    "        \"\"\"\n",
    "        if self.is_user_correlations_cached:\n",
    "            if self.min_common_elements == min_common_elements:\n",
    "                return self.user_correlations\n",
    "        return None\n",
    "\n",
    "    # Properties\n",
    "    @property\n",
    "    def ratings(self):\n",
    "        return self._ratings\n",
    "\n",
    "    @ratings.setter\n",
    "    def ratings(self, value):\n",
    "        self._ratings = value\n",
    "\n",
    "    @property\n",
    "    def movies(self):\n",
    "        return self._movies\n",
    "\n",
    "    @movies.setter\n",
    "    def movies(self, value):\n",
    "        self._movies = value\n",
    "\n",
    "    @property\n",
    "    def movie_ratings(self):\n",
    "        return self._movie_ratings\n",
    "\n",
    "    @movie_ratings.setter\n",
    "    def movie_ratings(self, value):\n",
    "        self._movie_ratings = value\n",
    "\n",
    "    @property\n",
    "    def user_movie_matrix(self):\n",
    "        return self._user_movie_matrix\n",
    "\n",
    "    @user_movie_matrix.setter\n",
    "    def user_movie_matrix(self, value):\n",
    "        self._user_movie_matrix = value\n",
    "\n",
    "    @property\n",
    "    def user_correlations(self):\n",
    "        return self._user_correlations\n",
    "\n",
    "    @user_correlations.setter\n",
    "    def user_correlations(self, value):\n",
    "        self._user_correlations = value\n",
    "\n",
    "    @property\n",
    "    def min_common_elements(self):\n",
    "        return self._min_common_elements\n",
    "\n",
    "    @min_common_elements.setter\n",
    "    def min_common_elements(self, value):\n",
    "        self._min_common_elements = value\n",
    "\n",
    "\n",
    "class TemporalCache(Cache):\n",
    "\n",
    "    def __init__(self,\n",
    "                 time_constraint: TimeConstraint,\n",
    "                 is_ratings_cached=False,\n",
    "                 ratings=None,\n",
    "                 is_movies_cached=False,\n",
    "                 movies=None,\n",
    "                 is_movie_ratings_cached=False,\n",
    "                 movie_ratings=None,\n",
    "                 is_user_movie_matrix_cached=False,\n",
    "                 user_movie_matrix=None,\n",
    "                 is_user_correlations_cached=False,\n",
    "                 user_correlations=None,\n",
    "                 min_common_elements=5,\n",
    "                 use_avg_ratings_cache=True,\n",
    "                 use_bulk_corr_cache=True):\n",
    "\n",
    "        super().__init__(is_ratings_cached=is_ratings_cached,\n",
    "                         ratings=ratings,\n",
    "                         is_movies_cached=is_movies_cached,\n",
    "                         movies=movies,\n",
    "                         is_movie_ratings_cached=is_movie_ratings_cached,\n",
    "                         movie_ratings=movie_ratings,\n",
    "                         is_user_movie_matrix_cached=is_user_movie_matrix_cached,\n",
    "                         user_movie_matrix=user_movie_matrix,\n",
    "                         is_user_correlations_cached=is_user_correlations_cached,\n",
    "                         user_correlations=user_correlations,\n",
    "                         min_common_elements=min_common_elements,\n",
    "                         use_avg_ratings_cache=use_avg_ratings_cache)\n",
    "\n",
    "        self.time_constraint = time_constraint\n",
    "        self.use_bulk_corr_cache = use_bulk_corr_cache\n",
    "        self.user_corrs_in_bulk = None\n",
    "\n",
    "    def is_temporal_cache_valid(self):\n",
    "        # No TimeConstraint, valid\n",
    "        if self._time_constraint is None:\n",
    "            return True\n",
    "        # Bin TimeConstraint or Max Limit TimeConstraint, valid\n",
    "        if self._time_constraint.is_valid_time_bin() or self._time_constraint.is_valid_max_limit():\n",
    "            return True\n",
    "        # Else, Not Valid\n",
    "        return False\n",
    "\n",
    "    def get_user_corrs_from_bulk(self, min_common_elements, time_constraint, bin_size):\n",
    "        if ((self.user_corrs_in_bulk is None) or (self.user_corrs_in_bulk is None)\n",
    "                or (time_constraint is None) or self.min_common_elements != min_common_elements):\n",
    "            return None\n",
    "\n",
    "        if time_constraint.is_valid_max_limit():\n",
    "            return self.user_corrs_in_bulk.get(time_constraint.end_dt.year)\n",
    "\n",
    "        if bin_size == -1:\n",
    "            return None\n",
    "\n",
    "        bins = self.user_corrs_in_bulk.get(bin_size)\n",
    "        if bins is not None:\n",
    "            return bins.get(time_constraint.start_dt.year)\n",
    "\n",
    "    def get_user_corrs(self, min_common_elements, time_constraint=None):\n",
    "        \"\"\"\n",
    "        If cached returns the cache, else none\n",
    "\n",
    "        :param min_common_elements: min common element in between users in order them to become neighbours\n",
    "        :param time_constraint: time constraint on user correlations\n",
    "        :return: user correlation matrix if cache found, else None\n",
    "        \"\"\"\n",
    "        if self.is_user_correlations_cached:\n",
    "            if self.time_constraint == time_constraint and self.min_common_elements == min_common_elements:\n",
    "                return self.user_correlations\n",
    "        return None\n",
    "\n",
    "    def set_user_corrs(self, user_corrs, min_common_elements, time_constraint):\n",
    "        # Only set when caching is open for user_correlations\n",
    "        if self.is_user_correlations_cached:\n",
    "            self._time_constraint = time_constraint\n",
    "            self.min_common_elements = min_common_elements\n",
    "            self.user_correlations = user_corrs\n",
    "\n",
    "    @property\n",
    "    def time_constraint(self):\n",
    "        return self._time_constraint\n",
    "\n",
    "    @time_constraint.setter\n",
    "    def time_constraint(self, value):\n",
    "        self._time_constraint = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "\n",
    "    @staticmethod\n",
    "    def rmse(predictions) -> float:\n",
    "        if type(predictions) is pd.DataFrame:\n",
    "            number_of_predictions = 0\n",
    "            sum_of_square_differences = 0.0\n",
    "            for row in predictions.itertuples(index=False):\n",
    "                # row[1] : actual rating, row[0] : prediction\n",
    "                prediction = row[0]\n",
    "                if prediction != 0:\n",
    "                    sum_of_square_differences += (row[1] - prediction) ** 2\n",
    "                    number_of_predictions += 1\n",
    "            return sum_of_square_differences / number_of_predictions if number_of_predictions != 0 else 0\n",
    "        elif type(predictions) is list:\n",
    "            number_of_predictions = 0\n",
    "            sum_of_square_differences = 0.0\n",
    "            for prediction, actual in predictions:\n",
    "                if prediction != 0:\n",
    "                    sum_of_square_differences += (actual - prediction) ** 2\n",
    "                    number_of_predictions += 1\n",
    "            return sum_of_square_differences / number_of_predictions if number_of_predictions != 0 else 0\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(ABC):\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def load():\n",
    "        \"\"\" Every subclass must provide static load method\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp'),\n",
    "                 ratings_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv',\n",
    "                 movies_col_names=('item_id', 'title', 'genres'),\n",
    "                 movies_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv',\n",
    "                 is_ratings_cached=True,\n",
    "                 is_movies_cached=True):\n",
    "        Dataset.__init__(self)\n",
    "        self.is_ratings_cached = is_ratings_cached\n",
    "        self.is_movies_cached = is_movies_cached\n",
    "        self.ratings = MovieLensDataset.load_ratings(ratings_path,\n",
    "                                                     ratings_col_names) if self.is_ratings_cached else None\n",
    "        self.movies = MovieLensDataset.load_movies(movies_path,\n",
    "                                                   movies_col_names) if self.is_movies_cached else None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_movies(movies_path,\n",
    "                    movies_col_names=('item_id', 'title', 'genres')):\n",
    "        if not os.path.isfile(movies_path) or not movies_col_names:\n",
    "            return None\n",
    "\n",
    "        # read movies\n",
    "        movies = pd.read_csv(movies_path, sep=',', header=1, names=movies_col_names)\n",
    "\n",
    "        # Extract Movie Year\n",
    "        movies['year'] = movies.title.str.extract(\"\\((\\d{4})\\)\", expand=True)\n",
    "        movies.year = pd.to_datetime(movies.year, format='%Y')\n",
    "        movies.year = movies.year.dt.year  # As there are some NaN years, resulting type will be float (decimals)\n",
    "\n",
    "        # Remove year part from the title\n",
    "        movies.title = movies.title.str[:-7]\n",
    "\n",
    "        return movies\n",
    "\n",
    "    @staticmethod\n",
    "    def load_ratings(ratings_path,\n",
    "                     ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp')):\n",
    "        if not os.path.isfile(ratings_path) or not ratings_col_names:\n",
    "            return None\n",
    "\n",
    "        # read ratings\n",
    "        ratings = pd.read_csv(ratings_path, sep=',', header=1, names=ratings_col_names)\n",
    "\n",
    "        # Convert timestamp into readable format\n",
    "        ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s', origin='unix')\n",
    "\n",
    "        return ratings\n",
    "\n",
    "    @staticmethod\n",
    "    def create_movie_ratings(ratings, movies):\n",
    "        return pd.merge(ratings, movies, on='item_id')\n",
    "\n",
    "    @staticmethod\n",
    "    def load(ratings_col_names=('user_id', 'item_id', 'rating', 'timestamp'),\n",
    "             ratings_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv',\n",
    "             movies_col_names=('item_id', 'title', 'genres'),\n",
    "             movies_path=r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv'\n",
    "             ):\n",
    "        # Load movies\n",
    "        movies = MovieLensDataset.load_movies(movies_path=movies_path, movies_col_names=movies_col_names)\n",
    "        # Load ratings\n",
    "        ratings = MovieLensDataset.load_ratings(ratings_path=ratings_path, ratings_col_names=ratings_col_names)\n",
    "\n",
    "        # Merge the ratings and movies\n",
    "        movie_ratings = pd.merge(ratings, movies, on='item_id')\n",
    "\n",
    "        return movie_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalPearson:\n",
    "\n",
    "    def __init__(self, cache: TemporalCache, time_constraint: TimeConstraint = None, min_common_elements: int = 5):\n",
    "        self.time_constraint = time_constraint\n",
    "        self.cache = cache\n",
    "        self.min_common_elements = min_common_elements\n",
    "        #from .trainset import TrainsetUser, TrainsetMovie\n",
    "        self.trainset_user = TrainsetUser(cache=self.cache)\n",
    "        self.trainset_movie = TrainsetMovie(cache=self.cache)\n",
    "\n",
    "    def mean_centered_pearson(self, user_id, movie_id, k_neighbours: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Mean Centered Prediction\n",
    "\n",
    "        :param user_id: user of interest\n",
    "        :param movie_id: the movie's rating is the one we we want to predict\n",
    "        :param k_neighbours: k nearest neighbours in DataFrame where index user_id, column correlation in between.\n",
    "        :return: Prediction rating\n",
    "        \"\"\"\n",
    "        # If a movie with movie_id not exists, predict 0\n",
    "        if self.trainset_movie.get_movie(movie_id=movie_id).empty:\n",
    "            return 0\n",
    "\n",
    "        if k_neighbours is None or k_neighbours.empty:\n",
    "            return 0\n",
    "\n",
    "        user_avg_rating = self.trainset_user.get_user_avg(user_id=user_id)\n",
    "\n",
    "        weighted_sum = 0.0\n",
    "        sum_of_weights = 0.0\n",
    "        for neighbour_id, data in k_neighbours.iterrows():\n",
    "            # Get each neighbour's correlation 'user_id' and her rating to 'movie_id'\n",
    "            neighbour_corr = data['correlation']\n",
    "            neighbour_rating = self.trainset_movie.get_movie_rating(movie_id=movie_id, user_id=neighbour_id)\n",
    "            # If the neighbour doesnt give rating to the movie_id, pass this around of the loop\n",
    "            if neighbour_rating == 0:\n",
    "                continue\n",
    "            neighbour_avg_rating = self.trainset_user.get_user_avg(user_id=neighbour_id)\n",
    "            neighbour_mean_centered_rating = neighbour_rating - neighbour_avg_rating\n",
    "            # Calculate Weighted sum and sum of weights\n",
    "            weighted_sum += neighbour_mean_centered_rating * neighbour_corr\n",
    "            sum_of_weights += neighbour_corr\n",
    "\n",
    "        # Predict\n",
    "        if sum_of_weights != 0:\n",
    "            prediction_rating = user_avg_rating + (weighted_sum / sum_of_weights)\n",
    "        else:\n",
    "            prediction_rating = 0  # In this case, none of the neighbours have given rating to 'the movie'\n",
    "\n",
    "        return prediction_rating\n",
    "\n",
    "    def get_corr_matrix(self, bin_size=-1):\n",
    "        user_corrs = None\n",
    "        # if valid cache found, try to get user corrs from there\n",
    "        if self.cache.is_temporal_cache_valid():\n",
    "            # First check user-correlations\n",
    "            user_corrs = self.cache.get_user_corrs(self.min_common_elements, self.time_constraint)\n",
    "            if user_corrs is not None:\n",
    "                return user_corrs\n",
    "            # Then check bulk-user-correlations\n",
    "            user_corrs = self.cache.get_user_corrs_from_bulk(time_constraint=self.time_constraint,\n",
    "                                                             min_common_elements=self.min_common_elements,\n",
    "                                                             bin_size=bin_size)\n",
    "            if user_corrs is not None:\n",
    "                return user_corrs\n",
    "\n",
    "        # here, if cache not found or no cache match\n",
    "\n",
    "        # Create user correlations\n",
    "        user_corrs = TemporalPearson.create_user_corrs(movie_ratings=self.cache.movie_ratings,\n",
    "                                                       time_constraint=self.time_constraint,\n",
    "                                                       min_common_elements=self.min_common_elements)\n",
    "        # Cache the user_corrs\n",
    "        self.cache.set_user_corrs(user_corrs=user_corrs,\n",
    "                                  min_common_elements=self.min_common_elements,\n",
    "                                  time_constraint=self.time_constraint)\n",
    "\n",
    "        return user_corrs\n",
    "\n",
    "    @staticmethod\n",
    "    def create_user_corrs(movie_ratings, time_constraint: TimeConstraint, min_common_elements):\n",
    "        # by default movie_ratings is for no time constraint\n",
    "        # with these controls change the time constraint of the movie_ratings\n",
    "        if time_constraint is not None:\n",
    "            if time_constraint.is_valid_max_limit():\n",
    "                movie_ratings = movie_ratings[movie_ratings.timestamp < time_constraint.end_dt]\n",
    "            elif time_constraint.is_valid_time_bin():\n",
    "                movie_ratings = movie_ratings[(movie_ratings.timestamp >= time_constraint.start_dt)\n",
    "                                              & (movie_ratings.timestamp < time_constraint.end_dt)]\n",
    "\n",
    "        user_movie_matrix = movie_ratings.pivot_table(index='title', columns='user_id', values='rating')\n",
    "        return user_movie_matrix.corr(method=\"pearson\", min_periods=min_common_elements)\n",
    "\n",
    "    def cache_user_corrs_in_bulk_for_max_limit(self, time_constraint: TimeConstraint, min_year, max_year):\n",
    "        \"\"\"\n",
    "        Cache user correlations by changing year of the time_constraint\n",
    "        for each year in between min_year and max_year(not included)\n",
    "\n",
    "        :param time_constraint: time_constraint apply\n",
    "        :param min_year: start of the range\n",
    "        :param max_year: end of the range\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.use_bulk_corr_cache:\n",
    "            if time_constraint is not None and time_constraint.is_valid_max_limit():\n",
    "                self.cache.user_corrs_in_bulk = dict()\n",
    "                for year in range(min_year, max_year):\n",
    "                    time_constraint.end_dt = time_constraint.end_dt.replace(year=year)\n",
    "                    corrs = TemporalPearson.create_user_corrs(self.cache.movie_ratings, time_constraint,\n",
    "                                                              self.min_common_elements)\n",
    "                    self.cache.user_corrs_in_bulk[year] = corrs\n",
    "            else:\n",
    "                raise Exception(\"Trying to cache user correlations in bulk for max_limit \"\n",
    "                                \"but start time is not max_limit!\")\n",
    "        else:\n",
    "            raise Exception(\"Trying to create bulk corr cache when use_bulk_corr_cache is False\")\n",
    "\n",
    "    def cache_user_corrs_in_bulk_for_time_bins(self, time_constraint: TimeConstraint, min_year, max_year,\n",
    "                                               min_time_bin_size=2, max_time_bin_size=10):\n",
    "        if self.cache.use_bulk_corr_cache:\n",
    "            if time_constraint is not None and time_constraint.is_valid_time_bin():\n",
    "                del self.cache.user_corrs_in_bulk    # invalidate old cache\n",
    "                self.cache.user_corrs_in_bulk = dict()\n",
    "                for time_bin_size in range(min_time_bin_size, max_time_bin_size):\n",
    "                    self.cache.user_corrs_in_bulk[time_bin_size] = dict()\n",
    "                    for shift in range(0, time_bin_size):\n",
    "                        curr_year = min_year + shift\n",
    "                        while (curr_year + time_bin_size) < max_year:\n",
    "                            time_constraint = TimeConstraint(start_dt=datetime(curr_year, 1, 1),\n",
    "                                                             end_dt=datetime(curr_year + time_bin_size, 1, 1))\n",
    "                            corrs = TemporalPearson.create_user_corrs(self.cache.movie_ratings,\n",
    "                                                                      time_constraint,\n",
    "                                                                      self.min_common_elements)\n",
    "                            self.cache.user_corrs_in_bulk[time_bin_size][curr_year] = corrs\n",
    "                            curr_year += time_bin_size\n",
    "        else:\n",
    "            raise Exception(\"Trying to create bulk corr cache when use_bulk_corr_cache is False\")\n",
    "\n",
    "    @property\n",
    "    def time_constraint(self):\n",
    "        return self._time_constraint\n",
    "\n",
    "    @time_constraint.setter\n",
    "    def time_constraint(self, value):\n",
    "        self._time_constraint = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainsetUser:\n",
    "\n",
    "    def __init__(self, cache: Cache):\n",
    "        \"\"\"\n",
    "        :param cache: Input cache must have movie_ratings not None !\n",
    "        \"\"\"\n",
    "        self.cache = cache\n",
    "\n",
    "        if not self.cache.is_movie_ratings_cached:\n",
    "            raise Exception(\"'movie_ratings' has not been cached !\")\n",
    "\n",
    "    def get_users(self):\n",
    "        \"\"\"\n",
    "        Get list of unique 'user_id's\n",
    "\n",
    "        Since MovieLens Have 'user_id's from 0 to 610 without any missing user, for now sending that directly\n",
    "        Uncomment the other lines later\n",
    "\n",
    "        :return: the ids of the users found in movie_ratings\n",
    "        \"\"\"\n",
    "        #\n",
    "        # if self.cache.is_ratings_cached:\n",
    "        #     data = self.cache.ratings\n",
    "        # else:\n",
    "        #     data = self.cache.movie_ratings\n",
    "        #\n",
    "        # return pd.unique(data['user_id'])\n",
    "        return range(0, 611)\n",
    "\n",
    "    def get_active_users(self, n=10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get Users in sorted order where the first one is the one who has given most ratings.\n",
    "\n",
    "        :param n: Number of users to retrieve.\n",
    "        :return: user DataFrame with index of 'user_id' and columns of ['mean_rating', 'No_of_ratings'] .\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.is_ratings_cached:                         # 30% faster than other choice\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "\n",
    "        active_users = pd.DataFrame(data.groupby('user_id')['rating'].mean())\n",
    "        active_users['No_of_ratings'] = pd.DataFrame(data.groupby('user_id')['rating'].count())\n",
    "        active_users.sort_values(by=['No_of_ratings'], ascending=False, inplace=True)\n",
    "        active_users.columns = ['mean_rating', 'No_of_ratings']\n",
    "        return active_users.head(n)\n",
    "\n",
    "    def get_random_users(self, n=1):\n",
    "        \"\"\"\n",
    "        Get list of random n number of 'user_id's\n",
    "\n",
    "        :param n: Number of random users\n",
    "        :return: List of random 'user_id's\n",
    "        \"\"\"\n",
    "\n",
    "        return random.choices(population=self.get_users(), k=n)\n",
    "\n",
    "    def get_user_ratings(self, user_id: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all the ratings given by of the chosen users\n",
    "\n",
    "        :param user_id: id of the chosen user\n",
    "        :return: Ratings given by the 'user_id'\n",
    "        \"\"\"\n",
    "        if self.cache.is_ratings_cached:                         # 2.2x faster than other choice\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "\n",
    "        return data.loc[data['user_id'] == user_id]\n",
    "\n",
    "    def get_user_avg(self, user_id: int):\n",
    "\n",
    "        if self.cache.use_avg_ratings_cache:\n",
    "            avg_user_rating = self.cache.avg_user_ratings.loc[user_id]\n",
    "            return avg_user_rating[0] if not avg_user_rating.empty else 0\n",
    "\n",
    "        user_ratings = self.get_user_ratings(user_id=user_id)\n",
    "        return user_ratings.rating.mean() if not user_ratings.empty else 0\n",
    "\n",
    "    def get_timestamp(self, user_id: int, movie_id: int):\n",
    "        \"\"\"\n",
    "        Get the timestamp of the given rating\n",
    "\n",
    "        :param user_id: the users whose rating timestamp we are searching\n",
    "        :param movie_id: id of the movie that the user gave the rating\n",
    "        :return: if found the datetime object otherwise None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.is_ratings_cached:\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "\n",
    "        timestamp = data.loc[(data['user_id'] == user_id) & (data['item_id'] == movie_id)]\n",
    "        return timestamp.values[0, 3] if not timestamp.empty else None\n",
    "\n",
    "    def get_first_timestamp(self):\n",
    "        if self.cache.is_ratings_cached:\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "        return data['timestamp'].min()\n",
    "\n",
    "    def get_user_avg_timestamp(self, user_id: int):\n",
    "        user_ratings = self.get_user_ratings(user_id=user_id)\n",
    "        return user_ratings.timestamp.mean() if not user_ratings.empty else 0\n",
    "\n",
    "    # TODO: Later, create TemporalDatasetUser, and put this method into that one\n",
    "    def get_user_ratings_at(self, user_id: int, at: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get user ratings up until the given datetime\n",
    "        :param user_id: id of the chosen user\n",
    "        :param at: only those ratings that are before this date will be taken into account\n",
    "        :return: Ratings given by the 'user_id' before given datetime\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.is_ratings_cached:\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "\n",
    "        return data.loc[(data['user_id'] == user_id) & (data.timestamp < at)]\n",
    "\n",
    "    # TODO: Later, create TemporalDatasetUser, and put this method into that one\n",
    "    def get_user_avg_at(self, user_id: int, at: datetime):\n",
    "        user_ratings = self.get_user_ratings_at(user_id, at)\n",
    "        return user_ratings.rating.mean() if not user_ratings.empty else 0\n",
    "\n",
    "\n",
    "class TrainsetMovie:\n",
    "\n",
    "    def __init__(self, cache: Cache):\n",
    "        \"\"\"\n",
    "        :param cache: Input cache must have movie_ratings not None !\n",
    "        \"\"\"\n",
    "        self.cache = cache\n",
    "\n",
    "        if not self.cache.is_movie_ratings_cached:\n",
    "            raise Exception(\"'movie_ratings' has not been cached !\")\n",
    "\n",
    "    def get_movie(self, movie_id) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get Movie Record\n",
    "\n",
    "        :return: DataFrame which contains the given 'movie_id's details. If not found empty DataFrame .\n",
    "        \"\"\"\n",
    "        if self.cache.is_movies_cached:\n",
    "            return self.cache.movies.loc[self.cache.movies['item_id'] == movie_id]\n",
    "        return self.cache.movie_ratings.loc[self.cache.movie_ratings['item_id'] == movie_id]\n",
    "\n",
    "    def get_movies(self):\n",
    "        \"\"\"\n",
    "        Get list of unique 'item_id's or in other words the movies.\n",
    "\n",
    "        :return: List of movie ids\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.is_movies_cached:\n",
    "            return self.cache.movies['item_id'].values.tolist()\n",
    "\n",
    "        return pd.unique(self.cache.movie_ratings['item_id'])\n",
    "\n",
    "    def get_random_movies(self, n=10):\n",
    "        \"\"\"\n",
    "        Get list of random n number of 'item_id's or in other words the movies\n",
    "\n",
    "        :param n: Number of random movies\n",
    "        :return: List of random 'movie_id's\n",
    "        \"\"\"\n",
    "        return random.choices(population=self.get_movies(), k=n)\n",
    "\n",
    "    def get_movies_watched(self, user_id: int, time_constraint: TimeConstraint = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all the movies watched by the chosen user.\n",
    "\n",
    "        :param user_id: the user that we want to get the movies he-she has watched.\n",
    "        :param time_constraint: type of the time constraint.\n",
    "        :return: DataFrame of all movies watched with 'item_id', 'rating' columns\n",
    "        \"\"\"\n",
    "\n",
    "        movie_ratings = self.cache.movie_ratings\n",
    "\n",
    "        if time_constraint is None:\n",
    "            return movie_ratings.loc[(movie_ratings['user_id'] == user_id)][['item_id', 'rating']]\n",
    "\n",
    "        if time_constraint.is_valid_max_limit():\n",
    "            return movie_ratings.loc[(movie_ratings['user_id'] == user_id)\n",
    "                                     & (movie_ratings.timestamp < time_constraint.end_dt)][['item_id', 'rating']]\n",
    "        elif time_constraint.is_valid_time_bin():\n",
    "            return movie_ratings.loc[(movie_ratings['user_id'] == user_id)\n",
    "                                     & (movie_ratings.timestamp >= time_constraint.start_dt)\n",
    "                                     & (movie_ratings.timestamp < time_constraint.end_dt)][['item_id', 'rating']]\n",
    "        raise Exception(\"Undefined time_constraint is given!\")\n",
    "\n",
    "    def get_movie_rating(self, movie_id: int, user_id: int) -> int:\n",
    "        \"\"\"\n",
    "        Get the movie rating taken by the chosen user\n",
    "\n",
    "        :param movie_id: the movie chosen movie's id\n",
    "        :param user_id: id of the chosen user\n",
    "        :return: Rating given by user. If not found, returns 0\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cache.is_ratings_cached:\n",
    "            data = self.cache.ratings\n",
    "        else:\n",
    "            data = self.cache.movie_ratings\n",
    "\n",
    "        movie_rating = data.loc[(data['user_id'] == user_id) & (data['item_id'] == movie_id)]\n",
    "        return movie_rating.values[0, 2] if not movie_rating.empty else 0\n",
    "\n",
    "    def get_random_movie_watched(self, user_id: int) -> int:\n",
    "        \"\"\"\n",
    "        Get random movie id watched.\n",
    "\n",
    "        :param user_id: User of interest\n",
    "        :return:  movie_id or item_id of the random movie watched by the user.\n",
    "                  In case non-valid user_id supplied then returns 0\n",
    "        \"\"\"\n",
    "        movies_watched = self.get_movies_watched(user_id=user_id)\n",
    "        return random.choice(movies_watched['item_id'].values.tolist()) if not movies_watched.empty else 0\n",
    "\n",
    "    def get_random_movies_watched(self, user_id: int, n=2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get random n movies watched by the user. Only use when n > 2\n",
    "\n",
    "        Use get_random_movie_watched if n=1 since that one 2 fold faster.\n",
    "\n",
    "        :param user_id: the user of interest\n",
    "        :param n: number of random movies to get\n",
    "        :return: DataFrame of movies, if none found then empty DataFrame\n",
    "        \"\"\"\n",
    "        movies_watched = self.get_movies_watched(user_id=user_id)\n",
    "        return random.choices(population=movies_watched['item_id'].values.tolist(),\n",
    "                              k=n) if not movies_watched.empty else movies_watched\n",
    "\n",
    "    def get_random_movie_per_user(self, user_id_list):\n",
    "        \"\"\"\n",
    "        Get random movie for each user given in the 'user_id_list'\n",
    "\n",
    "        :param user_id_list: List of valid user_ids\n",
    "        :return: List of (user_id, movie_id) tuples\n",
    "                where each movie_id is randomly chosen from watched movies of the user_id .\n",
    "                In case any one of the user_id's supplies invalid, then the movie_id will be 0 for that user.\n",
    "        \"\"\"\n",
    "        user_movie_list = list()\n",
    "        for user_id in user_id_list:\n",
    "            user_movie_list.append((user_id, self.get_random_movie_watched(user_id=user_id)))\n",
    "        return user_movie_list\n",
    "\n",
    "\n",
    "class Trainset:\n",
    "    def __init__(self, cache: TemporalCache, min_common_elements: int = 5):\n",
    "        self.cache = cache\n",
    "        self.min_common_elements = min_common_elements\n",
    "        self.similarity = TemporalPearson(time_constraint=None, cache=self.cache)\n",
    "\n",
    "        if not self.cache.is_movie_ratings_cached:\n",
    "            raise Exception(\"'movie_ratings' has not been cached !\")\n",
    "\n",
    "        self.trainset_movie = TrainsetMovie(cache=cache)\n",
    "        self.trainset_user = TrainsetUser(cache=cache)\n",
    "\n",
    "        # if caching is allowed, create user correlations cache\n",
    "        self.similarity.get_corr_matrix()\n",
    "\n",
    "    def predict_movies_watched(self, user_id, n=10, k=10, time_constraint=None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "\n",
    "        :param user_id: user of interest\n",
    "        :param n: Number of movies to predict\n",
    "        :param k: k neighbours to take into account\n",
    "        :param time_constraint: When calculating k neighbours,\n",
    "                                only those that comply to time_constraints will be taken into account.\n",
    "        :return: DataFrame of Predictions where columns = ['prediction', 'rating'] index = 'movie_id'\n",
    "        \"\"\"\n",
    "        # Get all movies watched by a user\n",
    "        movies_watched = self.trainset_movie.get_movies_watched(user_id=user_id)\n",
    "\n",
    "        if movies_watched.empty:\n",
    "            return None\n",
    "\n",
    "        predictions = list()\n",
    "        number_of_predictions = 0\n",
    "        for row in movies_watched.itertuples(index=False):\n",
    "            prediction = self.predict_movie(user_id=user_id, movie_id=row[0],\n",
    "                                            time_constraint=time_constraint, k=k)\n",
    "            if number_of_predictions == n:\n",
    "                break\n",
    "            predictions.append([prediction, row[1], row[0]])\n",
    "            number_of_predictions += 1\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions, columns=['prediction', 'rating', 'movie_id'])\n",
    "        predictions_df.movie_id = predictions_df.movie_id.astype(int)\n",
    "        return predictions_df.set_index('movie_id')\n",
    "\n",
    "    def predict_movie(self, user_id, movie_id, k=10, time_constraint=None, bin_size=-1):\n",
    "        prediction = self.similarity.mean_centered_pearson(user_id=user_id,\n",
    "                                                           movie_id=movie_id,\n",
    "                                                           k_neighbours=\n",
    "                                                           self.get_k_neighbours(user_id, k=k,\n",
    "                                                                                 time_constraint=time_constraint,\n",
    "                                                                                 bin_size=bin_size)\n",
    "                                                           )        \n",
    "        return prediction if prediction <= 5 else 5\n",
    "\n",
    "    def get_k_neighbours(self, user_id, k=20, time_constraint: TimeConstraint = None, bin_size=-1):\n",
    "        \"\"\"\n",
    "        :param user_id: the user of interest\n",
    "        :param k: number of neighbours to retrieve\n",
    "        :param time_constraint: time constraint when choosing neighbours\n",
    "        :param bin_size: Used when using time_bins, in order to select bin from cache\n",
    "        :return: Returns the k neighbours and correlations in between them. If no neighbours found, returns None\n",
    "                 DataFrame which has 'Correlation' column and 'user_id' index.\n",
    "        \"\"\"\n",
    "        self.similarity.time_constraint = time_constraint\n",
    "        user_corr_matrix = self.similarity.get_corr_matrix(bin_size=bin_size)\n",
    "\n",
    "        # Exit if matrix is None, no user found in self.cache.movie_ratings, something is wrong\n",
    "        if user_corr_matrix is None:\n",
    "            return None\n",
    "\n",
    "        # Get the chosen 'user_id's correlations\n",
    "        user_correlations = user_corr_matrix.get(user_id)\n",
    "        if user_correlations is None:\n",
    "            return None\n",
    "\n",
    "        # Drop any null, if found\n",
    "        user_correlations.dropna(inplace=True)\n",
    "        # Create A DataFrame from not-null correlations of the 'user_id'\n",
    "        users_alike = pd.DataFrame(user_correlations)\n",
    "        # Rename the only column to 'correlation'\n",
    "        users_alike.columns = ['correlation']\n",
    "\n",
    "        # Sort the user correlations in descending order\n",
    "        #     so that first one is the most similar, last one least similar\n",
    "        users_alike.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "\n",
    "        # Eliminate Correlation to itself by deleting first row,\n",
    "        #     since biggest corr is with itself it is in first row\n",
    "        return users_alike.iloc[1:k+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Evaluator:\n",
    "\n",
    "    def __init__(self, trainset: Trainset):\n",
    "        self.trainset = trainset\n",
    "\n",
    "    def evaluate_best_max_year_in_bulk(self, n,\n",
    "                                       n_users, n_movies, k=10,\n",
    "                                       min_year=-1,\n",
    "                                       max_year=-1) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate and collect data about best max year constraint which can be put instead of no constraint.\n",
    "\n",
    "        This method calls 'evaluate_best_max_year_constraint' method 'n' times.\n",
    "        Takes required precautions for bulk calling.\n",
    "\n",
    "        :param n: Number of runs that we run the evaluate_best_max_year_constraint() method\n",
    "        :param n_users: Number of users to check\n",
    "        :param n_movies: Number of movies per user to check\n",
    "        :param k: Number of neighbours of each user to take into account when making prediction\n",
    "        :param min_year: First year to evaluate\n",
    "        :param max_year: Last year to evaluate\n",
    "        :return: (no_constrain_rmse_data, best_year_constraint_results)\n",
    "        \"\"\"\n",
    "        if min_year == -1:\n",
    "            min_year = self.trainset.trainset_user.get_first_timestamp().year\n",
    "\n",
    "        if max_year == -1:\n",
    "            max_year = datetime.now().year\n",
    "\n",
    "        time_constraint = TimeConstraint(end_dt=datetime(year=min_year, month=1, day=1))\n",
    "        # Create cache if bulk_corr_cache is allowed\n",
    "        self.trainset.similarity.cache_user_corrs_in_bulk_for_max_limit(time_constraint,\n",
    "                                                                        min_year=min_year,\n",
    "                                                                        max_year=max_year)\n",
    "        \n",
    "        run_results = dict()\n",
    "        for i in range(n):\n",
    "            run_results[i] = self.evaluate_best_max_year_constraint(n_users=n_users, n_movies=n_movies, k=k,\n",
    "                                                                    min_year=min_year, max_year=max_year,\n",
    "                                                                    create_cache=False,)\n",
    "\n",
    "        return run_results\n",
    "\n",
    "    def evaluate_best_max_year_constraint(self, n_users, n_movies, k,\n",
    "                                          max_diff=0.1,\n",
    "                                          min_year=-1, max_year=-1,\n",
    "                                          create_cache=True) -> defaultdict:\n",
    "        \"\"\"\n",
    "        Evaluate the max_year constraint for evaluate_max_year_constraint method.\n",
    "\n",
    "        :param max_diff: maximum difference between rmse when no constraint and with given year constraint.\n",
    "        :param n_users: Number of users to evaluate\n",
    "        :param n_movies: Number of movies per user to evaluate\n",
    "        :param k: Number of neighbours of each user to take into account when making prediction\n",
    "        :param min_year: First year to evaluate\n",
    "        :param max_year: Last year to evaluate\n",
    "        :param create_cache: create cache before running. For bulk callers.\n",
    "        :return: Votes for years where each year got its vote\n",
    "                 when rmse is less than 'max_diff' in between no constraint and year constraint\n",
    "        \"\"\"\n",
    "\n",
    "        if min_year == -1:\n",
    "            min_year = self.trainset.trainset_user.get_first_timestamp().year\n",
    "\n",
    "        if max_year == -1:\n",
    "            max_year = datetime.now().year\n",
    "\n",
    "        if n_users > 600:\n",
    "            user_list = self.trainset.trainset_user.get_users()  # No need to random selection, get all users\n",
    "        else:\n",
    "            user_list = self.trainset.trainset_user.get_random_users(n=n_users)  # Select random n users\n",
    "\n",
    "        # Calculate RMSE With No Constraint\n",
    "        no_constraint_data = dict()\n",
    "        for user_id in user_list:\n",
    "            rmse = Accuracy.rmse(self.trainset.predict_movies_watched(user_id, n_movies, k))\n",
    "            no_constraint_data[user_id] = rmse\n",
    "\n",
    "        # # Calculate RMSE With Time Constraint\n",
    "\n",
    "        # Cache all years before processing\n",
    "        time_constraint = TimeConstraint(end_dt=datetime(year=min_year, month=1, day=1))\n",
    "        # Create cache if bulk_corr_cache is allowed\n",
    "        if create_cache:\n",
    "            self.trainset.similarity.cache_user_corrs_in_bulk_for_max_limit(time_constraint,\n",
    "                                                                            min_year=min_year,\n",
    "                                                                            max_year=max_year)\n",
    "        # Votes to years is stored inside time_constraint_data\n",
    "        time_constraint_data = defaultdict(int)\n",
    "        for year in range(min_year, max_year):\n",
    "            time_constraint.end_dt = time_constraint.end_dt.replace(year=year)\n",
    "\n",
    "            for user_id in user_list:\n",
    "                rmse = Accuracy.rmse(self.trainset.predict_movies_watched(user_id=user_id, n=n_movies, k=k,\n",
    "                                                                          time_constraint=time_constraint))\n",
    "                if abs(rmse - no_constraint_data[user_id]) < max_diff:\n",
    "                    time_constraint_data[year] += 1\n",
    "\n",
    "        return time_constraint_data\n",
    "\n",
    "    def evaluate_max_year_constraint(self, n_users, n_movies, k, time_constraint):\n",
    "        \"\"\"\n",
    "        Compare given time_constraint with normal where no constraint exists.\n",
    "\n",
    "        Time constraint is of type max_year which means the system will be set to a certain year.\n",
    "\n",
    "        :param n_users: Number of users to evaluate\n",
    "        :param n_movies: Number of movies per user to evaluate\n",
    "        :param k: Number of neighbours to take into account when making movie prediction\n",
    "        :param time_constraint: Time constraint which will be applied.\n",
    "        :return: DataFrame of results which contains rmse with constraint and no constraint, as well as runtime.\n",
    "        \"\"\"\n",
    "        trainset = self.trainset\n",
    "        data = list()\n",
    "\n",
    "        for i in range(n_users):\n",
    "            # Get Random User\n",
    "            user_id = random.randint(1, 610)\n",
    "            # Predict movies for user and record runtime\n",
    "            st = default_timer()\n",
    "            rmse = Accuracy.rmse(\n",
    "                trainset.predict_movies_watched(user_id=user_id, n=n_movies, k=k, time_constraint=None))\n",
    "            r1 = default_timer() - st\n",
    "            # Predict movies with time_constraint for user and record runtime\n",
    "            st = default_timer()\n",
    "            time_constrained_rmse = Accuracy.rmse(\n",
    "                trainset.predict_movies_watched(user_id=user_id, n=n_movies, k=k, time_constraint=time_constraint))\n",
    "            r2 = default_timer() - st\n",
    "            # Save iteration data\n",
    "            data.append([user_id, rmse, r1, time_constrained_rmse, r2])\n",
    "\n",
    "        data = pd.DataFrame(data)\n",
    "        data.columns = ['user_id', 'rmse', 'runtime1', 'temporal_rmse', 'runtime2']\n",
    "        data.set_index('user_id', inplace=True)\n",
    "        return data\n",
    "\n",
    "    def evaluate_time_bins_in_bulk(self, n, n_users, k=10,\n",
    "                                   min_year=-1,\n",
    "                                   max_year=-1,\n",
    "                                   min_time_bin_size=2, max_time_bin_size=10):\n",
    "        \"\"\"\n",
    "        Evaluate time bins and return the results.\n",
    "\n",
    "        This method calls 'evaluate_time_bins' method 'n' times. Takes required precautions for bulk calling.\n",
    "\n",
    "        :param n: Number of runs\n",
    "        :param n_users: Number of users\n",
    "        :param k: Number of neighbours will be used when making prediction\n",
    "        :param min_year: First year to start when taking time bins\n",
    "        :param max_year: When to stop when taking time bins, last is not included.\n",
    "        :param min_time_bin_size: Minimum bin size in years\n",
    "        :param max_time_bin_size: Maximum bin size in years\n",
    "        :return: Evaluation results\n",
    "        \"\"\"\n",
    "        if min_year == -1:\n",
    "            min_year = self.trainset.trainset_user.get_first_timestamp().year\n",
    "\n",
    "        if max_year == -1:\n",
    "            max_year = datetime.now().year\n",
    "\n",
    "        # Cache all years before processing\n",
    "        time_constraint = TimeConstraint(start_dt=datetime(year=min_year, month=1, day=1),\n",
    "                                         end_dt=datetime(year=max_year, month=1, day=1))\n",
    "        self.trainset.similarity.cache_user_corrs_in_bulk_for_time_bins(time_constraint,\n",
    "                                                                        min_year=min_year,\n",
    "                                                                        max_year=max_year,\n",
    "                                                                        min_time_bin_size=min_time_bin_size,\n",
    "                                                                        max_time_bin_size=max_time_bin_size)\n",
    "\n",
    "        run_results = dict()\n",
    "        for i in range(n):\n",
    "            run_results[i] = self.evaluate_time_bins(n_users=n_users, k=k, min_year=min_year, max_year=max_year,\n",
    "                                                     min_time_bin_size=min_time_bin_size,\n",
    "                                                     max_time_bin_size=max_time_bin_size,\n",
    "                                                     create_cache=False)\n",
    "\n",
    "        return run_results\n",
    "\n",
    "    def evaluate_time_bins(self, n_users, k, min_year=-1, max_year=-1,\n",
    "                           min_time_bin_size=2, max_time_bin_size=10,\n",
    "                           create_cache=True) -> dict:\n",
    "        \"\"\"\n",
    "\n",
    "        :param n_users: Number of users\n",
    "        :param k: Number of neighbours will be used when making prediction\n",
    "        :param min_year: First year to start when taking time bins\n",
    "        :param max_year: When to stop when taking time bins, last is not included.\n",
    "        :param min_time_bin_size: Minimum bin size in years\n",
    "        :param max_time_bin_size: Maximum bin size in years\n",
    "        :param create_cache: Create cache before calling time bins. For bulk callers.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        trainset = self.trainset\n",
    "\n",
    "        if min_year == -1:\n",
    "            min_year = self.trainset.trainset_user.get_first_timestamp().year\n",
    "\n",
    "        if max_year == -1:\n",
    "            max_year = datetime.now().year\n",
    "\n",
    "        if n_users > 600:\n",
    "            user_list = trainset.trainset_user.get_users()\n",
    "        else:\n",
    "            user_list = trainset.trainset_user.get_random_users(n=n_users)\n",
    "        user_movie_list = trainset.trainset_movie.get_random_movie_per_user(user_list)\n",
    "        data = dict()\n",
    "\n",
    "        result = list()\n",
    "\n",
    "        if create_cache:\n",
    "            # Cache all years before processing\n",
    "            time_constraint = TimeConstraint(start_dt=datetime(year=min_year, month=1, day=1),\n",
    "                                             end_dt=datetime(year=max_year, month=1, day=1))\n",
    "            self.trainset.similarity.cache_user_corrs_in_bulk_for_time_bins(time_constraint,\n",
    "                                                                            min_year=min_year,\n",
    "                                                                            max_year=max_year,\n",
    "                                                                            min_time_bin_size=min_time_bin_size,\n",
    "                                                                            max_time_bin_size=max_time_bin_size)\n",
    "\n",
    "        # Take each bins where first bin 'min_time_bin_size' years, last one 'max_time_bin_size - 1' years\n",
    "        for time_bin_size in range(min_time_bin_size, max_time_bin_size):\n",
    "            # Shift each time_bin starting with 0 years up until (time_bin-1) years\n",
    "            for shift in range(0, time_bin_size):\n",
    "                curr_year = min_year + shift\n",
    "                predictions = list()\n",
    "                start_time = default_timer()\n",
    "                # Scan and make predictions for all the time_bins\n",
    "                while (curr_year + time_bin_size) < max_year:\n",
    "                    for user_id, movie_id in user_movie_list:\n",
    "                        p = trainset.predict_movie(user_id=user_id, movie_id=movie_id, k=k,\n",
    "                                                   time_constraint=TimeConstraint(start_dt=datetime(curr_year, 1, 1),\n",
    "                                                                                  end_dt=datetime(curr_year+time_bin_size, 1, 1)),\n",
    "                                                   bin_size=time_bin_size)\n",
    "                        # if prediction has been done successfully\n",
    "                        if p != 0:\n",
    "                            r = trainset.trainset_movie.get_movie_rating(movie_id=movie_id, user_id=user_id)\n",
    "                            # Append (prediction, actual_rating)\n",
    "                            predictions.append((p, r))\n",
    "                    curr_year += time_bin_size\n",
    "                runtime = default_timer() - start_time\n",
    "                bin_rmse = Accuracy.rmse(predictions=predictions)\n",
    "                iteration_results = {\"bin_size\": time_bin_size,\n",
    "                                     \"start_year\": min_year + shift,\n",
    "                                     \"predictions\": predictions,\n",
    "                                     \"rmse\": bin_rmse,\n",
    "                                     \"runtime\": runtime\n",
    "                                     }\n",
    "                result.append(iteration_results)\n",
    "\n",
    "        data['result'] = result\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = TemporalCache(time_constraint=None, \n",
    "                  is_ratings_cached=True,\n",
    "                  is_movies_cached=True,\n",
    "                  is_movie_ratings_cached=True,\n",
    "                  ratings=MovieLensDataset.load_ratings(r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\ratings.csv'),\n",
    "                  movies=MovieLensDataset.load_movies(r'C:\\Users\\Yukawa\\datasets\\ml-latest-small\\movies.csv'),\n",
    "                  movie_ratings=MovieLensDataset.load(),\n",
    "                  is_user_correlations_cached=True,\n",
    "                  use_bulk_corr_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Trainset(cache=c, min_common_elements=5)\n",
    "e = Evaluator(trainset=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timebin Based Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MovieLensDataset(is_movies_cached=True, is_ratings_cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_timebin_corr(ratings, timebin, avg_rating, user_id, timebin_time_constraint):\n",
    "    curr_bin = t.trainset_movie.get_movies_watched(user_id, timebin_time_constraint)\n",
    "    merged = curr_bin.merge(timebin, on='item_id')\n",
    "    common_elements = len(merged)\n",
    "    \n",
    "    user_avg_rating = t.trainset_user.get_user_avg(user_id)\n",
    "    numenator = ((merged['rating_x'] - avg_rating) * (merged['rating_y'] - user_avg_rating)).sum()\n",
    "    denominator = math.sqrt(((merged['rating_x'] - avg_rating) ** 2).sum())\n",
    "    denominator *= math.sqrt(((merged['rating_y'] - user_avg_rating) ** 2).sum())\n",
    "    pearson = numenator / denominator\n",
    "    \n",
    "    return pearson, common_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_movies_watched_from_timebin(timebin, user_id: int, n=2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get random n movies watched by the user. \n",
    "    \"\"\"\n",
    "    return sample(timebin.index.to_list(),n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1996-12-16 19:09:27')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.trainset_user.get_timestamp(user_id=206, movie_id=786)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timebin_size(tc: TimeConstraint):\n",
    "    return abs((tc.start_dt - tc.end_dt).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movies_watched(ratings, user_id: int, time_constraint: TimeConstraint = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get all the movies watched by the chosen user.\n",
    "\n",
    "    :param user_id: the user that we want to get the movies he-she has watched.\n",
    "    :param time_constraint: type of the time constraint.\n",
    "    :return: DataFrame of all movies watched with 'item_id', 'rating' columns\n",
    "    \"\"\"\n",
    "\n",
    "    movie_ratings = ratings\n",
    "    \n",
    "    if time_constraint is None:\n",
    "        return movie_ratings.loc[(movie_ratings['user_id'] == user_id)][['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "\n",
    "    if time_constraint.is_valid_max_limit():\n",
    "        return movie_ratings.loc[(movie_ratings['user_id'] == user_id)\n",
    "                                 & (movie_ratings.timestamp < time_constraint.end_dt)][['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "    elif time_constraint.is_valid_time_bin():\n",
    "        return movie_ratings.loc[(movie_ratings['user_id'] == user_id)\n",
    "                                 & (movie_ratings.timestamp >= time_constraint.start_dt)\n",
    "                                 & (movie_ratings.timestamp < time_constraint.end_dt)][['item_id', 'rating', 'timestamp']].set_index('item_id')\n",
    "    raise Exception(\"Undefined time_constraint is given!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timebin(ratings, user_id: int, time_constraint) -> pd.DataFrame:\n",
    "    return get_movies_watched(ratings, user_id=user_id, time_constraint=time_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timebin_neighbours(ratings, user_id, time_constraint, k:int):\n",
    "    # Get the user timebin\n",
    "    timebin = get_timebin(ratings, user_id, time_constraint)\n",
    "    \n",
    "    # Count number of common ratings with other users\n",
    "    userlist = [0 for i in range(611)]\n",
    "    for movie_id in timebin.index.values.tolist():\n",
    "        users_who_watched = ratings.loc[(ratings['item_id'] == 3) & (ratings['timestamp'] < time_constraint.end_dt)][['user_id']].values.tolist()\n",
    "        for user_who_watched in users_who_watched:\n",
    "            userlist[user_who_watched[0]] += 1\n",
    "    \n",
    "    # Filter k of them\n",
    "    neighbour_id_list = []\n",
    "    for i in range(0, 611):\n",
    "        if userlist[i] > k:\n",
    "            neighbour_id_list.append(i)\n",
    "    return neighbour_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 6,\n",
       " 19,\n",
       " 32,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 51,\n",
       " 58,\n",
       " 64,\n",
       " 68,\n",
       " 91,\n",
       " 100,\n",
       " 102,\n",
       " 116,\n",
       " 117,\n",
       " 150,\n",
       " 151,\n",
       " 169,\n",
       " 179,\n",
       " 217,\n",
       " 226,\n",
       " 240,\n",
       " 269,\n",
       " 270,\n",
       " 288,\n",
       " 289,\n",
       " 294,\n",
       " 302,\n",
       " 307,\n",
       " 308,\n",
       " 321,\n",
       " 330,\n",
       " 337,\n",
       " 368,\n",
       " 410,\n",
       " 414,\n",
       " 448,\n",
       " 456,\n",
       " 470,\n",
       " 477,\n",
       " 480,\n",
       " 492,\n",
       " 501,\n",
       " 544,\n",
       " 552,\n",
       " 555,\n",
       " 588,\n",
       " 590,\n",
       " 594,\n",
       " 599,\n",
       " 608]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_timebin_neighbours(dataset.ratings, 443, TimeConstraint(start_dt=datetime(year=2017, month=1, day=1), end_dt=datetime.now()), k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prevalent_genre(timebin):\n",
    "    genre_voting = defaultdict(int)\n",
    "    for row in timebin.itertuples(index=False):\n",
    "        genres = row[4]\n",
    "        for genre in genres.split(\"|\"): \n",
    "            genre_voting[genre] += 1\n",
    "    return max(genre_voting.items(), key=lambda a: a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timebin_length(timebin) -> timedelta:\n",
    "    genre = get_prevalent_genre(fd)   # like ('action', 10)\n",
    "    first_item_ts = timebin.iloc[-1][1]\n",
    "    last_item_ts = None\n",
    "    for i in range(len(timebin)):\n",
    "        if 'Action' in timebin.iloc[-i][4].split(\"|\"):\n",
    "            last_item_ts = timebin.iloc[-i][1]\n",
    "    return first_item_ts - last_item_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_timebins(ratings, user_id, time_constraint, k, n):\n",
    "    \"\"\"\n",
    "    k: Komşunun genel ortak film sayısı\n",
    "    n: timebin içindeki ortak film sayısı\n",
    "    \"\"\"\n",
    "    # Get the user timebin\n",
    "    timebin = get_timebin(ratings, user_id, time_constraint)\n",
    "    timebin_size_in_days = get_timebin_size(time_constraint)\n",
    "    first_timestamp = ratings['timestamp'].min()\n",
    "    max_timebin_size_in_days = abs((first_timestamp - time_constraint.end_dt).days)\n",
    "    \n",
    "    # Neighbours contains users who has watched the movies in common\n",
    "    neighbours = get_timebin_neighbours(ratings, user_id, time_constraint, k)\n",
    "    \n",
    "    # Avg rating of the user\n",
    "    avg_rating = t.trainset_user.get_user_avg(user_id)\n",
    "    \n",
    "    data = list()\n",
    "    start_time = default_timer()\n",
    "    for timebin_size in range(timebin_size_in_days, max_timebin_size_in_days, timebin_size_in_days):\n",
    "        for shift in range(0, timebin_size, timebin_size//10):    # make 10 start time shift\n",
    "            start_dt = first_timestamp + timedelta(days=shift)   # assign start time \n",
    "            curr_dt = start_dt\n",
    "            while (curr_dt + timedelta(days=timebin_size)) < time_constraint.end_dt:\n",
    "                for user_id in neighbours:   \n",
    "                    end_dt = curr_dt + timedelta(days=timebin_size)\n",
    "                    # find curr_timebin\n",
    "                    timebin_time_constraint = TimeConstraint(start_dt=curr_dt, end_dt=end_dt)\n",
    "                    corr, common_elements = find_timebin_corr(ratings, timebin=timebin, avg_rating=avg_rating, \n",
    "                                             user_id=user_id, timebin_time_constraint=timebin_time_constraint)\n",
    "                    # if and only if more than n movies in common rated in the temporal time bin, get the correlations\n",
    "                    if not math.isnan(corr) and common_elements > n:\n",
    "                        data.append( (user_id, curr_dt, timebin_size, corr) )\n",
    "                        \n",
    "                curr_dt = end_dt                        #curr_dt + timedelta(days=timebin_size)\n",
    "                #print(f\"start={start_dt} -  curr={curr_dt}, shift={shift}, timebin_size ={timebin_size}\")\n",
    "    runtime = default_timer() - start_time\n",
    "    #print(f\"runtime={runtime}\")    \n",
    "    return pd.DataFrame(data, columns=['user_id', 'start_dt', 'bin_size_in_days', 'pearson_corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yukawa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "df = get_most_similar_timebins(dataset.ratings, 443, TimeConstraint(start_dt=datetime(year=2017, month=1, day=1), end_dt=datetime.now()), 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(['bin_size_in_days', 'pearson_corr'], ascending=[True, False], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>start_dt</th>\n",
       "      <th>bin_size_in_days</th>\n",
       "      <th>pearson_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>414</td>\n",
       "      <td>1999-08-25 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>414</td>\n",
       "      <td>1999-12-27 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>414</td>\n",
       "      <td>2000-04-29 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>414</td>\n",
       "      <td>1999-08-21 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>414</td>\n",
       "      <td>1997-04-05 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.935812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>217</td>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>8708</td>\n",
       "      <td>-0.514511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>308</td>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>8708</td>\n",
       "      <td>-0.532620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>91</td>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>8708</td>\n",
       "      <td>-0.552446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>68</td>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>8708</td>\n",
       "      <td>-0.597608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>307</td>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>8708</td>\n",
       "      <td>-0.677503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>966 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id            start_dt  bin_size_in_days  pearson_corr\n",
       "16       414 1999-08-25 18:36:55              1244      0.943269\n",
       "48       414 1999-12-27 18:36:55              1244      0.943269\n",
       "71       414 2000-04-29 18:36:55              1244      0.943269\n",
       "242      414 1999-08-21 18:36:55              1244      0.943269\n",
       "88       414 1997-04-05 18:36:55              1244      0.935812\n",
       "..       ...                 ...               ...           ...\n",
       "946      217 1996-03-29 18:36:55              8708     -0.514511\n",
       "953      308 1996-03-29 18:36:55              8708     -0.532620\n",
       "942       91 1996-03-29 18:36:55              8708     -0.552446\n",
       "941       68 1996-03-29 18:36:55              8708     -0.597608\n",
       "952      307 1996-03-29 18:36:55              8708     -0.677503\n",
       "\n",
       "[966 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>start_dt</th>\n",
       "      <th>bin_size_in_days</th>\n",
       "      <th>pearson_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>414</td>\n",
       "      <td>1999-08-25 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>414</td>\n",
       "      <td>1999-12-27 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>414</td>\n",
       "      <td>2000-04-29 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>414</td>\n",
       "      <td>1999-08-21 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>414</td>\n",
       "      <td>1997-04-05 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.935812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>414</td>\n",
       "      <td>1997-08-07 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.935812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>414</td>\n",
       "      <td>1997-12-09 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.935812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>414</td>\n",
       "      <td>1998-04-12 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.935812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>414</td>\n",
       "      <td>1998-08-14 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.935812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>414</td>\n",
       "      <td>1998-12-16 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.935812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>414</td>\n",
       "      <td>1999-04-19 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.935812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>169</td>\n",
       "      <td>2003-01-20 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.929164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>169</td>\n",
       "      <td>2003-05-24 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.929164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>169</td>\n",
       "      <td>2001-01-02 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.929164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>169</td>\n",
       "      <td>2001-05-06 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.929164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>169</td>\n",
       "      <td>2001-09-07 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.929164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>169</td>\n",
       "      <td>2002-01-09 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.929164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>169</td>\n",
       "      <td>2002-05-13 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.929164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>169</td>\n",
       "      <td>2002-09-14 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.929164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>169</td>\n",
       "      <td>2003-01-16 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.929164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>555</td>\n",
       "      <td>1999-08-25 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.922836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>555</td>\n",
       "      <td>1999-12-27 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.922836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>555</td>\n",
       "      <td>2000-04-29 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.922836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>555</td>\n",
       "      <td>2000-08-31 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.922836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>555</td>\n",
       "      <td>2001-01-02 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.922836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id            start_dt  bin_size_in_days  pearson_corr\n",
       "16       414 1999-08-25 18:36:55              1244      0.943269\n",
       "48       414 1999-12-27 18:36:55              1244      0.943269\n",
       "71       414 2000-04-29 18:36:55              1244      0.943269\n",
       "242      414 1999-08-21 18:36:55              1244      0.943269\n",
       "88       414 1997-04-05 18:36:55              1244      0.935812\n",
       "111      414 1997-08-07 18:36:55              1244      0.935812\n",
       "133      414 1997-12-09 18:36:55              1244      0.935812\n",
       "156      414 1998-04-12 18:36:55              1244      0.935812\n",
       "178      414 1998-08-14 18:36:55              1244      0.935812\n",
       "200      414 1998-12-16 18:36:55              1244      0.935812\n",
       "221      414 1999-04-19 18:36:55              1244      0.935812\n",
       "20       169 2003-01-20 18:36:55              1244      0.929164\n",
       "53       169 2003-05-24 18:36:55              1244      0.929164\n",
       "113      169 2001-01-02 18:36:55              1244      0.929164\n",
       "136      169 2001-05-06 18:36:55              1244      0.929164\n",
       "158      169 2001-09-07 18:36:55              1244      0.929164\n",
       "181      169 2002-01-09 18:36:55              1244      0.929164\n",
       "203      169 2002-05-13 18:36:55              1244      0.929164\n",
       "224      169 2002-09-14 18:36:55              1244      0.929164\n",
       "246      169 2003-01-16 18:36:55              1244      0.929164\n",
       "18       555 1999-08-25 18:36:55              1244      0.922836\n",
       "50       555 1999-12-27 18:36:55              1244      0.922836\n",
       "73       555 2000-04-29 18:36:55              1244      0.922836\n",
       "93       555 2000-08-31 18:36:55              1244      0.922836\n",
       "115      555 2001-01-02 18:36:55              1244      0.922836"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_dt</th>\n",
       "      <th>bin_size_in_days</th>\n",
       "      <th>pearson_corr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1999-08-25 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1997-08-08 18:36:55</td>\n",
       "      <td>4976</td>\n",
       "      <td>0.929164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>1999-08-21 18:36:55</td>\n",
       "      <td>2488</td>\n",
       "      <td>0.922836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>2488</td>\n",
       "      <td>0.830366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>1998-12-16 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.788232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>6220</td>\n",
       "      <td>0.757158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>2009-11-08 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.698530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>2006-06-13 18:36:55</td>\n",
       "      <td>2488</td>\n",
       "      <td>0.666486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>3732</td>\n",
       "      <td>0.665170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>2000-04-29 18:36:55</td>\n",
       "      <td>3732</td>\n",
       "      <td>0.642239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>1998-04-12 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.583354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>7464</td>\n",
       "      <td>0.567006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1998-04-12 18:36:55</td>\n",
       "      <td>2488</td>\n",
       "      <td>0.489127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>1998-04-14 18:36:55</td>\n",
       "      <td>7464</td>\n",
       "      <td>0.484322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>1996-07-31 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.479119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2003-09-25 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.428254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-12-09 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.314515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2002-05-15 18:36:55</td>\n",
       "      <td>3732</td>\n",
       "      <td>0.286113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>2488</td>\n",
       "      <td>0.245177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>2488</td>\n",
       "      <td>0.094906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>3732</td>\n",
       "      <td>0.062453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1997-12-09 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.042771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>8708</td>\n",
       "      <td>0.015333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>-0.039277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>2008-06-30 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>-0.296567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>2001-05-08 18:36:55</td>\n",
       "      <td>6220</td>\n",
       "      <td>-0.323415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1996-03-29 18:36:55</td>\n",
       "      <td>8708</td>\n",
       "      <td>-0.347745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>1997-12-09 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>-0.514511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>2005-06-07 18:36:55</td>\n",
       "      <td>3732</td>\n",
       "      <td>-0.532620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2005-02-02 18:36:55</td>\n",
       "      <td>1244</td>\n",
       "      <td>-0.552446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2003-01-12 18:36:55</td>\n",
       "      <td>2488</td>\n",
       "      <td>-0.564306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>1998-04-14 18:36:55</td>\n",
       "      <td>7464</td>\n",
       "      <td>-0.677503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   start_dt  bin_size_in_days  pearson_corr\n",
       "user_id                                                    \n",
       "414     1999-08-25 18:36:55              1244      0.943269\n",
       "169     1997-08-08 18:36:55              4976      0.929164\n",
       "555     1999-08-21 18:36:55              2488      0.922836\n",
       "32      1996-03-29 18:36:55              2488      0.830366\n",
       "288     1998-12-16 18:36:55              1244      0.788232\n",
       "179     1996-03-29 18:36:55              6220      0.757158\n",
       "590     2009-11-08 18:36:55              1244      0.698530\n",
       "480     2006-06-13 18:36:55              2488      0.666486\n",
       "43      1996-03-29 18:36:55              3732      0.665170\n",
       "477     2000-04-29 18:36:55              3732      0.642239\n",
       "294     1998-04-12 18:36:55              1244      0.583354\n",
       "58      1996-03-29 18:36:55              7464      0.567006\n",
       "42      1998-04-12 18:36:55              2488      0.489127\n",
       "226     1998-04-14 18:36:55              7464      0.484322\n",
       "302     1996-07-31 18:36:55              1244      0.479119\n",
       "64      2003-09-25 18:36:55              1244      0.428254\n",
       "1       1997-12-09 18:36:55              1244      0.314515\n",
       "448     2002-05-15 18:36:55              3732      0.286113\n",
       "588     1996-03-29 18:36:55              2488      0.245177\n",
       "6       1996-03-29 18:36:55              2488      0.094906\n",
       "117     1996-03-29 18:36:55              3732      0.062453\n",
       "368     1997-12-09 18:36:55              1244      0.042771\n",
       "240     1996-03-29 18:36:55              8708      0.015333\n",
       "470     1996-03-29 18:36:55              1244     -0.039277\n",
       "330     2008-06-30 18:36:55              1244     -0.296567\n",
       "608     2001-05-08 18:36:55              6220     -0.323415\n",
       "599     1996-03-29 18:36:55              8708     -0.347745\n",
       "217     1997-12-09 18:36:55              1244     -0.514511\n",
       "308     2005-06-07 18:36:55              3732     -0.532620\n",
       "91      2005-02-02 18:36:55              1244     -0.552446\n",
       "68      2003-01-12 18:36:55              2488     -0.564306\n",
       "307     1998-04-14 18:36:55              7464     -0.677503"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='pearson_corr', ascending=False).drop_duplicates('user_id').set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-08-03 01:08:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2017-08-03 01:07:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-08-03 01:07:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-08-03 01:07:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2017-08-03 01:07:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2017-08-03 01:07:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2017-08-03 01:08:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5952</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2017-08-03 01:08:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7153</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2017-08-03 01:08:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79132</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-08-03 01:11:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96430</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2017-08-03 01:09:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106918</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2017-08-03 01:09:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating           timestamp\n",
       "item_id                            \n",
       "1           4.0 2017-08-03 01:08:02\n",
       "110         4.5 2017-08-03 01:07:57\n",
       "260         4.0 2017-08-03 01:07:45\n",
       "296         4.0 2017-08-03 01:07:47\n",
       "318         5.0 2017-08-03 01:07:38\n",
       "356         5.0 2017-08-03 01:07:41\n",
       "608         5.0 2017-08-03 01:08:15\n",
       "5952        5.0 2017-08-03 01:08:17\n",
       "7153        5.0 2017-08-03 01:08:18\n",
       "79132       1.0 2017-08-03 01:11:35\n",
       "96430       5.0 2017-08-03 01:09:03\n",
       "106918      5.0 2017-08-03 01:09:46"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = TimeConstraint(start_dt=datetime(year=2017, month=1, day=1), end_dt=datetime.now())\n",
    "tb = get_timebin(dataset.ratings, user_id=443, time_constraint=tc)\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_movies_watched(timebin, user_id, timeconstraint):\n",
    "    movies_watched = timebin.index.to_list()\n",
    "    predictions = list()\n",
    "    for movie in movies_watched:\n",
    "        prediction = t.predict_movie(user_id=user_id, movie_id=movie, k=10)\n",
    "        if prediction != 0:\n",
    "            actual = t.trainset_movie.get_movie_rating(movie_id=movie, user_id=user_id)\n",
    "            predictions.append((prediction, actual))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.121621621621622, 4.5),\n",
       " (3.5412895265381175, 4.0),\n",
       " (3.9815086592962965, 4.0),\n",
       " (4.726583392125758, 5.0),\n",
       " (4.511206121155887, 5.0),\n",
       " (5, 5.0),\n",
       " (4.900062273990611, 5.0),\n",
       " (4.900062273990611, 5.0),\n",
       " (3.939462223865436, 1.0),\n",
       " (4.042003787226717, 5.0)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict_movies_watched(timebin=tb, user_id=443, timeconstraint=tc)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.024577353020557"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timebin_neighbours_data(ratings, timebin, similar_timebins, corr_threshold=0.5):\n",
    "    data = defaultdict(list)\n",
    "    for row in similar_timebins.itertuples(index=False):\n",
    "        neighbour_id = row[0]\n",
    "        start_dt = row[1]\n",
    "        timebin_size = row[2]\n",
    "        corr = row[3]\n",
    "        if corr < corr_threshold:                     # Dont process if corr is less than corr_threshold\n",
    "            continue\n",
    "        end_dt = start_dt + timedelta(days=timebin_size)\n",
    "        timebin_tc = TimeConstraint(start_dt=start_dt, end_dt=end_dt)\n",
    "        neighbour_bin = get_timebin(ratings, user_id=neighbour_id, time_constraint=timebin_tc)\n",
    "        merged_bin = pd.merge(timebin, neighbour_bin, left_index=True, right_index=True)\n",
    "        for bin_row in merged_bin.itertuples(index=True):\n",
    "            curr_movie = bin_row[0]\n",
    "            neighbour_rating = bin_row[3]\n",
    "            #print(bin_row[0], bin_row[1], bin_row[2], bin_row[3], bin_row[4])\n",
    "            data[curr_movie].append( (neighbour_rating, corr) )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_timebin_neighbours_data(dataset.ratings, timebin=tb, similar_timebins=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_movies_watched_using_timebin_neighbours(data, user_id, min_neighbour_count=5):\n",
    "    predictions = list()\n",
    "    for movie_id, rating_corr_list in data.items():\n",
    "        weighted_sum = 0\n",
    "        weight_sum = 0\n",
    "        count = 0\n",
    "        for rating_corr in rating_corr_list:\n",
    "            count += 1\n",
    "            rating = rating_corr[0]\n",
    "            corr = rating_corr[1]\n",
    "            weighted_sum += rating * corr\n",
    "            weight_sum += corr\n",
    "        if count < min_neighbour_count:         # if less than min_neighbour_count neighbour found, pass\n",
    "            continue\n",
    "        prediction = weighted_sum / weight_sum\n",
    "        actual = t.trainset_movie.get_movie_rating(movie_id=movie_id, user_id=user_id)\n",
    "        predictions.append( (prediction, actual) )\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.9154652826623155, 4.0),\n",
       " (4.695140644695054, 4.5),\n",
       " (4.869577181383489, 4.0),\n",
       " (4.33078315429006, 4.0),\n",
       " (4.890852323386359, 5.0),\n",
       " (4.889431782559287, 5.0),\n",
       " (4.384031184453677, 5.0),\n",
       " (4.6369288051115065, 5.0),\n",
       " (4.436814972732457, 5.0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_using_timebins = predict_movies_watched_using_timebin_neighbours(data, 443)\n",
    "predictions_using_timebins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19592912827023481"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy.rmse(predictions_using_timebins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal: 1.024577353020557 \t Timebin:0.19592912827023481\n"
     ]
    }
   ],
   "source": [
    "print(f\"Normal: {Accuracy.rmse(predictions)} \\t Timebin:{Accuracy.rmse(predictions_using_timebins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_normal_and_timebin_predictions(ratings, user_id, time_constraint):\n",
    "    tc = time_constraint\n",
    "    timebin = get_timebin(dataset.ratings, user_id=user_id, time_constraint=tc)\n",
    "    normal_rmse = Accuracy.rmse(predict_movies_watched(timebin=timebin, user_id=user_id, timeconstraint=tc))\n",
    "    \n",
    "    # k -> komşunun en az ortak film sayısı, n -> benzer timebin içindeki en az ortak film sayısı\n",
    "    similar_timebins = get_most_similar_timebins(ratings, user_id,tc, k=5, n=3)\n",
    "    \n",
    "    # Komşuların verilerini topla, bunu yaparken 0.5den aşşağı benzerliği olanları alma.\n",
    "    data = get_timebin_neighbours_data(ratings, timebin, similar_timebins, corr_threshold=0.5)\n",
    "    \n",
    "    # En az 'min_neighbour_count' sayıda farklı timebin'den veri gelmediyse o filme tahmin yapma.\n",
    "    predictions = predict_movies_watched_using_timebin_neighbours(data, user_id, min_neighbour_count=5)\n",
    "    \n",
    "    predictions_using_timebins = Accuracy.rmse(predictions)\n",
    "    \n",
    "    print(f\"Normal RMSE: {normal_rmse} \\t Timebin RMSE:{predictions_using_timebins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yukawa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal RMSE: 1.024577353020557 \t Timebin RMSE:0.19592912827023407\n"
     ]
    }
   ],
   "source": [
    "compare_normal_and_timebin_predictions(dataset.ratings, 443, TimeConstraint(start_dt=datetime(year=2017, month=1, day=1), end_dt=datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yukawa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal RMSE: 0.6604464424540253 \t Timebin RMSE:0.5938418606262376\n"
     ]
    }
   ],
   "source": [
    "compare_normal_and_timebin_predictions(dataset.ratings, 610, TimeConstraint(start_dt=datetime(year=2017, month=5, day=3), end_dt=datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_normal_and_timebin_predictions_(ratings, n, s):\n",
    "    \"\"\"\n",
    "    Compare n number of users\n",
    "    \"\"\"\n",
    "    output = list()\n",
    "    count = 0\n",
    "    while count < n:\n",
    "        # Rastgele kullanıcı seç\n",
    "        user_id = random.randint(0,610)\n",
    "        # Rastgele Filmini seç\n",
    "        movie_id = t.trainset_movie.get_random_movie_watched(user_id)\n",
    "        movie_ts = t.trainset_user.get_timestamp(user_id, movie_id)\n",
    "        # Filmi izlemeden önceki s adet filmden olustan timebini al\n",
    "        try:\n",
    "            movies_watched_until_the_movie_ts = get_movies_watched(ratings, user_id, TimeConstraint(end_dt=movie_ts))\n",
    "            if len(movies_watched_until_the_movie_ts) < s:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # s inci filmin timestamp i ile baslayip, aradığımız movie_ts ile sonlanan veriyi timebin olarak al\n",
    "        tc = TimeConstraint(start_dt=movies_watched_until_the_movie_ts.iloc[-s]['timestamp'], end_dt=movie_ts)\n",
    "        timebin = get_timebin(dataset.ratings, user_id=user_id, time_constraint=tc)\n",
    "        \n",
    "        # Timebin must be at least 500 day\n",
    "        if get_timebin_size(tc) < 500:\n",
    "            continue\n",
    "        \n",
    "        # Normal olarak, timebin içinde bulunan tüm filmlere, kullanıcının vereceği puanları tahmin et \n",
    "        normal_rmse = Accuracy.rmse(predict_movies_watched(timebin=timebin, user_id=user_id, timeconstraint=tc))\n",
    "        \n",
    "        if normal_rmse < 0.5:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # k -> komşunun en az ortak film sayısı, n -> benzer timebin içindeki en az ortak film sayısı\n",
    "            similar_timebins = get_most_similar_timebins(ratings, user_id, tc, k=5, n=3)\n",
    "            count += 1\n",
    "        except:\n",
    "            continue\n",
    "        # Komşuların verilerini topla, bunu yaparken 0.5den aşşağı benzerliği olanları alma.\n",
    "        data = get_timebin_neighbours_data(ratings, timebin, similar_timebins, corr_threshold=0.5)\n",
    "\n",
    "        # En az 'min_neighbour_count' sayıda farklı timebin'den veri gelmediyse o filme tahmin yapma.\n",
    "        predictions = predict_movies_watched_using_timebin_neighbours(data, user_id, min_neighbour_count=5)\n",
    "\n",
    "        predictions_using_timebins = Accuracy.rmse(predictions)\n",
    "\n",
    "        print(f\"Normal RMSE: {normal_rmse} \\t Timebin RMSE:{predictions_using_timebins}\")\n",
    "        output.append( (normal_rmse, predictions_using_timebins) )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yukawa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal RMSE: 0.7382973334844534 \t Timebin RMSE:0.2629040465090774\n",
      "Normal RMSE: 1.6475346821608656 \t Timebin RMSE:1.2916666666666663\n",
      "Normal RMSE: 1.44962522785342 \t Timebin RMSE:0.512563449254115\n",
      "Normal RMSE: 1.1840908663021852 \t Timebin RMSE:1.376222236915341\n",
      "Normal RMSE: 0.8349445813369731 \t Timebin RMSE:0.33783859292393975\n",
      "Normal RMSE: 0.6218708154107632 \t Timebin RMSE:0.590242326126544\n",
      "Normal RMSE: 0.8539375670897097 \t Timebin RMSE:0.8471936102589853\n",
      "Normal RMSE: 1.3722234031033484 \t Timebin RMSE:1.3649797359147402\n",
      "Normal RMSE: 0.5146715911167783 \t Timebin RMSE:1.5403319302271676\n",
      "Normal RMSE: 1.0 \t Timebin RMSE:0.7328972099572972\n",
      "Normal RMSE: 2.6573669413938052 \t Timebin RMSE:0.8345298781073721\n",
      "Normal RMSE: 0.5142856304253163 \t Timebin RMSE:0.18445245489119225\n",
      "Normal RMSE: 1.2937671018080432 \t Timebin RMSE:1.2641673969239176\n",
      "Normal RMSE: 1.4979852439984926 \t Timebin RMSE:1.5180034574356958\n",
      "Normal RMSE: 0.520272918445327 \t Timebin RMSE:0.3775373505877774\n",
      "Normal RMSE: 0.801564018672429 \t Timebin RMSE:1.308199260417625\n",
      "Normal RMSE: 0.6539614517553419 \t Timebin RMSE:0.9999999999999998\n",
      "Normal RMSE: 1.5690618690157128 \t Timebin RMSE:0.6731052050750019\n",
      "Normal RMSE: 1.44962522785342 \t Timebin RMSE:0.512563449254115\n",
      "Normal RMSE: 0.591353602787188 \t Timebin RMSE:0.803275285184281\n",
      "Normal RMSE: 0.6098698433836868 \t Timebin RMSE:0.2798966547862102\n",
      "Normal RMSE: 0.7723500497580585 \t Timebin RMSE:1.3442847314284865\n",
      "Normal RMSE: 1.458924463135164 \t Timebin RMSE:1.6933322830194404\n",
      "Normal RMSE: 0.5261119270792581 \t Timebin RMSE:0.5391593506552161\n",
      "Normal RMSE: 0.6469038693612013 \t Timebin RMSE:0.6406215369437505\n",
      "Normal RMSE: 0.5624413915177403 \t Timebin RMSE:0.6612859929406826\n",
      "Normal RMSE: 1.3722234031033484 \t Timebin RMSE:1.618307432083908\n",
      "Normal RMSE: 1.458924463135164 \t Timebin RMSE:1.6933322830194404\n",
      "Normal RMSE: 0.5706955953800467 \t Timebin RMSE:0.21153846153845934\n",
      "Normal RMSE: 1.1840908663021852 \t Timebin RMSE:1.376222236915341\n",
      "Normal RMSE: 0.5990121755803665 \t Timebin RMSE:0.41336326323266365\n",
      "Normal RMSE: 0.6271589389730029 \t Timebin RMSE:0.156249999999999\n",
      "Normal RMSE: 1.44962522785342 \t Timebin RMSE:0.48558193227471197\n",
      "Normal RMSE: 0.512650904600127 \t Timebin RMSE:0.3822802432616045\n",
      "Normal RMSE: 0.9380072690380682 \t Timebin RMSE:0.9303132191132332\n",
      "Normal RMSE: 0.7382973334844534 \t Timebin RMSE:0.2629040465090774\n",
      "Normal RMSE: 1.3722234031033484 \t Timebin RMSE:1.4213699468448329\n",
      "Normal RMSE: 1.3722234031033484 \t Timebin RMSE:1.5676278290824113\n",
      "Normal RMSE: 0.5990121755803665 \t Timebin RMSE:0.41336326323266365\n",
      "Normal RMSE: 1.6958337439549747 \t Timebin RMSE:0.8713714237165776\n",
      "Normal RMSE: 0.7694506088075154 \t Timebin RMSE:0.7333439383083388\n",
      "Normal RMSE: 0.6458052753742186 \t Timebin RMSE:0.6236305606859297\n",
      "Normal RMSE: 0.5255289939329161 \t Timebin RMSE:0.2666666666666665\n",
      "Normal RMSE: 1.458924463135164 \t Timebin RMSE:1.889880342008259\n",
      "Normal RMSE: 0.5893569029485765 \t Timebin RMSE:0.12499999999999767\n",
      "Normal RMSE: 1.44962522785342 \t Timebin RMSE:0.512563449254115\n",
      "Normal RMSE: 0.8045913486272996 \t Timebin RMSE:0.19078700750283364\n",
      "Normal RMSE: 1.6958337439549747 \t Timebin RMSE:0.8713714237165776\n",
      "Normal RMSE: 0.5198411019670195 \t Timebin RMSE:0.21080690002340804\n",
      "Normal RMSE: 0.600355298058862 \t Timebin RMSE:0.7418944429716018\n",
      "Normal RMSE: 0.683875184301736 \t Timebin RMSE:0.9168758233076474\n",
      "Normal RMSE: 0.6500561440874322 \t Timebin RMSE:0.6146686251790056\n",
      "Normal RMSE: 0.6271589389730029 \t Timebin RMSE:0.156249999999999\n",
      "Normal RMSE: 1.6958337439549747 \t Timebin RMSE:0.8713714237165776\n",
      "Normal RMSE: 0.8740507351895574 \t Timebin RMSE:0.8749021993110275\n",
      "Normal RMSE: 0.5475624309536242 \t Timebin RMSE:0.4999999999999998\n",
      "Normal RMSE: 0.5146715911167783 \t Timebin RMSE:1.5403319302271676\n",
      "Normal RMSE: 1.458924463135164 \t Timebin RMSE:1.9362709203591373\n",
      "Normal RMSE: 0.5473195330309504 \t Timebin RMSE:0.17857585338308848\n",
      "Normal RMSE: 0.8591049701886984 \t Timebin RMSE:1.2588716605427743\n",
      "Normal RMSE: 0.671063022228881 \t Timebin RMSE:0.7225332567993155\n",
      "Normal RMSE: 0.9380072690380682 \t Timebin RMSE:0.9757918616193229\n",
      "Normal RMSE: 5.811445419023843 \t Timebin RMSE:0.2884615384615384\n",
      "Normal RMSE: 1.2937671018080432 \t Timebin RMSE:1.2641673969239176\n",
      "Normal RMSE: 0.5055310111763783 \t Timebin RMSE:0.2666666666666665\n",
      "Normal RMSE: 0.7524060913612184 \t Timebin RMSE:0.1428571428571431\n",
      "Normal RMSE: 0.7939321344549173 \t Timebin RMSE:0.7412793526716033\n",
      "Normal RMSE: 0.5462865643675934 \t Timebin RMSE:0.3284319770976541\n",
      "Normal RMSE: 0.6099555024448482 \t Timebin RMSE:0.019164883671666917\n",
      "Normal RMSE: 0.6094171294610057 \t Timebin RMSE:0.18750000000000033\n",
      "Normal RMSE: 1.2888210882479096 \t Timebin RMSE:1.5895598884439708\n"
     ]
    }
   ],
   "source": [
    "output1 = compare_normal_and_timebin_predictions_(dataset.ratings, n=100, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = compare_normal_and_timebin_predictions_(dataset.ratings, n=100, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = compare_normal_and_timebin_predictions_(dataset.ratings, n=100, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = pd.merge(ff, dataset.movies, left_index=True, right_on='item_id').iloc[-50:]\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
